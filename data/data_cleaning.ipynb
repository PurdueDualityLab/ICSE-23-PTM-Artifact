{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69853\n"
     ]
    }
   ],
   "source": [
    "dic = {'None': 56933,\n",
    " 'common_voice': 328,\n",
    " 'conll2003': 580,\n",
    " 'emotion': 477,\n",
    " 'glue': 355,\n",
    " 'mozilla-foundation/common_voice_7_0': 218,\n",
    " 'mozilla-foundation/common_voice_8_0': 344,\n",
    " 'other': 3263,\n",
    " 'speech-recognition-community-v2/dev_data': 340,\n",
    " 'universal_dependencies': 6834,\n",
    " 'xtreme': 181}\n",
    "\n",
    "count = 0\n",
    "for name in dic:\n",
    "    count += dic[name]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"modelinfo.json\")\n",
    "df.to_csv(\"modelinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ndjson\n",
      "  Using cached ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: ndjson\n",
      "Successfully installed ndjson-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ndjson\n",
    "\n",
    "with open(\"modelinfo.json\", \"r\") as file:\n",
    "\n",
    "    data = json.load(file)\n",
    "    output = ndjson.dumps(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelinfo.ndjson', 'w') as f:\n",
    "    ndjson.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datamodelinfo.json\", \"r\") as file:\n",
    "    # with open(\"./reproducibility/results.json\", \"r\") as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {m[\"id\"]: m for m in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': {'modelId': 'bert-base-uncased',\n",
       "  'sha': '418430c3b5df7ace92f2aede75700d22c78a0f95',\n",
       "  'lastModified': '2022-06-06T11:41:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-uncased',\n",
       "  'downloads': 21607939,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 183,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-macbert-base': {'modelId': 'hfl/chinese-macbert-base',\n",
       "  'sha': 'a986e004d2a7f2a1c2f5a3edef4e20604a974ed1',\n",
       "  'lastModified': '2021-05-19T19:09:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-macbert-base',\n",
       "  'downloads': 11988756,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased': {'modelId': 'distilbert-base-uncased',\n",
       "  'sha': '043235d6088ecd3dd5fb5ca3592b6913fd516027',\n",
       "  'lastModified': '2022-05-31T19:08:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased',\n",
       "  'downloads': 9046961,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 63,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gpt2': {'modelId': 'gpt2',\n",
       "  'sha': '6c0e6080953db56375760c0471a8c5f2929baf11',\n",
       "  'lastModified': '2021-05-19T16:25:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tflite',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2',\n",
       "  'downloads': 9042237,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 141,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['exbert'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english': {'modelId': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "  'sha': '83dbcb26ba08b01dd9dc2ce4eac7670b5c69e255',\n",
       "  'lastModified': '2022-06-27T19:44:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sst2',\n",
       "   'dataset:glue',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "  'downloads': 8359004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 69,\n",
       "  'model-index': [{'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "    'results': [{'task': {'type': 'text-classification',\n",
       "       'name': 'Text Classification'},\n",
       "      'dataset': {'name': 'glue',\n",
       "       'type': 'glue',\n",
       "       'config': 'sst2',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9105504587155964,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.8978260869565218,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9301801801801802,\n",
       "        'verified': True},\n",
       "       {'name': 'AUC',\n",
       "        'type': 'auc',\n",
       "        'value': 0.9716626673402374,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9137168141592922,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.39013850688934326,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['sst2', 'glue'],\n",
       "   'model-index': [{'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "     'results': [{'task': {'type': 'text-classification',\n",
       "        'name': 'Text Classification'},\n",
       "       'dataset': {'name': 'glue',\n",
       "        'type': 'glue',\n",
       "        'config': 'sst2',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9105504587155964,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.8978260869565218,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9301801801801802,\n",
       "         'verified': True},\n",
       "        {'name': 'AUC',\n",
       "         'type': 'auc',\n",
       "         'value': 0.9716626673402374,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9137168141592922,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.39013850688934326,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilgpt2': {'modelId': 'distilgpt2',\n",
       "  'sha': 'b754fe8f9410238372bd6adc0a6c17282c4bb9c0',\n",
       "  'lastModified': '2022-06-02T08:10:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tflite',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:openwebtext',\n",
       "   'arxiv:1910.01108',\n",
       "   'arxiv:2201.08542',\n",
       "   'arxiv:2203.12574',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:1503.02531',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'distilgpt2',\n",
       "  'downloads': 5800175,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 73,\n",
       "  'model-index': [{'name': 'distilgpt2',\n",
       "    'results': [{'task': {'type': 'text-generation',\n",
       "       'name': 'Text Generation'},\n",
       "      'dataset': {'type': 'wikitext', 'name': 'WikiText-103'},\n",
       "      'metrics': [{'type': 'perplexity',\n",
       "        'name': 'Perplexity',\n",
       "        'value': 21.1}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['openwebtext'],\n",
       "   'model-index': [{'name': 'distilgpt2',\n",
       "     'results': [{'task': {'type': 'text-generation',\n",
       "        'name': 'Text Generation'},\n",
       "       'dataset': {'type': 'wikitext', 'name': 'WikiText-103'},\n",
       "       'metrics': [{'type': 'perplexity',\n",
       "         'name': 'Perplexity',\n",
       "         'value': 21.1}]}]}],\n",
       "   'co2_eq_emissions': '149200 g'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-base': {'modelId': 'xlm-roberta-base',\n",
       "  'sha': 'f6d161e8f5f6f2ed433fb4023d6cb34146506b3f',\n",
       "  'lastModified': '2022-06-06T11:40:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'zh',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-base',\n",
       "  'downloads': 5710750,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'zh'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'roberta-base': {'modelId': 'roberta-base',\n",
       "  'sha': '251c3c36356d3ad6845eb0554fdb9703d632c6cc',\n",
       "  'lastModified': '2021-07-06T10:34:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'roberta-base',\n",
       "  'downloads': 5301889,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 42,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-cased': {'modelId': 'bert-base-cased',\n",
       "  'sha': 'a8d257ba9925ef39f3036bfc338acf5283c512d9',\n",
       "  'lastModified': '2021-09-06T08:07:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-cased',\n",
       "  'downloads': 5060964,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-multilingual-cased': {'modelId': 'bert-base-multilingual-cased',\n",
       "  'sha': 'aff660c4522e466f4d0de19eaf94f91e4e2e7375',\n",
       "  'lastModified': '2021-05-18T16:18:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-multilingual-cased',\n",
       "  'downloads': 3281257,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-roberta-wwm-ext': {'modelId': 'hfl/chinese-roberta-wwm-ext',\n",
       "  'sha': '5c58d0b8ec1d9014354d691c538661bf00bfdb44',\n",
       "  'lastModified': '2022-03-01T09:13:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-roberta-wwm-ext',\n",
       "  'downloads': 3270783,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 43,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-multilingual-cased': {'modelId': 'distilbert-base-multilingual-cased',\n",
       "  'sha': '1a01b38498875d45f69b2a6721bf6fe87425da39',\n",
       "  'lastModified': '2020-12-11T21:24:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-multilingual-cased',\n",
       "  'downloads': 3042085,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-chinese': {'modelId': 'bert-base-chinese',\n",
       "  'sha': '4b1f5fb6deac3583018fcf351473024a3d65b2d4',\n",
       "  'lastModified': '2021-05-18T16:13:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-chinese',\n",
       "  'downloads': 2998046,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 99,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SpanBERT/spanbert-large-cased': {'modelId': 'SpanBERT/spanbert-large-cased',\n",
       "  'sha': 'a49cba45de9565a5d3e7b089a94dbae679e64e79',\n",
       "  'lastModified': '2021-05-19T11:31:33.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SpanBERT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SpanBERT/spanbert-large-cased',\n",
       "  'downloads': 2980751,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openai/clip-vit-base-patch32': {'modelId': 'openai/clip-vit-base-patch32',\n",
       "  'sha': 'f4881ba48ee4d21b7ed5602603b9e3e92eb1b346',\n",
       "  'lastModified': '2022-03-14T17:58:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-base-patch32',\n",
       "  'downloads': 2931985,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 41,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'albert-base-v2': {'modelId': 'albert-base-v2',\n",
       "  'sha': '51dbd9db43a0c6eba97f74b91ce26fface509e0b',\n",
       "  'lastModified': '2021-08-30T12:04:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-base-v2',\n",
       "  'downloads': 2221973,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-distilbert-base': {'modelId': 'sentence-transformers/stsb-distilbert-base',\n",
       "  'sha': '815bb3e4dbcba340b5f8c0c0489800230880e06e',\n",
       "  'lastModified': '2022-06-15T19:43:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/stsb-distilbert-base',\n",
       "  'downloads': 2127464,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-zh-en': {'modelId': 'Helsinki-NLP/opus-mt-zh-en',\n",
       "  'sha': 'b7d1e2a349f4e50df9fbde0f8a07280899331cb1',\n",
       "  'lastModified': '2021-02-26T18:53:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'zh',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-zh-en',\n",
       "  'downloads': 2107900,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'}, {'text': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'}],\n",
       "  'likes': 31,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'roberta-large': {'modelId': 'roberta-large',\n",
       "  'sha': '619fd8c2ca2bc7ac3959b7f71b6c426c897ba407',\n",
       "  'lastModified': '2021-05-21T08:57:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'roberta-large',\n",
       "  'downloads': 2041188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-12-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
       "  'sha': '97f7dcbdd6ab58fe7f44368c795fc5200b48fcbe',\n",
       "  'lastModified': '2021-08-05T08:39:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
       "  'downloads': 1779635,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-MiniLM-L6-v2': {'modelId': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sha': '3746fd5f4cfd46ae64fc781df53e7cbb7849eb62',\n",
       "  'lastModified': '2022-06-15T21:28:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'downloads': 1723581,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 43,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-large-finetuned-conll03-english': {'modelId': 'xlm-roberta-large-finetuned-conll03-english',\n",
       "  'sha': '3daa989491c928b218c78023b6dafd57b5dba966',\n",
       "  'lastModified': '2020-10-12T12:57:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-large-finetuned-conll03-english',\n",
       "  'downloads': 1712460,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased': {'modelId': 'bert-large-uncased',\n",
       "  'sha': '3835a195d41f7ddc47d5ecab84b64f71d6f144e9',\n",
       "  'lastModified': '2021-05-18T16:40:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased',\n",
       "  'downloads': 1666413,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L6-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
       "  'sha': '68b97aaedb0c72be3c88c1af64296b3bbb8001fa',\n",
       "  'lastModified': '2022-06-15T18:39:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
       "  'downloads': 1654328,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased-conversational': {'modelId': 'DeepPavlov/rubert-base-cased-conversational',\n",
       "  'sha': '645946ce91842a52eaacb2705c77e59194145ffa',\n",
       "  'lastModified': '2021-11-08T13:06:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased-conversational',\n",
       "  'downloads': 1394793,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-j-6B': {'modelId': 'EleutherAI/gpt-j-6B',\n",
       "  'sha': '918ad376364058dee23512629bc385380c98e57d',\n",
       "  'lastModified': '2022-03-15T13:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'arxiv:2104.09864',\n",
       "   'arxiv:2101.00027',\n",
       "   'transformers',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 1}}},\n",
       "  'id': 'EleutherAI/gpt-j-6B',\n",
       "  'downloads': 1359238,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 228,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-sentiment': {'modelId': 'cardiffnlp/twitter-roberta-base-sentiment',\n",
       "  'sha': 'b636d90b2ed53d7ba6006cefd76f29cd354dd9da',\n",
       "  'lastModified': '2022-04-06T08:10:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-sentiment',\n",
       "  'downloads': 1214346,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 55,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-xlm-roberta-base-sentiment': {'modelId': 'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
       "  'sha': 'f3e34b6c30bf27b6649f72eca85d0bbe79df1e55',\n",
       "  'lastModified': '2022-06-22T19:15:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'arxiv:2104.12250',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
       "  'downloads': 1180933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ü§ó'},\n",
       "   {'text': \"T'estimo! ‚ù§Ô∏è\"},\n",
       "   {'text': 'I love you!'},\n",
       "   {'text': 'I hate you ü§Æ'},\n",
       "   {'text': 'Mahal kita!'},\n",
       "   {'text': 'ÏÇ¨ÎûëÌï¥!'},\n",
       "   {'text': 'ÎÇú ÎÑàÍ∞Ä Ïã´Ïñ¥'},\n",
       "   {'text': 'üòçüòçüòç'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'widget': [{'text': 'ü§ó'},\n",
       "    {'text': \"T'estimo! ‚ù§Ô∏è\"},\n",
       "    {'text': 'I love you!'},\n",
       "    {'text': 'I hate you ü§Æ'},\n",
       "    {'text': 'Mahal kita!'},\n",
       "    {'text': 'ÏÇ¨ÎûëÌï¥!'},\n",
       "    {'text': 'ÎÇú ÎÑàÍ∞Ä Ïã´Ïñ¥'},\n",
       "    {'text': 'üòçüòçüòç'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'unc-nlp/lxmert-base-uncased': {'modelId': 'unc-nlp/lxmert-base-uncased',\n",
       "  'sha': '628572c96242d1496147beec1c13a1bb7869605d',\n",
       "  'lastModified': '2021-03-10T02:39:25.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'lxmert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'unc-nlp',\n",
       "  'config': {'architectures': ['LxmertModel'], 'model_type': 'lxmert'},\n",
       "  'id': 'unc-nlp/lxmert-base-uncased',\n",
       "  'downloads': 1160943,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-de': {'modelId': 'Helsinki-NLP/opus-mt-en-de',\n",
       "  'sha': '6c00b328d3da7183582a4928b638b24a4a14a79f',\n",
       "  'lastModified': '2021-09-09T21:34:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-de',\n",
       "  'downloads': 1125890,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/bert-base-nli-mean-tokens': {'modelId': 'sentence-transformers/bert-base-nli-mean-tokens',\n",
       "  'sha': '18fc720063106176044380e71bad038d01e821d1',\n",
       "  'lastModified': '2022-06-09T12:34:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/bert-base-nli-mean-tokens',\n",
       "  'downloads': 1112113,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-large': {'modelId': 'xlm-roberta-large',\n",
       "  'sha': 'b2a6150f8be56457baf80c74342cc424080260f0',\n",
       "  'lastModified': '2022-06-27T11:25:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'zh',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-large',\n",
       "  'downloads': 1064428,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'zh'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large-mnli': {'modelId': 'facebook/bart-large-mnli',\n",
       "  'sha': 'c626438eeca63a93bd6024b0a0fbf8b3c0c30d7b',\n",
       "  'lastModified': '2021-08-09T08:25:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:multi_nli',\n",
       "   'arxiv:1910.13461',\n",
       "   'arxiv:1909.00161',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'facebook/bart-large-mnli',\n",
       "  'downloads': 1036212,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 136,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'datasets': ['multi_nli']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-small': {'modelId': 't5-small',\n",
       "  'sha': 'd769bbacbd41c68c0f603aeab871b82e1081ffd8',\n",
       "  'lastModified': '2022-03-18T17:23:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-small',\n",
       "  'downloads': 1005992,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-base-squad2': {'modelId': 'deepset/roberta-base-squad2',\n",
       "  'sha': '4dc572bcd6c09de8738cded464ab9cd46a6b9c61',\n",
       "  'lastModified': '2022-06-08T10:38:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-base-squad2',\n",
       "  'downloads': 991395,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 78,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/codebert-base': {'modelId': 'microsoft/codebert-base',\n",
       "  'sha': '3b0952feddeffad0063f274080e3c23d75e7eb39',\n",
       "  'lastModified': '2022-02-11T19:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2002.08155',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/codebert-base',\n",
       "  'downloads': 872831,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ProsusAI/finbert': {'modelId': 'ProsusAI/finbert',\n",
       "  'sha': '5ea63b3d0c737ad6f06e061d9af36b1f7bbd1a4b',\n",
       "  'lastModified': '2022-06-03T06:34:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:1908.10063',\n",
       "   'transformers',\n",
       "   'financial-sentiment-analysis',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ProsusAI',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ProsusAI/finbert',\n",
       "  'downloads': 812206,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Stocks rallied and the British pound gained.'}],\n",
       "  'likes': 76,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
       "   'widget': [{'text': 'Stocks rallied and the British pound gained.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-base': {'modelId': 't5-base',\n",
       "  'sha': '686f1db74a0daa55de154507afe13c68843311cf',\n",
       "  'lastModified': '2022-03-18T17:22:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-base',\n",
       "  'downloads': 746393,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 50,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking': {'modelId': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       "  'sha': 'ab68bf4a4d55e7772b1fbea6441bdab72aaf949c',\n",
       "  'lastModified': '2021-09-23T13:45:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       "  'downloads': 684415,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/scibert_scivocab_uncased': {'modelId': 'allenai/scibert_scivocab_uncased',\n",
       "  'sha': '2ab156b969f2dbbd7ecc0080b78bc2cd272c4092',\n",
       "  'lastModified': '2021-05-19T11:41:40.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'allenai/scibert_scivocab_uncased',\n",
       "  'downloads': 672403,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/bart-large-cnn': {'modelId': 'facebook/bart-large-cnn',\n",
       "  'sha': '6d60e92a7a2bbf5051227d6fbeda503e53a26b68',\n",
       "  'lastModified': '2021-11-19T14:51:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'facebook/bart-large-cnn',\n",
       "  'downloads': 669575,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 63,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-fr': {'modelId': 'Helsinki-NLP/opus-mt-en-fr',\n",
       "  'sha': 'a8fbc1c711cb6263e8a20c5229b210cc05c57ff0',\n",
       "  'lastModified': '2021-09-09T21:35:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-fr',\n",
       "  'downloads': 666422,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-mpnet-base-v2': {'modelId': 'sentence-transformers/all-mpnet-base-v2',\n",
       "  'sha': 'cd48db1124ae80604e59187546e6d7708a7745ee',\n",
       "  'lastModified': '2022-07-02T21:42:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:s2orc',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/all-mpnet-base-v2',\n",
       "  'downloads': 601937,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['s2orc']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': {'modelId': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sha': 'b8ef00830037f9868450f778081ea683e900fe39',\n",
       "  'lastModified': '2022-06-15T18:43:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'downloads': 592144,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 41,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-base': {'modelId': 'facebook/bart-base',\n",
       "  'sha': '84358834e73de6a82c22cec1d90eb45ef4f6eba5',\n",
       "  'lastModified': '2022-06-03T09:43:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'facebook/bart-base',\n",
       "  'downloads': 591912,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-base-NER': {'modelId': 'dslim/bert-base-NER',\n",
       "  'sha': 'f7c2808a659015eeb8828f3f809a2f1be67a2446',\n",
       "  'lastModified': '2021-09-05T12:00:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-base-NER',\n",
       "  'downloads': 579289,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 57,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['conll2003'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-cased-distilled-squad': {'modelId': 'distilbert-base-cased-distilled-squad',\n",
       "  'sha': '626af3168b44d7bc30c62ce00f32b76b189137a5',\n",
       "  'lastModified': '2020-12-11T21:23:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-cased-distilled-squad',\n",
       "  'downloads': 577595,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'metrics': ['squad'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-sentiment-latest': {'modelId': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
       "  'sha': '5916057ce88cf0a408a195082b6c06d3dce12552',\n",
       "  'lastModified': '2022-03-31T09:47:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'english',\n",
       "   'arxiv:2202.03829',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
       "  'downloads': 576895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Covid cases are increasing fast!'}],\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'english',\n",
       "   'widget': [{'text': 'Covid cases are increasing fast!'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-cnn-12-6': {'modelId': 'sshleifer/distilbart-cnn-12-6',\n",
       "  'sha': 'a4f8f3ea906ed274767e9906dbaede7531d660ff',\n",
       "  'lastModified': '2021-06-14T07:51:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'sshleifer/distilbart-cnn-12-6',\n",
       "  'downloads': 569966,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 51,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-TinyBERT-L-2': {'modelId': 'cross-encoder/ms-marco-TinyBERT-L-2',\n",
       "  'sha': 'e9ed04745b2b19e8c4499360253ea5d5b41b5810',\n",
       "  'lastModified': '2021-08-05T08:39:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-TinyBERT-L-2',\n",
       "  'downloads': 557872,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-es': {'modelId': 'Helsinki-NLP/opus-mt-en-es',\n",
       "  'sha': 'c8b63c83f30e46417ce423a585f9b9e20e1b877d',\n",
       "  'lastModified': '2021-07-13T16:24:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-es',\n",
       "  'downloads': 540882,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'es'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/minilm-uncased-squad2': {'modelId': 'deepset/minilm-uncased-squad2',\n",
       "  'sha': 'c28e1a1d3d7d077ee7f4e63387f1a140f251daea',\n",
       "  'lastModified': '2021-10-21T12:19:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/minilm-uncased-squad2',\n",
       "  'downloads': 522179,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-multilingual-uncased': {'modelId': 'bert-base-multilingual-uncased',\n",
       "  'sha': '99406b9f2cfa046409626308a01da45a2a078f62',\n",
       "  'lastModified': '2021-05-18T16:19:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-multilingual-uncased',\n",
       "  'downloads': 508907,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased': {'modelId': 'bert-large-cased',\n",
       "  'sha': 'd9238236d8326ce4bc117132bb3b7e62e95f3a9a',\n",
       "  'lastModified': '2021-05-18T16:33:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased',\n",
       "  'downloads': 498013,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v2': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
       "  'sha': '896fbacdabde59de4cb8d75dea7b9bff6066015c',\n",
       "  'lastModified': '2022-06-15T19:24:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
       "  'downloads': 496518,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-large': {'modelId': 'flair/ner-english-large',\n",
       "  'sha': 'e2b1caabf7f9bac1e7829db73eac734df7e6ad7b',\n",
       "  'lastModified': '2021-05-08T15:36:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-large',\n",
       "  'downloads': 487895,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/deberta-v3-base': {'modelId': 'microsoft/deberta-v3-base',\n",
       "  'sha': '559062ad13d311b87b2c455e67dcd5f1c8f65111',\n",
       "  'lastModified': '2022-02-06T09:55:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-base',\n",
       "  'downloads': 486180,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'gpt2-medium': {'modelId': 'gpt2-medium',\n",
       "  'sha': 'db57b2fa2912ce3edbf35caa7aa70cadbb1e91b5',\n",
       "  'lastModified': '2021-05-21T09:17:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-medium',\n",
       "  'downloads': 470649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-question-generation-ap': {'modelId': 'mrm8488/t5-base-finetuned-question-generation-ap',\n",
       "  'sha': '7281097a2e51b1b57684b7de9999e32a0250dd83',\n",
       "  'lastModified': '2022-06-06T21:28:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-question-generation-ap',\n",
       "  'downloads': 461427,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'widget': [{'text': 'answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-cased': {'modelId': 'distilbert-base-cased',\n",
       "  'sha': '935ac13b473164bb9d578640e33d9f21144c365e',\n",
       "  'lastModified': '2020-12-11T21:23:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-cased',\n",
       "  'downloads': 457738,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/all-MiniLM-L12-v2': {'modelId': 'sentence-transformers/all-MiniLM-L12-v2',\n",
       "  'sha': '2e5bea94bf8c0163aeb984db01be29cfb79e8e6b',\n",
       "  'lastModified': '2022-06-21T14:55:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/all-MiniLM-L12-v2',\n",
       "  'downloads': 457493,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Helsinki-NLP/opus-mt-fr-en': {'modelId': 'Helsinki-NLP/opus-mt-fr-en',\n",
       "  'sha': '967b0840416a86ccf02573c8fedf9dd0e0b42fd6',\n",
       "  'lastModified': '2021-09-09T21:53:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'fr',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-fr-en',\n",
       "  'downloads': 440412,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mon nom est Wolfgang et je vis √† Berlin'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-large-NER': {'modelId': 'dslim/bert-large-NER',\n",
       "  'sha': '95c62bc0d4109bd97d0578e5ff482e6b84c2b8b9',\n",
       "  'lastModified': '2022-06-27T20:58:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-large-NER',\n",
       "  'downloads': 432664,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['conll2003'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bhadresh-savani/distilbert-base-uncased-emotion': {'modelId': 'bhadresh-savani/distilbert-base-uncased-emotion',\n",
       "  'sha': 'fff238f293a3a39108ff7345a1638e000be81b50',\n",
       "  'lastModified': '2021-09-15T17:40:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:emotion',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'emotion',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bhadresh-savani',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'bhadresh-savani/distilbert-base-uncased-emotion',\n",
       "  'downloads': 430839,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': 'https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4',\n",
       "   'tags': ['text-classification', 'emotion', 'pytorch'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['emotion'],\n",
       "   'metrics': ['Accuracy, F1 Score']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jplu/tf-xlm-roberta-base': {'modelId': 'jplu/tf-xlm-roberta-base',\n",
       "  'sha': 'bda462ca2c5c2b6851396df539f3eb01f08f3869',\n",
       "  'lastModified': '2020-12-11T21:48:00.000Z',\n",
       "  'tags': ['tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'jplu',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'jplu/tf-xlm-roberta-base',\n",
       "  'downloads': 428286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-base-960h': {'modelId': 'facebook/wav2vec2-base-960h',\n",
       "  'sha': '706111756296bc76512407a11e69526cf4e22aae',\n",
       "  'lastModified': '2022-06-30T00:05:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-base-960h',\n",
       "  'downloads': 423146,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 50,\n",
       "  'model-index': [{'name': 'wav2vec2-base-960h',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.4}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 8.6}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'model-index': [{'name': 'wav2vec2-base-960h',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.4}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 8.6}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'klue/bert-base': {'modelId': 'klue/bert-base',\n",
       "  'sha': '812449f1a6bc736e693db7aa0e513e5e90795a62',\n",
       "  'lastModified': '2021-10-20T15:23:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'klue/bert-base',\n",
       "  'downloads': 418977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-large': {'modelId': 'microsoft/DialoGPT-large',\n",
       "  'sha': '06a70b2c3cecc1f56edc9fdc58d2e90641c9ae9e',\n",
       "  'lastModified': '2021-05-23T09:06:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-large',\n",
       "  'downloads': 418526,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'daigo/bert-base-japanese-sentiment': {'modelId': 'daigo/bert-base-japanese-sentiment',\n",
       "  'sha': '51ac2d2c0a5645d77ca26078fc5f02c349fbb93d',\n",
       "  'lastModified': '2021-05-19T14:36:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ja',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'daigo',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'daigo/bert-base-japanese-sentiment',\n",
       "  'downloads': 418022,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'alvaroalon2/biobert_chemical_ner': {'modelId': 'alvaroalon2/biobert_chemical_ner',\n",
       "  'sha': 'f34bbf172059e8f6418c43efa925122feb46842b',\n",
       "  'lastModified': '2021-07-07T12:35:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'English',\n",
       "   'dataset:BC5CDR-chemicals',\n",
       "   'dataset:BC4CHEMD',\n",
       "   'transformers',\n",
       "   'NER',\n",
       "   'Biomedical',\n",
       "   'Chemicals',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'alvaroalon2',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'alvaroalon2/biobert_chemical_ner',\n",
       "  'downloads': 413385,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'English',\n",
       "   'tags': ['token-classification', 'NER', 'Biomedical', 'Chemicals'],\n",
       "   'datasets': ['BC5CDR-chemicals', 'BC4CHEMD'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-bert-base-uncased': {'modelId': 'princeton-nlp/sup-simcse-bert-base-uncased',\n",
       "  'sha': '2d82fab19ac3a73a20dd20333d27eb8a52d6e97f',\n",
       "  'lastModified': '2021-05-20T02:54:31.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/sup-simcse-bert-base-uncased',\n",
       "  'downloads': 401184,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/nli-distilroberta-base': {'modelId': 'cross-encoder/nli-distilroberta-base',\n",
       "  'sha': '99f096e70ef1fb038b8f0aecabc5a0f491684084',\n",
       "  'lastModified': '2021-08-05T08:40:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:snli',\n",
       "   'transformers',\n",
       "   'distilroberta-base',\n",
       "   'license:apache-2.0',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/nli-distilroberta-base',\n",
       "  'downloads': 392951,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilroberta-base'],\n",
       "   'datasets': ['multi_nli', 'snli'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ROMANCE': {'modelId': 'Helsinki-NLP/opus-mt-en-ROMANCE',\n",
       "  'sha': '92870a2f094c444064c7a568c25eef6971e07b03',\n",
       "  'lastModified': '2021-09-09T21:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'roa',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ROMANCE',\n",
       "  'downloads': 389575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-bert-wwm-ext': {'modelId': 'hfl/chinese-bert-wwm-ext',\n",
       "  'sha': '2a995a880017c60e4683869e817130d8af548486',\n",
       "  'lastModified': '2021-05-19T19:06:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-bert-wwm-ext',\n",
       "  'downloads': 386389,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jean-Baptiste/camembert-ner-with-dates': {'modelId': 'Jean-Baptiste/camembert-ner-with-dates',\n",
       "  'sha': '8c2d77a331733d26e0ca95a8f525e0ca3aa8e909',\n",
       "  'lastModified': '2021-08-30T12:55:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'Jean-Baptiste/camembert-ner-with-dates',\n",
       "  'downloads': 383067,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je m'appelle jean-baptiste et j'habite √† montr√©al depuis fevr 2012\"}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Je m'appelle jean-baptiste et j'habite √† montr√©al depuis fevr 2012\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-base-NER-uncased': {'modelId': 'dslim/bert-base-NER-uncased',\n",
       "  'sha': '1f52ebe0381dc9e285c0aa7c2971b350894f1efa',\n",
       "  'lastModified': '2021-05-19T16:10:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-base-NER-uncased',\n",
       "  'downloads': 382315,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert-base': {'modelId': 'camembert-base',\n",
       "  'sha': '482393b6198924f9da270b1aaf37d238aafca99b',\n",
       "  'lastModified': '2021-06-09T00:01:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1911.03894',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'camembert-base',\n",
       "  'downloads': 379731,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr', 'license': 'mit', 'datasets': ['oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad': {'modelId': 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       "  'sha': '242d9dbb66bb5033025196d5678907307f8fb098',\n",
       "  'lastModified': '2021-05-18T16:35:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       "  'downloads': 373885,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlmv2-base-uncased': {'modelId': 'microsoft/layoutlmv2-base-uncased',\n",
       "  'sha': '5c1ca07c23780c6dc123807def206ae9c4d59aca',\n",
       "  'lastModified': '2021-12-23T12:52:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'layoutlmv2',\n",
       "   'en',\n",
       "   'arxiv:2012.14740',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv2'},\n",
       "  'id': 'microsoft/layoutlmv2-base-uncased',\n",
       "  'downloads': 362553,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/layoutlm-base-uncased': {'modelId': 'microsoft/layoutlm-base-uncased',\n",
       "  'sha': 'ca841ce8d2f46b13b0ac3f635b8eb7d2e1d758d5',\n",
       "  'lastModified': '2021-08-11T05:27:42.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'layoutlm', 'arxiv:1912.13318', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlm'},\n",
       "  'id': 'microsoft/layoutlm-base-uncased',\n",
       "  'downloads': 362286,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'absa/classifier-rest-0.2': {'modelId': 'absa/classifier-rest-0.2',\n",
       "  'sha': 'd1712a46a6725043e6bb3b720dc314d2a523977c',\n",
       "  'lastModified': '2021-05-19T11:37:54.000Z',\n",
       "  'tags': ['tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'absa',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'absa/classifier-rest-0.2',\n",
       "  'downloads': 350136,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'TFAutoModel'}},\n",
       " 'microsoft/deberta-v3-large': {'modelId': 'microsoft/deberta-v3-large',\n",
       "  'sha': '360b9940401fa4d3411a0ca9f796631ec36f287a',\n",
       "  'lastModified': '2022-01-13T17:50:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-large',\n",
       "  'downloads': 342995,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 33,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'gpt2-xl': {'modelId': 'gpt2-xl',\n",
       "  'sha': '82bb8104f524eef7f49c81a339b62f5866ef95b6',\n",
       "  'lastModified': '2022-03-08T09:48:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-xl',\n",
       "  'downloads': 333346,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_bert': {'modelId': 'Rostlab/prot_bert',\n",
       "  'sha': '3d05bf06e79014892defacad82e0efd06e977ff6',\n",
       "  'lastModified': '2020-12-11T21:30:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fill-mask',\n",
       "   'protein',\n",
       "   'dataset:Uniref100',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'Rostlab/prot_bert',\n",
       "  'downloads': 322668,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['Uniref100']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'emilyalsentzer/Bio_Discharge_Summary_BERT': {'modelId': 'emilyalsentzer/Bio_Discharge_Summary_BERT',\n",
       "  'sha': 'affde836a50e4d333f15dae9270f5a856d59540b',\n",
       "  'lastModified': '2022-02-27T13:59:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.03323',\n",
       "   'arxiv:1901.08746',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'emilyalsentzer',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emilyalsentzer/Bio_Discharge_Summary_BERT',\n",
       "  'downloads': 321886,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['fill-mask'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-mpnet-base-v2': {'modelId': 'sentence-transformers/paraphrase-mpnet-base-v2',\n",
       "  'sha': '18df4b22cd35517843308534d066190182ff39ef',\n",
       "  'lastModified': '2022-06-15T19:23:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/paraphrase-mpnet-base-v2',\n",
       "  'downloads': 305105,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large': {'modelId': 'facebook/bart-large',\n",
       "  'sha': 'cb48c1365bd826bd521f650dc2e0940aee54720c',\n",
       "  'lastModified': '2022-06-03T10:00:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'facebook/bart-large',\n",
       "  'downloads': 296934,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilroberta-base': {'modelId': 'distilroberta-base',\n",
       "  'sha': 'ec58a5b7f760a8df0e81accb5abf0ab8368612f8',\n",
       "  'lastModified': '2021-05-20T22:47:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:openwebtext',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'distilroberta-base',\n",
       "  'downloads': 294170,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['openwebtext']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yiyanghkust/finbert-tone': {'modelId': 'yiyanghkust/finbert-tone',\n",
       "  'sha': '69507fb7dad65fd5ee96679690e6336211edc7a5',\n",
       "  'lastModified': '2022-06-09T12:05:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'financial-sentiment-analysis',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'yiyanghkust',\n",
       "  'config': {'architectures': ['BertForSequenceClassification']},\n",
       "  'id': 'yiyanghkust/finbert-tone',\n",
       "  'downloads': 287500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'growth is strong and we have plenty of liquidity'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
       "   'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification'}},\n",
       " 'joeddav/xlm-roberta-large-xnli': {'modelId': 'joeddav/xlm-roberta-large-xnli',\n",
       "  'sha': '9c1619b90a142cd2913190d80d5f488d6612f57e',\n",
       "  'lastModified': '2020-12-17T16:39:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:xnli',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'joeddav',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'joeddav/xlm-roberta-large-xnli',\n",
       "  'downloads': 287482,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '–ó–∞ –∫–æ–≥–æ –≤—ã –≥–æ–ª–æ—Å—É–µ—Ç–µ –≤ 2020 –≥–æ–¥—É?',\n",
       "    'candidate_labels': 'politique √©trang√®re, Europe, √©lections, affaires, politique',\n",
       "    'multi_class': True},\n",
       "   {'text': 'ŸÑŸÖŸÜ ÿ™ÿµŸàÿ™ ŸÅŸä 2020ÿü',\n",
       "    'candidate_labels': 'ÿßŸÑÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿÆÿßÿ±ÿ¨Ÿäÿ©, ÿ£Ÿàÿ±Ÿàÿ®ÿß, ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™, ÿßŸÑÿ£ÿπŸÖÿßŸÑ, ÿßŸÑÿ≥Ÿäÿßÿ≥ÿ©',\n",
       "    'multi_class': True},\n",
       "   {'text': \"2020'de kime oy vereceksiniz?\",\n",
       "    'candidate_labels': 'dƒ±≈ü politika, Avrupa, se√ßimler, ticaret, siyaset',\n",
       "    'multi_class': True}],\n",
       "  'likes': 44,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['text-classification', 'pytorch', 'tensorflow'],\n",
       "   'datasets': ['multi_nli', 'xnli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': '–ó–∞ –∫–æ–≥–æ –≤—ã –≥–æ–ª–æ—Å—É–µ—Ç–µ –≤ 2020 –≥–æ–¥—É?',\n",
       "     'candidate_labels': 'politique √©trang√®re, Europe, √©lections, affaires, politique',\n",
       "     'multi_class': True},\n",
       "    {'text': 'ŸÑŸÖŸÜ ÿ™ÿµŸàÿ™ ŸÅŸä 2020ÿü',\n",
       "     'candidate_labels': 'ÿßŸÑÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿÆÿßÿ±ÿ¨Ÿäÿ©, ÿ£Ÿàÿ±Ÿàÿ®ÿß, ÿßŸÑÿßŸÜÿ™ÿÆÿßÿ®ÿßÿ™, ÿßŸÑÿ£ÿπŸÖÿßŸÑ, ÿßŸÑÿ≥Ÿäÿßÿ≥ÿ©',\n",
       "     'multi_class': True},\n",
       "    {'text': \"2020'de kime oy vereceksiniz?\",\n",
       "     'candidate_labels': 'dƒ±≈ü politika, Avrupa, se√ßimler, ticaret, siyaset',\n",
       "     'multi_class': True}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gpt2-large': {'modelId': 'gpt2-large',\n",
       "  'sha': '897ad545cc0303ac6a2a8b7105c48aaca02f5294',\n",
       "  'lastModified': '2021-09-08T15:44:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-large',\n",
       "  'downloads': 281109,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/m2m100_418M': {'modelId': 'facebook/m2m100_418M',\n",
       "  'sha': '441fd5182f1298d7e39f34013ac0b905f8ff4429',\n",
       "  'lastModified': '2022-05-26T22:26:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'm2m_100',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'ast',\n",
       "   'az',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'ilo',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'lb',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ns',\n",
       "   'oc',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'arxiv:2010.11125',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['M2M100ForConditionalGeneration'],\n",
       "   'model_type': 'm2m_100'},\n",
       "  'id': 'facebook/m2m100_418M',\n",
       "  'downloads': 278355,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'ast',\n",
       "    'az',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'ilo',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'lb',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ns',\n",
       "    'oc',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'license': 'mit',\n",
       "   'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-roberta-base': {'modelId': 'cross-encoder/stsb-roberta-base',\n",
       "  'sha': '90a6796bd3c504b63351dad78c76ffb40e3d6e5a',\n",
       "  'lastModified': '2021-08-05T08:41:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-roberta-base',\n",
       "  'downloads': 277780,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'finiteautomata/beto-sentiment-analysis': {'modelId': 'finiteautomata/beto-sentiment-analysis',\n",
       "  'sha': '2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38',\n",
       "  'lastModified': '2022-06-22T13:46:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'finiteautomata/beto-sentiment-analysis',\n",
       "  'downloads': 273730,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-base-v1': {'modelId': 'albert-base-v1',\n",
       "  'sha': 'aeffd769076a5c4f83b2546aea99ca45a15a5da4',\n",
       "  'lastModified': '2021-01-13T15:08:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-base-v1',\n",
       "  'downloads': 269242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-small-discriminator': {'modelId': 'google/electra-small-discriminator',\n",
       "  'sha': '153f486d928bcfc213932f8fc91fc2e3c41af769',\n",
       "  'lastModified': '2021-04-29T15:24:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-small-discriminator',\n",
       "  'downloads': 265244,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlptown/bert-base-multilingual-uncased-sentiment': {'modelId': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
       "  'sha': 'e06857fdb0325a7798a8fc361b417dfeec3a3b98',\n",
       "  'lastModified': '2022-04-18T16:46:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'nl',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'nlptown',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
       "  'downloads': 259805,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 54,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'nl', 'de', 'fr', 'it', 'es'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-common_gen': {'modelId': 'mrm8488/t5-base-finetuned-common_gen',\n",
       "  'sha': '5c3010b4532b7834039c65580e688e9656626835',\n",
       "  'lastModified': '2021-09-24T08:52:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:common_gen',\n",
       "   'arxiv:1910.10683',\n",
       "   'arxiv:1911.03705',\n",
       "   'transformers',\n",
       "   'common sense',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-common_gen',\n",
       "  'downloads': 258993,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'tree plant ground hole dig'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['common sense'],\n",
       "   'datasets': ['common_gen'],\n",
       "   'widget': [{'text': 'tree plant ground hole dig'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-tiny': {'modelId': 'prajjwal1/bert-tiny',\n",
       "  'sha': '6f75de8b60a9f8a2fdf7b69cbd86d9e64bcb3837',\n",
       "  'lastModified': '2021-10-27T18:29:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-tiny',\n",
       "  'downloads': 256147,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/bert_uncased_L-2_H-128_A-2': {'modelId': 'google/bert_uncased_L-2_H-128_A-2',\n",
       "  'sha': '1ae49ff827beda5996998802695c4cac8e9932c6',\n",
       "  'lastModified': '2021-05-19T17:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-2_H-128_A-2',\n",
       "  'downloads': 254004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'xlnet-base-cased': {'modelId': 'xlnet-base-cased',\n",
       "  'sha': '593a21e8b79948a7f952811aa44f37d76e23d586',\n",
       "  'lastModified': '2021-09-16T09:43:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1906.08237',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'xlnet-base-cased',\n",
       "  'downloads': 249967,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dalle-mini/vqgan_imagenet_f16_16384': {'modelId': 'dalle-mini/vqgan_imagenet_f16_16384',\n",
       "  'sha': '9f990ec03ea054204706013e2176bbb498ebc387',\n",
       "  'lastModified': '2022-03-01T17:28:10.000Z',\n",
       "  'tags': ['jax', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dalle-mini',\n",
       "  'config': {'architectures': ['del']},\n",
       "  'id': 'dalle-mini/vqgan_imagenet_f16_16384',\n",
       "  'downloads': 248552,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'del'}},\n",
       " 'deepset/bert-large-uncased-whole-word-masking-squad2': {'modelId': 'deepset/bert-large-uncased-whole-word-masking-squad2',\n",
       "  'sha': '8ebfa4a417e72eb0307ef9d6b2b2bf742b5ca879',\n",
       "  'lastModified': '2021-05-19T15:28:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/bert-large-uncased-whole-word-masking-squad2',\n",
       "  'downloads': 234038,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-ctx_encoder-single-nq-base': {'modelId': 'facebook/dpr-ctx_encoder-single-nq-base',\n",
       "  'sha': 'aa2a11a692b50b73fa245d59e49be796eefd888f',\n",
       "  'lastModified': '2020-11-25T16:58:35.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-ctx_encoder-single-nq-base',\n",
       "  'downloads': 234037,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'seyonec/PubChem10M_SMILES_BPE_450k': {'modelId': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
       "  'sha': 'c18fccd09b3326bf2d4633412c256d7db872156d',\n",
       "  'lastModified': '2021-05-20T21:02:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'seyonec',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
       "  'downloads': 228396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dccuchile/bert-base-spanish-wwm-uncased': {'modelId': 'dccuchile/bert-base-spanish-wwm-uncased',\n",
       "  'sha': '767afcc9ffdf900341128e9e0bfe44d522461c51',\n",
       "  'lastModified': '2022-05-31T15:02:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:1904.09077',\n",
       "   'arxiv:1906.01502',\n",
       "   'arxiv:1812.10464',\n",
       "   'arxiv:1901.07291',\n",
       "   'arxiv:1904.02099',\n",
       "   'arxiv:1906.01569',\n",
       "   'arxiv:1908.11828',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dccuchile',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dccuchile/bert-base-spanish-wwm-uncased',\n",
       "  'downloads': 227818,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Mi nombre es [MASK] y vivo en Nueva York.'},\n",
       "   {'text': 'El espa√±ol es un idioma muy [MASK] en el mundo.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['masked-lm']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'openai/clip-vit-large-patch14': {'modelId': 'openai/clip-vit-large-patch14',\n",
       "  'sha': '0993c71e8ad62658387de2714a69f723ddfffacb',\n",
       "  'lastModified': '2022-03-14T18:01:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-large-patch14',\n",
       "  'downloads': 226511,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'roberta-large-mnli': {'modelId': 'roberta-large-mnli',\n",
       "  'sha': '130fb28e151bc632feba36b7417d01ad6dd6c424',\n",
       "  'lastModified': '2021-05-20T19:32:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'roberta-large-mnli',\n",
       "  'downloads': 223665,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. </s></s> I love you.'}],\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'widget': [{'text': 'I like you. </s></s> I love you.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'airesearch/wangchanberta-base-att-spm-uncased': {'modelId': 'airesearch/wangchanberta-base-att-spm-uncased',\n",
       "  'sha': 'abe46f39cf2c911a6ad5ec8299bdf7503edc95e4',\n",
       "  'lastModified': '2022-02-16T14:42:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'th',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1801.06146',\n",
       "   'arxiv:1808.06226',\n",
       "   'arxiv:2101.09635',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'airesearch',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'airesearch/wangchanberta-base-att-spm-uncased',\n",
       "  'downloads': 223518,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥<mask>‡∏°‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô<pad>'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'th',\n",
       "   'widget': [{'text': '‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡πà‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏¢‡∏≤‡∏ô‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥<mask>‡∏°‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô<pad>'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv02': {'modelId': 'aubmindlab/bert-base-arabertv02',\n",
       "  'sha': 'b214583e9b05a7bbc024d58daeb54a1b2a2997a0',\n",
       "  'lastModified': '2022-04-06T15:24:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv02',\n",
       "  'downloads': 220476,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' ÿπÿßÿµŸÖÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled'],\n",
       "   'widget': [{'text': ' ÿπÿßÿµŸÖÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-cased': {'modelId': 'dbmdz/bert-base-italian-cased',\n",
       "  'sha': 'bcecdad25ce7cdd99c58c4e504ab97e6ff7222cf',\n",
       "  'lastModified': '2021-05-19T14:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-cased',\n",
       "  'downloads': 220307,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma √® la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita √® [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-question_encoder-single-nq-base': {'modelId': 'facebook/dpr-question_encoder-single-nq-base',\n",
       "  'sha': '7fee7988e53c713da8323b184f6015d47861a1bf',\n",
       "  'lastModified': '2020-11-25T16:59:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-question_encoder-single-nq-base',\n",
       "  'downloads': 220096,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-distilroberta-base-v1': {'modelId': 'sentence-transformers/paraphrase-distilroberta-base-v1',\n",
       "  'sha': 'de91d53e03a544451c0e18312391a3f279f7f4ef',\n",
       "  'lastModified': '2022-06-15T20:03:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-distilroberta-base-v1',\n",
       "  'downloads': 218993,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-xsum': {'modelId': 'google/pegasus-xsum',\n",
       "  'sha': 'a0aa5531c00f59a32a167b75130805098b046f9c',\n",
       "  'lastModified': '2021-09-14T07:25:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-xsum',\n",
       "  'downloads': 218500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 39,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v2-xlarge': {'modelId': 'microsoft/deberta-v2-xlarge',\n",
       "  'sha': '30597019711d3531f994d1e21defffd0d8cd55ab',\n",
       "  'lastModified': '2022-01-13T17:21:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v2-xlarge',\n",
       "  'downloads': 217009,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'nlpaueb/legal-bert-small-uncased': {'modelId': 'nlpaueb/legal-bert-small-uncased',\n",
       "  'sha': '0e23f7a9a39f59768ea7e09766d8ee308580fb17',\n",
       "  'lastModified': '2022-04-28T14:43:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/legal-bert-small-uncased',\n",
       "  'downloads': 216773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'imone/pangu_2_6B': {'modelId': 'imone/pangu_2_6B',\n",
       "  'sha': 'f6185c345ee59518384a463350bc834ace46e557',\n",
       "  'lastModified': '2021-12-13T06:34:22.000Z',\n",
       "  'tags': ['pytorch', 'gpt_pangu', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'imone',\n",
       "  'config': {'architectures': ['GPTPanguForCausalLM'],\n",
       "   'model_type': 'gpt_pangu'},\n",
       "  'id': 'imone/pangu_2_6B',\n",
       "  'downloads': 216405,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation'}},\n",
       " 'microsoft/deberta-base': {'modelId': 'microsoft/deberta-base',\n",
       "  'sha': '7d4c0126b06bd59dccd3e48e467ed11e37b77f3f',\n",
       "  'lastModified': '2022-01-13T13:56:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'deberta',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-base',\n",
       "  'downloads': 212662,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uer/gpt2-chinese-cluecorpussmall': {'modelId': 'uer/gpt2-chinese-cluecorpussmall',\n",
       "  'sha': 'eac5b94f575ae754c1d39100da152369a7699e6c',\n",
       "  'lastModified': '2022-02-20T04:46:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 320}}},\n",
       "  'id': 'uer/gpt2-chinese-cluecorpussmall',\n",
       "  'downloads': 209231,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ËøôÊòØÂæà‰πÖ‰πãÂâçÁöÑ‰∫ãÊÉÖ‰∫Ü'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': 'ËøôÊòØÂæà‰πÖ‰πãÂâçÁöÑ‰∫ãÊÉÖ‰∫Ü'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-xlm-r-multilingual-v1': {'modelId': 'sentence-transformers/paraphrase-xlm-r-multilingual-v1',\n",
       "  'sha': '50f7fa9e273db3db51beceacc1b111e4a1a31d34',\n",
       "  'lastModified': '2022-06-15T19:25:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-xlm-r-multilingual-v1',\n",
       "  'downloads': 207323,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 31,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cahya/t5-base-indonesian-summarization-cased': {'modelId': 'cahya/t5-base-indonesian-summarization-cased',\n",
       "  'sha': '3afd080677efb3978dfce95a19324d91caff3064',\n",
       "  'lastModified': '2021-06-23T12:05:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'id',\n",
       "   'dataset:id_liputan6',\n",
       "   'transformers',\n",
       "   'pipeline:summarization',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'cahya',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'cahya/t5-base-indonesian-summarization-cased',\n",
       "  'downloads': 207288,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['pipeline:summarization', 'summarization', 't5'],\n",
       "   'datasets': ['id_liputan6']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-tiny-chinese': {'modelId': 'ckiplab/albert-tiny-chinese',\n",
       "  'sha': 'd1edf497761caf4fdd83d2c4488132a8c56f9e3c',\n",
       "  'lastModified': '2022-05-10T03:28:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-tiny-chinese',\n",
       "  'downloads': 205977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-base': {'modelId': 'google/t5-v1_1-base',\n",
       "  'sha': '650d7745bf1e502d6949b22cc19155cd656d3d4e',\n",
       "  'lastModified': '2021-06-23T01:54:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-base',\n",
       "  'downloads': 205906,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/specter': {'modelId': 'allenai/specter',\n",
       "  'sha': 'c15597dc3bf1f00444f1c5a59c9bb80c93499635',\n",
       "  'lastModified': '2022-06-25T16:04:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:SciDocs',\n",
       "   'arxiv:2004.07180',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allenai/specter',\n",
       "  'downloads': 204047,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://camo.githubusercontent.com/7d080b7a769f7fdf64ac0ebeb47b039cb50be35287e3071f9d633f0fe33e7596/68747470733a2f2f692e6962622e636f2f33544331576d472f737065637465722d6c6f676f2d63726f707065642e706e67',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['SciDocs'],\n",
       "   'metrics': ['F1', 'accuracy', 'map', 'ndcg']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'emilyalsentzer/Bio_ClinicalBERT': {'modelId': 'emilyalsentzer/Bio_ClinicalBERT',\n",
       "  'sha': '41943bf7f983007123c758373c5246305cc536ec',\n",
       "  'lastModified': '2022-02-27T13:59:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.03323',\n",
       "   'arxiv:1901.08746',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'emilyalsentzer',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emilyalsentzer/Bio_ClinicalBERT',\n",
       "  'downloads': 199851,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['fill-mask'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'flair/ner-english-ontonotes-large': {'modelId': 'flair/ner-english-ontonotes-large',\n",
       "  'sha': '4ffb3596f4359f0c8799ea15bbf5dbb3b0915a53',\n",
       "  'lastModified': '2021-05-08T15:35:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes-large',\n",
       "  'downloads': 199677,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George won 1 dollar while watching Game of Thrones.'}],\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George won 1 dollar while watching Game of Thrones.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Jean-Baptiste/roberta-large-ner-english': {'modelId': 'Jean-Baptiste/roberta-large-ner-english',\n",
       "  'sha': 'c272484a77f6bacd3569d32936fda04555fb4006',\n",
       "  'lastModified': '2022-07-04T15:02:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['RobertaForTokenClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'Jean-Baptiste/roberta-large-ner-english',\n",
       "  'downloads': 196068,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is jean-baptiste and I live in montreal'},\n",
       "   {'text': 'My name is clara and I live in berkeley, california.'},\n",
       "   {'text': 'My name is wolfgang and I live in berlin'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'My name is jean-baptiste and I live in montreal'},\n",
       "    {'text': 'My name is clara and I live in berkeley, california.'},\n",
       "    {'text': 'My name is wolfgang and I live in berlin'}],\n",
       "   'train-eval-index': [{'config': 'conll2003',\n",
       "     'task': 'token-classification',\n",
       "     'task_id': 'entity_extraction',\n",
       "     'splits': {'eval_split': 'validation'},\n",
       "     'col_mapping': {'tokens': 'tokens', 'ner_tags': 'tags'}}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer': {'modelId': 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
       "  'sha': '17bdd111a862ec99279be149fc9efa4f9122bcc1',\n",
       "  'lastModified': '2022-06-16T18:13:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:STSbenchmark',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'search',\n",
       "   'roberta',\n",
       "   'xlm-r-distilroberta-base-paraphrase-v1',\n",
       "   'paraphrase',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'T-Systems-onsite',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
       "  'downloads': 193158,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentence_embedding',\n",
       "    'search',\n",
       "    'pytorch',\n",
       "    'xlm-roberta',\n",
       "    'roberta',\n",
       "    'xlm-r-distilroberta-base-paraphrase-v1',\n",
       "    'paraphrase'],\n",
       "   'datasets': ['STSbenchmark'],\n",
       "   'metrics': ['Spearman‚Äôs rank correlation', 'cosine similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-11b': {'modelId': 't5-11b',\n",
       "  'sha': '952e4a1e7643d8e4dbebcae9b4d4f8f7507b523b',\n",
       "  'lastModified': '2022-03-18T17:24:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-11b',\n",
       "  'downloads': 192660,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-400M-distill': {'modelId': 'facebook/blenderbot-400M-distill',\n",
       "  'sha': 'a2084cb58dd4810f45302724dd07c68051fe9ed3',\n",
       "  'lastModified': '2022-05-16T19:39:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:2004.13637',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-400M-distill',\n",
       "  'downloads': 190103,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/unsup-simcse-bert-base-uncased': {'modelId': 'princeton-nlp/unsup-simcse-bert-base-uncased',\n",
       "  'sha': '6504ae026e02a1464538d443b15e36afc318e034',\n",
       "  'lastModified': '2021-05-20T02:57:45.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/unsup-simcse-bert-base-uncased',\n",
       "  'downloads': 188550,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-base': {'modelId': 'vinai/bertweet-base',\n",
       "  'sha': 'f9365bd897a63399564af0859aa981deb6deb0f3',\n",
       "  'lastModified': '2022-06-08T04:43:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-base',\n",
       "  'downloads': 188493,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-v1.1': {'modelId': 'dmis-lab/biobert-v1.1',\n",
       "  'sha': '551ca18efd7f052c8dfa0b01c94c2a8e68bc5488',\n",
       "  'lastModified': '2021-05-19T16:03:17.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biobert-v1.1',\n",
       "  'downloads': 184048,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/segmentation': {'modelId': 'pyannote/segmentation',\n",
       "  'sha': '56d5828a2dc341a294417c01e7dcdd0da4cf461a',\n",
       "  'lastModified': '2022-03-23T09:25:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'arxiv:2104.04045',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-model',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-segmentation',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'resegmentation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'voice-activity-detection',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/segmentation',\n",
       "  'downloads': 183970,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-model',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-segmentation',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'resegmentation'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-6-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "  'sha': 'b2cfda50a1a9fc7919e7444afbb52610d268af92',\n",
       "  'lastModified': '2021-08-05T08:39:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "  'downloads': 183868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid': {'modelId': 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid',\n",
       "  'sha': 'f9c8e9f03396500a107dbe97024d7efa23f57e69',\n",
       "  'lastModified': '2022-07-04T09:04:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sst2',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'echarlaix',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid',\n",
       "  'downloads': 182079,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['text-classification'],\n",
       "   'datasets': ['sst2'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bhadresh-savani/bert-base-go-emotion': {'modelId': 'bhadresh-savani/bert-base-go-emotion',\n",
       "  'sha': '6ecebb2840243665ab089020504c52e086862848',\n",
       "  'lastModified': '2021-11-29T10:43:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'dataset:go_emotions',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'go-emotion',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bhadresh-savani',\n",
       "  'config': {'architectures': ['DistilBertForMultilabelSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bhadresh-savani/bert-base-go-emotion',\n",
       "  'downloads': 181018,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': 'https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4',\n",
       "   'tags': ['text-classification', 'go-emotion', 'pytorch'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['go_emotions'],\n",
       "   'metrics': ['Accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'DistilBertForMultilabelSequenceClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'j-hartmann/emotion-english-distilroberta-base': {'modelId': 'j-hartmann/emotion-english-distilroberta-base',\n",
       "  'sha': 'd23807173703d44b48d60ca252664f60d0d46563',\n",
       "  'lastModified': '2022-06-09T12:43:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'distilroberta',\n",
       "   'sentiment',\n",
       "   'emotion',\n",
       "   'twitter',\n",
       "   'reddit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'j-hartmann',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'j-hartmann/emotion-english-distilroberta-base',\n",
       "  'downloads': 179816,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Oh wow. I didn't know that.\"},\n",
       "   {'text': 'This movie always makes me cry..'},\n",
       "   {'text': 'Oh Happy Day'}],\n",
       "  'likes': 29,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['distilroberta', 'sentiment', 'emotion', 'twitter', 'reddit'],\n",
       "   'widget': [{'text': \"Oh wow. I didn't know that.\"},\n",
       "    {'text': 'This movie always makes me cry..'},\n",
       "    {'text': 'Oh Happy Day'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_paraphraser_on_T5': {'modelId': 'prithivida/parrot_paraphraser_on_T5',\n",
       "  'sha': '9f32aa1e456e8e8a90d97e8673365f3090fa49fa',\n",
       "  'lastModified': '2021-05-18T07:53:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'prithivida/parrot_paraphraser_on_T5',\n",
       "  'downloads': 175057,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/bert-base-cased-goemotions-original': {'modelId': 'monologg/bert-base-cased-goemotions-original',\n",
       "  'sha': '13c44c849132f82bb61188d909a574badffb27a3',\n",
       "  'lastModified': '2021-05-19T23:48:33.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['BertForMultiLabelClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'monologg/bert-base-cased-goemotions-original',\n",
       "  'downloads': 173202,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'BertForMultiLabelClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-roberta-base': {'modelId': 'google/bigbird-roberta-base',\n",
       "  'sha': '5a145f7852cba9bd431386a58137bf8a29903b90',\n",
       "  'lastModified': '2021-06-02T14:30:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'big_bird',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:cc_news',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdForPreTraining'],\n",
       "   'model_type': 'big_bird'},\n",
       "  'id': 'google/bigbird-roberta-base',\n",
       "  'downloads': 172226,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'cc_news']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased-whole-word-masking': {'modelId': 'bert-large-uncased-whole-word-masking',\n",
       "  'sha': '90d3333009848fae4860a5338419d17f70be940c',\n",
       "  'lastModified': '2021-05-18T16:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased-whole-word-masking',\n",
       "  'downloads': 172022,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-large': {'modelId': 't5-large',\n",
       "  'sha': '0c716e16629cc307ee64f3870869c1227c24fef1',\n",
       "  'lastModified': '2022-03-18T17:24:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-large',\n",
       "  'downloads': 171229,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'neuralmind/bert-base-portuguese-cased': {'modelId': 'neuralmind/bert-base-portuguese-cased',\n",
       "  'sha': '94d69c95f98f7d5b2a8700c420230ae10def0baa',\n",
       "  'lastModified': '2022-06-14T14:37:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'neuralmind',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'neuralmind/bert-base-portuguese-cased',\n",
       "  'downloads': 167156,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['bert', 'pytorch'],\n",
       "   'datasets': ['brWaC']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-base-cased': {'modelId': 'allegro/herbert-base-cased',\n",
       "  'sha': '50e33e0567be0c0b313832314c586e3df0dc2297',\n",
       "  'lastModified': '2022-06-09T11:36:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'pl',\n",
       "   'transformers',\n",
       "   'herbert',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allegro/herbert-base-cased',\n",
       "  'downloads': 164340,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl', 'tags': ['herbert'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-large-cased-finetuned-conll03-english': {'modelId': 'dbmdz/bert-large-cased-finetuned-conll03-english',\n",
       "  'sha': 'f2482bf01f5da0f0eb8e183ffd8cc3885aa90b14',\n",
       "  'lastModified': '2021-05-19T15:17:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-large-cased-finetuned-conll03-english',\n",
       "  'downloads': 159877,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/pos-english-fast': {'modelId': 'flair/pos-english-fast',\n",
       "  'sha': '78bf413a631e2de4cb977e1f2794295d981e4c13',\n",
       "  'lastModified': '2021-03-02T22:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/pos-english-fast',\n",
       "  'downloads': 157175,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'I love Berlin.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'I love Berlin.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'finiteautomata/bertweet-base-sentiment-analysis': {'modelId': 'finiteautomata/bertweet-base-sentiment-analysis',\n",
       "  'sha': 'cf6b0f60e84096e077c171fe3176093674370291',\n",
       "  'lastModified': '2022-06-23T13:01:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'finiteautomata/bertweet-base-sentiment-analysis',\n",
       "  'downloads': 153824,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neo-125M': {'modelId': 'EleutherAI/gpt-neo-125M',\n",
       "  'sha': 'b559e35121e91087f94903c07213d208d2412f68',\n",
       "  'lastModified': '2021-12-31T13:46:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'], 'model_type': 'gpt_neo'},\n",
       "  'id': 'EleutherAI/gpt-neo-125M',\n",
       "  'downloads': 149642,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality': {'modelId': 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality',\n",
       "  'sha': '443d721ecccee1cb38cce6f50cfd6c15e44e6ea0',\n",
       "  'lastModified': '2021-09-21T05:21:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality',\n",
       "  'downloads': 148487,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'oliverguhr/fullstop-punctuation-multilang-large': {'modelId': 'oliverguhr/fullstop-punctuation-multilang-large',\n",
       "  'sha': '4740a83c496dc2416c0cf8ae3c6572dfb6851228',\n",
       "  'lastModified': '2022-06-09T11:51:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'dataset:wmt/europarl',\n",
       "   'transformers',\n",
       "   'punctuation prediction',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'oliverguhr/fullstop-punctuation-multilang-large',\n",
       "  'downloads': 138173,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Ho sentito che ti sei laureata il che mi fa molto piacere',\n",
       "    'example_title': 'Italian'},\n",
       "   {'text': 'Tous les matins vers quatre heures mon p√®re ouvrait la porte de ma chambre',\n",
       "    'example_title': 'French'},\n",
       "   {'text': 'Ist das eine Frage Frau M√ºller', 'example_title': 'German'},\n",
       "   {'text': 'Yet she blushed as if with guilt when Cynthia reading her thoughts said to her one day Molly youre very glad to get rid of us are not you',\n",
       "    'example_title': 'English'}],\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de', 'fr', 'it'],\n",
       "   'tags': ['punctuation prediction', 'punctuation'],\n",
       "   'datasets': 'wmt/europarl',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Ho sentito che ti sei laureata il che mi fa molto piacere',\n",
       "     'example_title': 'Italian'},\n",
       "    {'text': 'Tous les matins vers quatre heures mon p√®re ouvrait la porte de ma chambre',\n",
       "     'example_title': 'French'},\n",
       "    {'text': 'Ist das eine Frage Frau M√ºller', 'example_title': 'German'},\n",
       "    {'text': 'Yet she blushed as if with guilt when Cynthia reading her thoughts said to her one day Molly youre very glad to get rid of us are not you',\n",
       "     'example_title': 'English'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huggingface/CodeBERTa-small-v1': {'modelId': 'huggingface/CodeBERTa-small-v1',\n",
       "  'sha': 'e93b5898cff07f03f1c1c09cde284d1b85962363',\n",
       "  'lastModified': '2022-06-27T15:48:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'code',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'huggingface',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'huggingface/CodeBERTa-small-v1',\n",
       "  'downloads': 137341,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'code',\n",
       "   'thumbnail': 'https://cdn-media.huggingface.co/CodeBERTa/CodeBERTa.png',\n",
       "   'datasets': ['code_search_net']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-960h': {'modelId': 'facebook/wav2vec2-large-960h',\n",
       "  'sha': 'bdeaacdf88f7a155f50a2704bc967aa81fbbb2ab',\n",
       "  'lastModified': '2022-04-05T16:40:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-960h',\n",
       "  'downloads': 136715,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'uer/albert-base-chinese-cluecorpussmall': {'modelId': 'uer/albert-base-chinese-cluecorpussmall',\n",
       "  'sha': 'f26cf2e20801f5f9b4974eda54f4155159de7104',\n",
       "  'lastModified': '2022-02-20T08:46:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'uer/albert-base-chinese-cluecorpussmall',\n",
       "  'downloads': 134960,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰∏≠ÂõΩÁöÑÈ¶ñÈÉΩÊòØ[MASK]‰∫¨'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': '‰∏≠ÂõΩÁöÑÈ¶ñÈÉΩÊòØ[MASK]‰∫¨'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cmarkea/distilcamembert-base-sentiment': {'modelId': 'cmarkea/distilcamembert-base-sentiment',\n",
       "  'sha': 'b7804e295dc3cf2aa8ce8cff83f22e0bdd249558',\n",
       "  'lastModified': '2022-05-24T15:56:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'fr',\n",
       "   'dataset:amazon_reviews_multi',\n",
       "   'dataset:allocine',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base-sentiment',\n",
       "  'downloads': 134584,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je pensais lire un livre nul, mais finalement je l'ai trouv√© super !\"},\n",
       "   {'text': \"Cette banque est tr√®s bien, mais elle n'offre pas les services de paiements sans contact.\"},\n",
       "   {'text': 'Cette banque est tr√®s bien et elle offre en plus les services de paiements sans contact.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['amazon_reviews_multi', 'allocine'],\n",
       "   'widget': [{'text': \"Je pensais lire un livre nul, mais finalement je l'ai trouv√© super !\"},\n",
       "    {'text': \"Cette banque est tr√®s bien, mais elle n'offre pas les services de paiements sans contact.\"},\n",
       "    {'text': 'Cette banque est tr√®s bien et elle offre en plus les services de paiements sans contact.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tuner007/pegasus_paraphrase': {'modelId': 'tuner007/pegasus_paraphrase',\n",
       "  'sha': '0159e2949ca73657a2f1329898f51b7bb53b9ab2',\n",
       "  'lastModified': '2021-03-22T21:11:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'paraphrasing',\n",
       "   'seq2seq',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'tuner007',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'tuner007/pegasus_paraphrase',\n",
       "  'downloads': 133980,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 50,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['pegasus', 'paraphrasing', 'seq2seq']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/allenai-specter': {'modelId': 'sentence-transformers/allenai-specter',\n",
       "  'sha': '29f9f45ff2a85fe9dfe8ce2cef3d8ec4e65c5f37',\n",
       "  'lastModified': '2022-06-15T21:31:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/allenai-specter',\n",
       "  'downloads': 133673,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-one-to-many-mmt': {'modelId': 'facebook/mbart-large-50-one-to-many-mmt',\n",
       "  'sha': '3cc64aaf129efb58cdc6345618b39ce776d888b4',\n",
       "  'lastModified': '2022-05-26T22:28:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50-one-to-many-mmt',\n",
       "  'downloads': 132999,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-2-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
       "  'sha': 'f4db9595e5310ba9e0cfbf391154583933b533eb',\n",
       "  'lastModified': '2021-08-05T08:39:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
       "  'downloads': 131564,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/hubert-large-ls960-ft': {'modelId': 'facebook/hubert-large-ls960-ft',\n",
       "  'sha': 'ece5fabbf034c1073acae96d5401b25be96709d8',\n",
       "  'lastModified': '2022-05-24T10:43:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'hubert',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:libri-light',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2106.07447',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['HubertForCTC'], 'model_type': 'hubert'},\n",
       "  'id': 'facebook/hubert-large-ls960-ft',\n",
       "  'downloads': 131404,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': [{'name': 'hubert-large-ls960-ft',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['libri-light', 'librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'hubert-large-ls960-ft',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-base-discriminator': {'modelId': 'google/electra-base-discriminator',\n",
       "  'sha': '1b48ef100dac4676d84125a8a7b7ab7c51e00386',\n",
       "  'lastModified': '2021-04-30T07:33:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-base-discriminator',\n",
       "  'downloads': 130555,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese': {'modelId': 'cl-tohoku/bert-base-japanese',\n",
       "  'sha': '5dc6dbba88a42d21da3b71025c109c42462307f2',\n",
       "  'lastModified': '2021-09-23T13:45:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese',\n",
       "  'downloads': 130402,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flexudy/t5-base-multi-sentence-doctor': {'modelId': 'flexudy/t5-base-multi-sentence-doctor',\n",
       "  'sha': '85ef24d555e2e6cabd5ce8264e9ce1627c406bad',\n",
       "  'lastModified': '2020-12-11T23:33:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'flexudy',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'flexudy/t5-base-multi-sentence-doctor',\n",
       "  'downloads': 129272,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5_sentence_paraphraser': {'modelId': 'ramsrigouthamg/t5_sentence_paraphraser',\n",
       "  'sha': '6887902ca669ce785cb8a01b3425e843011bc110',\n",
       "  'lastModified': '2021-06-23T13:47:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5_sentence_paraphraser',\n",
       "  'downloads': 127843,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/afriberta_large': {'modelId': 'castorini/afriberta_large',\n",
       "  'sha': 'e74edb9488208f8a2aeb69be4c16d179ab385564',\n",
       "  'lastModified': '2022-06-10T12:05:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'om',\n",
       "   'am',\n",
       "   'rw',\n",
       "   'rn',\n",
       "   'ha',\n",
       "   'ig',\n",
       "   'pcm',\n",
       "   'so',\n",
       "   'sw',\n",
       "   'ti',\n",
       "   'yo',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'castorini/afriberta_large',\n",
       "  'downloads': 125304,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['om',\n",
       "    'am',\n",
       "    'rw',\n",
       "    'rn',\n",
       "    'ha',\n",
       "    'ig',\n",
       "    'pcm',\n",
       "    'so',\n",
       "    'sw',\n",
       "    'ti',\n",
       "    'yo',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-summarize-news': {'modelId': 'mrm8488/t5-base-finetuned-summarize-news',\n",
       "  'sha': 'ada499546852c489d6327cae23439ec309f6869f',\n",
       "  'lastModified': '2022-01-18T15:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'news',\n",
       "   'summary',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-summarize-news',\n",
       "  'downloads': 125079,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['news', 'summary']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-distilroberta-v1': {'modelId': 'sentence-transformers/all-distilroberta-v1',\n",
       "  'sha': '073209d43b6df0d157f306d61bd9af92939be0c5',\n",
       "  'lastModified': '2022-06-21T14:56:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/all-distilroberta-v1',\n",
       "  'downloads': 124608,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/longformer-base-4096': {'modelId': 'allenai/longformer-base-4096',\n",
       "  'sha': 'e351d9d5da3eed48886f39eed7b64014debe4925',\n",
       "  'lastModified': '2021-03-10T02:30:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'longformer',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-base-4096',\n",
       "  'downloads': 124267,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 30,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'tals/albert-xlarge-vitaminc-mnli': {'modelId': 'tals/albert-xlarge-vitaminc-mnli',\n",
       "  'sha': '4c79eb5353f6104eb148d9221560c913f45677c7',\n",
       "  'lastModified': '2022-06-24T01:33:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'text-classification',\n",
       "   'python',\n",
       "   'dataset:fever',\n",
       "   'dataset:glue',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:tals/vitaminc',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'tals',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'tals/albert-xlarge-vitaminc-mnli',\n",
       "  'downloads': 123954,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'python',\n",
       "   'datasets': ['fever', 'glue', 'multi_nli', 'tals/vitaminc']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/distilbart-mnli-12-1': {'modelId': 'valhalla/distilbart-mnli-12-1',\n",
       "  'sha': '506336d4214470e3b3b36021358daae28e25ceac',\n",
       "  'lastModified': '2021-06-14T10:27:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-1',\n",
       "  'downloads': 123703,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune': {'modelId': 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune',\n",
       "  'sha': 'cf3d414acf70f8f8e68108a2efde164b129e6bfa',\n",
       "  'lastModified': '2022-06-27T20:56:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2104.02443',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'summarization'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'SEBIS',\n",
       "  'config': {'architectures': ['T5Model'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'max_length': 512,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'program synthesis: '}}},\n",
       "  'id': 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune',\n",
       "  'downloads': 120430,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'you are given an array of numbers a and a number b , compute the difference of elements in a and b'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'widget': [{'text': 'you are given an array of numbers a and a number b , compute the difference of elements in a and b'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-reader-single-nq-base': {'modelId': 'facebook/dpr-reader-single-nq-base',\n",
       "  'sha': '5114ec3299284784702848f7d4e598c59df1a35c',\n",
       "  'lastModified': '2020-11-25T16:59:53.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRReader'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-reader-single-nq-base',\n",
       "  'downloads': 120154,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRReader',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2': {'modelId': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sha': 'ef15aed8b328d308d7237b9bf15269f2cd19e268',\n",
       "  'lastModified': '2022-06-15T19:38:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'downloads': 119720,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-span-sentiment-extraction': {'modelId': 'mrm8488/t5-base-finetuned-span-sentiment-extraction',\n",
       "  'sha': '04a3fe1f7373c1f33b82e9fb06d2b2635e0fc5a0',\n",
       "  'lastModified': '2021-08-23T21:29:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'extracion',\n",
       "   'passage',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-span-sentiment-extraction',\n",
       "  'downloads': 119013,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'question: positive context: On the monday, so i wont be able to be with you! i love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sentiment', 'extracion', 'passage'],\n",
       "   'widget': [{'text': 'question: positive context: On the monday, so i wont be able to be with you! i love you'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/grammar_error_correcter_v1': {'modelId': 'prithivida/grammar_error_correcter_v1',\n",
       "  'sha': '28f92ee33c2512814c22268b056a725364dae143',\n",
       "  'lastModified': '2021-07-04T10:44:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'prithivida/grammar_error_correcter_v1',\n",
       "  'downloads': 118737,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-de-en': {'modelId': 'Helsinki-NLP/opus-mt-de-en',\n",
       "  'sha': '6137149949ac01d19d8eeef6e35d32221dabc8e4',\n",
       "  'lastModified': '2021-09-09T21:30:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-de-en',\n",
       "  'downloads': 116939,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neo-1.3B': {'modelId': 'EleutherAI/gpt-neo-1.3B',\n",
       "  'sha': '797174552ae47f449ab70b684cabcb6603e5e85e',\n",
       "  'lastModified': '2021-12-31T13:48:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:the_pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'EleutherAI/gpt-neo-1.3B',\n",
       "  'downloads': 115918,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 39,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['the_pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'digitalepidemiologylab/covid-twitter-bert': {'modelId': 'digitalepidemiologylab/covid-twitter-bert',\n",
       "  'sha': '945b4ea68241df3ccb8554cd1927ba81d2c9ecaa',\n",
       "  'lastModified': '2021-05-19T15:52:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'Twitter',\n",
       "   'COVID-19',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'digitalepidemiologylab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'digitalepidemiologylab/covid-twitter-bert',\n",
       "  'downloads': 115096,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/digitalepidemiologylab/covid-twitter-bert/master/images/COVID-Twitter-BERT_small.png',\n",
       "   'tags': ['Twitter', 'COVID-19'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/gbert-base': {'modelId': 'deepset/gbert-base',\n",
       "  'sha': '4a45e506eccc3405ed2e2a0502995d3f7e483509',\n",
       "  'lastModified': '2022-02-17T14:05:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OPUS',\n",
       "   'dataset:OpenLegalData',\n",
       "   'arxiv:2010.10906',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'deepset/gbert-base',\n",
       "  'downloads': 114931,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['wikipedia', 'OPUS', 'OpenLegalData']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking': {'modelId': 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking',\n",
       "  'sha': '8ae74eb0fbe7c8d82bb3d1a91fca56f352074e7f',\n",
       "  'lastModified': '2022-06-15T19:34:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking',\n",
       "  'downloads': 112322,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-cased': {'modelId': 'dbmdz/bert-base-turkish-cased',\n",
       "  'sha': 'bd38a3ecfe5d183400a573b1903e940d8d34902b',\n",
       "  'lastModified': '2021-05-19T15:14:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-cased',\n",
       "  'downloads': 111865,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-distilroberta-base-v2': {'modelId': 'sentence-transformers/paraphrase-distilroberta-base-v2',\n",
       "  'sha': 'd9461390caf1e64923d00bc55fa02d3c1ed2b9e5',\n",
       "  'lastModified': '2022-06-15T19:42:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-distilroberta-base-v2',\n",
       "  'downloads': 111064,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/detr-resnet-50': {'modelId': 'facebook/detr-resnet-50',\n",
       "  'sha': '272941311143979e4ade5424ede52fb5e84c9969',\n",
       "  'lastModified': '2022-06-27T08:29:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForObjectDetection'],\n",
       "   'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-50',\n",
       "  'downloads': 105601,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 42,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-es-en': {'modelId': 'Helsinki-NLP/opus-mt-es-en',\n",
       "  'sha': '7709af724cf305012a250cbd13cf3bfdbd2b66b0',\n",
       "  'lastModified': '2021-01-18T08:23:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'es',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-es-en',\n",
       "  'downloads': 104492,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Me llamo Wolfgang y vivo en Berlin'},\n",
       "   {'text': 'Los ingredientes de una tortilla de patatas son: huevos, patatas y cebolla'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-base-squad2-covid': {'modelId': 'deepset/roberta-base-squad2-covid',\n",
       "  'sha': 'b3506f363ab164823a64b5372d5cc98f36504cd6',\n",
       "  'lastModified': '2021-10-21T12:19:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'deepset/roberta-base-squad2-covid',\n",
       "  'downloads': 104396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv2': {'modelId': 'aubmindlab/bert-base-arabertv2',\n",
       "  'sha': '599b85458968e0cbad56126802f8328e649b3bec',\n",
       "  'lastModified': '2022-04-06T15:22:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv2',\n",
       "  'downloads': 104252,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' ÿπÿßÿµŸÖ +ÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled'],\n",
       "   'widget': [{'text': ' ÿπÿßÿµŸÖ +ÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-xxl-cased': {'modelId': 'dbmdz/bert-base-italian-xxl-cased',\n",
       "  'sha': 'e25680c78556c0d9002dba60d712e1df3095240e',\n",
       "  'lastModified': '2021-05-19T15:01:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-xxl-cased',\n",
       "  'downloads': 104236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma √® la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita √® [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-base': {'modelId': 'facebook/wav2vec2-base',\n",
       "  'sha': '0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8',\n",
       "  'lastModified': '2021-12-28T12:44:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-base',\n",
       "  'downloads': 104121,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'bert-base-german-dbmdz-uncased': {'modelId': 'bert-base-german-dbmdz-uncased',\n",
       "  'sha': '8f95e94bad647a9267df8620d729248ede4f86f4',\n",
       "  'lastModified': '2021-05-18T16:16:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-german-dbmdz-uncased',\n",
       "  'downloads': 102436,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/LaBSE-en-ru': {'modelId': 'cointegrated/LaBSE-en-ru',\n",
       "  'sha': '9e6d1e5fa79584f5346a5262b69bb6dc64b46f98',\n",
       "  'lastModified': '2022-06-23T12:43:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'feature-extraction',\n",
       "   'embeddings',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/LaBSE-en-ru',\n",
       "  'downloads': 99433,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['feature-extraction', 'embeddings', 'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mobilebert-uncased': {'modelId': 'google/mobilebert-uncased',\n",
       "  'sha': '1f90a6c24c7879273a291d34a849033eba2dbc0f',\n",
       "  'lastModified': '2021-04-19T13:32:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'mobilebert',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MobileBertForPreTraining'],\n",
       "   'model_type': 'mobilebert'},\n",
       "  'id': 'google/mobilebert-uncased',\n",
       "  'downloads': 97856,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabert': {'modelId': 'aubmindlab/bert-base-arabert',\n",
       "  'sha': '4b7ceb4967371d5e0b559b275e006f54d671c48e',\n",
       "  'lastModified': '2021-05-19T11:49:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabert',\n",
       "  'downloads': 96387,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' ÿπÿßÿµŸÖ +ÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia', 'OSIAN', '1.5B Arabic Corpus'],\n",
       "   'widget': [{'text': ' ÿπÿßÿµŸÖ +ÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ru-en': {'modelId': 'Helsinki-NLP/opus-mt-ru-en',\n",
       "  'sha': '1f663e796e1a2187db2f04065d143dda4c634a44',\n",
       "  'lastModified': '2021-09-10T14:02:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ru-en',\n",
       "  'downloads': 95567,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–æ–ª—å—Ñ–≥–∞–Ω–≥ –∏ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª–∏–Ω–µ'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'speechbrain/spkrec-ecapa-voxceleb': {'modelId': 'speechbrain/spkrec-ecapa-voxceleb',\n",
       "  'sha': '5c0be3875fda05e81f3c004ed8c7c06be308de1e',\n",
       "  'lastModified': '2022-06-26T23:15:06.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:voxceleb',\n",
       "   'arxiv:2106.04624',\n",
       "   'speechbrain',\n",
       "   'embeddings',\n",
       "   'Speaker',\n",
       "   'Verification',\n",
       "   'Identification',\n",
       "   'pytorch',\n",
       "   'ECAPA',\n",
       "   'TDNN',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'speechbrain',\n",
       "  'config': {'speechbrain': {'interface': 'SpeakerRecognition'}},\n",
       "  'id': 'speechbrain/spkrec-ecapa-voxceleb',\n",
       "  'downloads': 95347,\n",
       "  'library_name': 'speechbrain',\n",
       "  'widgetData': [{'example_title': 'VoxCeleb Speaker id10003',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb1_00003.wav'},\n",
       "   {'example_title': 'VoxCeleb Speaker id10004',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb_00004.wav'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': None,\n",
       "   'tags': ['speechbrain',\n",
       "    'embeddings',\n",
       "    'Speaker',\n",
       "    'Verification',\n",
       "    'Identification',\n",
       "    'pytorch',\n",
       "    'ECAPA',\n",
       "    'TDNN'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['voxceleb'],\n",
       "   'metrics': ['EER'],\n",
       "   'widget': [{'example_title': 'VoxCeleb Speaker id10003',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb1_00003.wav'},\n",
       "    {'example_title': 'VoxCeleb Speaker id10004',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb_00004.wav'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dbmdz/distilbert-base-turkish-cased': {'modelId': 'dbmdz/distilbert-base-turkish-cased',\n",
       "  'sha': '2fffa20b389e66113ec7182349efdec00fce1ff5',\n",
       "  'lastModified': '2021-01-24T01:01:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'tr',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'dbmdz/distilbert-base-turkish-cased',\n",
       "  'downloads': 95188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'EleutherAI/gpt-neo-2.7B': {'modelId': 'EleutherAI/gpt-neo-2.7B',\n",
       "  'sha': '51568a6e0ae813a3f2a9da558ab7beac5e3acc24',\n",
       "  'lastModified': '2021-12-31T13:46:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'EleutherAI/gpt-neo-2.7B',\n",
       "  'downloads': 95126,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 85,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-german-cased': {'modelId': 'bert-base-german-cased',\n",
       "  'sha': '702774c02b32a4f360d5fea60ab034d64bf0141c',\n",
       "  'lastModified': '2021-05-18T16:14:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-german-cased',\n",
       "  'downloads': 93964,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://static.tildacdn.com/tild6438-3730-4164-b266-613634323466/german_bert.png',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutxlm-base': {'modelId': 'microsoft/layoutxlm-base',\n",
       "  'sha': 'b95ef788341ccd507115d74e10c4bb7137559f19',\n",
       "  'lastModified': '2022-06-15T14:51:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'layoutlmv2',\n",
       "   'arxiv:2104.08836',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv2'},\n",
       "  'id': 'microsoft/layoutxlm-base',\n",
       "  'downloads': 93961,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'bigscience/T0_3B': {'modelId': 'bigscience/T0_3B',\n",
       "  'sha': '8794c7177e3a67b8a0ec739d94eecfa6a591c974',\n",
       "  'lastModified': '2022-06-21T01:31:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0_3B',\n",
       "  'downloads': 93943,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 40,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'squeezebert/squeezebert-uncased': {'modelId': 'squeezebert/squeezebert-uncased',\n",
       "  'sha': '7978b0c163f11850ec35d5cd541828159313ac41',\n",
       "  'lastModified': '2020-12-11T22:02:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'squeezebert',\n",
       "   'arxiv:2006.11316',\n",
       "   'arxiv:1904.00962',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'squeezebert',\n",
       "  'config': {'model_type': 'squeezebert'},\n",
       "  'id': 'squeezebert/squeezebert-uncased',\n",
       "  'downloads': 93796,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/mt5-base': {'modelId': 'google/mt5-base',\n",
       "  'sha': 'd86816880b5acc27e697e52bc237e816dc828b17',\n",
       "  'lastModified': '2022-05-27T15:05:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-base',\n",
       "  'downloads': 93164,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'BaptisteDoyen/camembert-base-xnli': {'modelId': 'BaptisteDoyen/camembert-base-xnli',\n",
       "  'sha': '791c5260a7c5984c7d96e622b45ca4c3ee6ea7d8',\n",
       "  'lastModified': '2022-06-29T09:30:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'fr',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'xnli',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'BaptisteDoyen',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'BaptisteDoyen/camembert-base-xnli',\n",
       "  'downloads': 92269,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['zero-shot-classification', 'xnli', 'nli', 'fr'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'datasets': ['xnli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cmarkea/distilcamembert-base': {'modelId': 'cmarkea/distilcamembert-base',\n",
       "  'sha': 'bf14fbad88b19c837997f26dd1684bf98404f96b',\n",
       "  'lastModified': '2022-05-24T15:57:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base',\n",
       "  'downloads': 91806,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"J'aime lire les <mask> de SF.\"}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['oscar'],\n",
       "   'widget': [{'text': \"J'aime lire les <mask> de SF.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bvanaken/clinical-assertion-negation-bert': {'modelId': 'bvanaken/clinical-assertion-negation-bert',\n",
       "  'sha': 'f381df19e34e690108f3b8e3e8433f7c9d2e2f9d',\n",
       "  'lastModified': '2022-06-01T12:28:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'medical',\n",
       "   'clinical',\n",
       "   'assertion',\n",
       "   'negation'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bvanaken',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bvanaken/clinical-assertion-negation-bert',\n",
       "  'downloads': 91226,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Patient denies [entity] SOB [entity].'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bert',\n",
       "    'medical',\n",
       "    'clinical',\n",
       "    'assertion',\n",
       "    'negation',\n",
       "    'text-classification'],\n",
       "   'widget': [{'text': 'Patient denies [entity] SOB [entity].'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cambridgeltl/BioRedditBERT-uncased': {'modelId': 'cambridgeltl/BioRedditBERT-uncased',\n",
       "  'sha': '53c71817b807682020273a0fa13aca033dfca292',\n",
       "  'lastModified': '2021-05-19T13:43:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2010.03295',\n",
       "   'transformers',\n",
       "   'BioNLP',\n",
       "   'social_media'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cambridgeltl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'cambridgeltl/BioRedditBERT-uncased',\n",
       "  'downloads': 91060,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['BioNLP', 'social_media']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-roberta-large': {'modelId': 'cross-encoder/stsb-roberta-large',\n",
       "  'sha': '9e35bf01ec28b309411c8903d0d4165567303eb4',\n",
       "  'lastModified': '2021-08-05T08:42:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-roberta-large',\n",
       "  'downloads': 90047,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/koelectra-small-v3-discriminator': {'modelId': 'monologg/koelectra-small-v3-discriminator',\n",
       "  'sha': '7488f8db0f208beff4a1f3f9bb3ed04650a89ed7',\n",
       "  'lastModified': '2020-12-26T16:24:33.000Z',\n",
       "  'tags': ['pytorch', 'electra', 'pretraining', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'monologg/koelectra-small-v3-discriminator',\n",
       "  'downloads': 87921,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/mpnet-base': {'modelId': 'microsoft/mpnet-base',\n",
       "  'sha': '5b7474c98ab5f1801502f9d2348485acf4cbbe71',\n",
       "  'lastModified': '2020-12-03T15:59:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'microsoft/mpnet-base',\n",
       "  'downloads': 87630,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/speaker-diarization': {'modelId': 'pyannote/speaker-diarization',\n",
       "  'sha': '3602c22f60dd2d07ef42263fa798c08b4dacedd0',\n",
       "  'lastModified': '2022-03-23T09:22:56.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'dataset:voxceleb',\n",
       "   'arxiv:2012.01477',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-diarization',\n",
       "   'speaker-change-detection',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/speaker-diarization',\n",
       "  'downloads': 87600,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-diarization',\n",
       "    'speaker-change-detection',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse', 'voxceleb'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/distilbart-mnli-12-3': {'modelId': 'valhalla/distilbart-mnli-12-3',\n",
       "  'sha': 'ef9a58ce6a9cd44cd0d4c2f7db1cd67f81019a8b',\n",
       "  'lastModified': '2021-06-14T10:29:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-3',\n",
       "  'downloads': 84725,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-v2': {'modelId': 'cl-tohoku/bert-base-japanese-v2',\n",
       "  'sha': 'e4211d7c20b078ac29b022be35ae4b63f3fe1679',\n",
       "  'lastModified': '2021-09-23T13:45:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-v2',\n",
       "  'downloads': 84442,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-many-to-many-mmt': {'modelId': 'facebook/mbart-large-50-many-to-many-mmt',\n",
       "  'sha': '0ece2bb75a89350002537169ecadeb2b3d043b6b',\n",
       "  'lastModified': '2022-05-26T22:28:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50-many-to-many-mmt',\n",
       "  'downloads': 84171,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-en-ro': {'modelId': 'facebook/mbart-large-en-ro',\n",
       "  'sha': '2534e987b9ed03c416bbbaefa1a39e3441439bdd',\n",
       "  'lastModified': '2021-03-10T03:46:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mbart',\n",
       "   'en',\n",
       "   'ro',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-en-ro',\n",
       "  'downloads': 83723,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'],\n",
       "   'language': ['en', 'ro'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openai-gpt': {'modelId': 'openai-gpt',\n",
       "  'sha': 'f5d43767fdae6d1add916f893596656254202d13',\n",
       "  'lastModified': '2020-12-09T18:29:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'openai-gpt',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['OpenAIGPTLMHeadModel'],\n",
       "   'model_type': 'openai-gpt',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'openai-gpt',\n",
       "  'downloads': 83002,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-qa-qg-hl': {'modelId': 'valhalla/t5-small-qa-qg-hl',\n",
       "  'sha': 'a9d81e686f2169360fd59d8329235d3c4ba74f4f',\n",
       "  'lastModified': '2021-06-23T14:42:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-small-qa-qg-hl',\n",
       "  'downloads': 82671,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1': {'modelId': 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
       "  'sha': '491739ea56794e0ffa273a3fdf7737d26db1e969',\n",
       "  'lastModified': '2022-06-21T14:48:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
       "  'downloads': 82546,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/bart-large-cnn-samsum': {'modelId': 'philschmid/bart-large-cnn-samsum',\n",
       "  'sha': '78e20b3792d507739ebb9e5a417bcc87606d3293',\n",
       "  'lastModified': '2022-07-04T13:10:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'sagemaker',\n",
       "   'summarization',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'philschmid/bart-large-cnn-samsum',\n",
       "  'downloads': 82164,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Jeff: Can I train a ü§ó Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\\n'}],\n",
       "  'likes': 15,\n",
       "  'model-index': [{'name': 'bart-large-cnn-samsum',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "       'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rogue-1',\n",
       "        'value': 42.621},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 21.9825},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 33.034},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 41.3174},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 20.8716},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 32.1337}]},\n",
       "     {'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'samsum',\n",
       "       'type': 'samsum',\n",
       "       'config': 'samsum',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 41.3282,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 20.8755,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 32.1353,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 38.401,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 1.4297215938568115,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 60.0757,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sagemaker', 'bart', 'summarization'],\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': 'Jeff: Can I train a ü§ó Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\\n'}],\n",
       "   'model-index': [{'name': 'bart-large-cnn-samsum',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "        'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rogue-1',\n",
       "         'value': 42.621},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 21.9825},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 33.034},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 41.3174},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 20.8716},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 32.1337}]},\n",
       "      {'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'samsum',\n",
       "        'type': 'samsum',\n",
       "        'config': 'samsum',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 41.3282,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 20.8755,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 32.1353,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 38.401,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 1.4297215938568115,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 60.0757,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased-sentence': {'modelId': 'DeepPavlov/rubert-base-cased-sentence',\n",
       "  'sha': '78b5122d6365337dd4114281b0d08cd1edbb3bc8',\n",
       "  'lastModified': '2021-05-18T18:18:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'arxiv:1508.05326',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased-sentence',\n",
       "  'downloads': 81886,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huggingtweets/pabloiglesias': {'modelId': 'huggingtweets/pabloiglesias',\n",
       "  'sha': '3d30ed2ab81feb7ce8eac1df3d7fa03db8c13e4e',\n",
       "  'lastModified': '2021-05-22T17:52:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'huggingtweets'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'huggingtweets',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 160,\n",
       "     'min_length': 10,\n",
       "     'prefix': '<|endoftext|>',\n",
       "     'temperature': 1,\n",
       "     'top_p': 0.95}}},\n",
       "  'id': 'huggingtweets/pabloiglesias',\n",
       "  'downloads': 80764,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My dream is'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://www.huggingtweets.com/pabloiglesias/1621002350351/predictions.png',\n",
       "   'tags': ['huggingtweets'],\n",
       "   'widget': [{'text': 'My dream is'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'smanjil/German-MedBERT': {'modelId': 'smanjil/German-MedBERT',\n",
       "  'sha': 'b4e8a3e260ca938390616816402ab23d98775b07',\n",
       "  'lastModified': '2022-06-13T16:52:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'German',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'smanjil',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'smanjil/German-MedBERT',\n",
       "  'downloads': 80576,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'tags': ['exbert', 'German']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/xglm-4.5B': {'modelId': 'facebook/xglm-4.5B',\n",
       "  'sha': '19523cf39b8f6f61232e9aa4191fa9473b398bff',\n",
       "  'lastModified': '2022-02-15T01:32:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'arxiv:2112.10668',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'facebook/xglm-4.5B',\n",
       "  'downloads': 80188,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'anferico/bert-for-patents': {'modelId': 'anferico/bert-for-patents',\n",
       "  'sha': 'd1a25632e9c586399068a2f139d5664306b32ad8',\n",
       "  'lastModified': '2022-06-23T19:22:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'anferico',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'anferico/bert-for-patents',\n",
       "  'downloads': 79700,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The present [MASK] provides a torque sensor that is small and highly rigid and for which high production efficiency is possible.'},\n",
       "   {'text': 'The present invention relates to [MASK] accessories and pertains particularly to a brake light unit for bicycles.'},\n",
       "   {'text': 'The present invention discloses a space-bound-free [MASK] and its coordinate determining circuit for determining a coordinate of a stylus pen.'},\n",
       "   {'text': 'The illuminated [MASK] includes a substantially translucent canopy supported by a plurality of ribs pivotally swingable towards and away from a shaft.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['masked-lm', 'pytorch'],\n",
       "   'pipeline-tag': 'fill-mask',\n",
       "   'mask-token': '[MASK]',\n",
       "   'widget': [{'text': 'The present [MASK] provides a torque sensor that is small and highly rigid and for which high production efficiency is possible.'},\n",
       "    {'text': 'The present invention relates to [MASK] accessories and pertains particularly to a brake light unit for bicycles.'},\n",
       "    {'text': 'The present invention discloses a space-bound-free [MASK] and its coordinate determining circuit for determining a coordinate of a stylus pen.'},\n",
       "    {'text': 'The illuminated [MASK] includes a substantially translucent canopy supported by a plurality of ribs pivotally swingable towards and away from a shaft.'}],\n",
       "   'license': 'apache-2.0',\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'patrickvonplaten/t5-tiny-random': {'modelId': 'patrickvonplaten/t5-tiny-random',\n",
       "  'sha': 'a3735d6adf1f23b7c32e6622fd6da7bc46d7f123',\n",
       "  'lastModified': '2021-11-03T17:13:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'patrickvonplaten',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'patrickvonplaten/t5-tiny-random',\n",
       "  'downloads': 78167,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/small': {'modelId': 'funnel-transformer/small',\n",
       "  'sha': 'ff0f4c11e46720ca10aa2dd668c2c58fe00ad214',\n",
       "  'lastModified': '2020-12-11T21:40:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/small',\n",
       "  'downloads': 77622,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased',\n",
       "  'sha': '47709c6fb2c53d30c871b05b8fb2693a5428d96a',\n",
       "  'lastModified': '2022-06-21T14:55:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased',\n",
       "  'downloads': 77232,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/vit-base-patch16-224-in21k': {'modelId': 'google/vit-base-patch16-224-in21k',\n",
       "  'sha': '1ba429d32753f33a0660b80ac6f43a3c80c18938',\n",
       "  'lastModified': '2022-01-12T08:03:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTModel'], 'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch16-224-in21k',\n",
       "  'downloads': 76868,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-21k'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/vit-base-patch16-224': {'modelId': 'google/vit-base-patch16-224',\n",
       "  'sha': '5dca96d358b3fcb9d53b3d3881eb1ae20b6752d1',\n",
       "  'lastModified': '2022-06-23T07:42:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch16-224',\n",
       "  'downloads': 76229,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 46,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k', 'imagenet-21k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'deepset/bert-base-cased-squad2': {'modelId': 'deepset/bert-base-cased-squad2',\n",
       "  'sha': '66162c7d1eb6b238ca147485bf392ca907576b57',\n",
       "  'lastModified': '2021-05-19T15:24:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/bert-base-cased-squad2',\n",
       "  'downloads': 75516,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-et-en': {'modelId': 'Helsinki-NLP/opus-mt-et-en',\n",
       "  'sha': '3649f4761e2a1b25f2c19e3f1732c6ba9ef61519',\n",
       "  'lastModified': '2021-09-09T21:46:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'et',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-et-en',\n",
       "  'downloads': 75388,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-large-squad2': {'modelId': 'deepset/roberta-large-squad2',\n",
       "  'sha': '9de49fc56513eb53d822f3344ce2e898cfeca170',\n",
       "  'lastModified': '2021-05-20T16:09:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-large-squad2',\n",
       "  'downloads': 75045,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '888433dbfd0f07dc22d9e038d9acb20e5ca7e0d5',\n",
       "  'lastModified': '2022-06-15T20:07:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 74899,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/LaBSE': {'modelId': 'sentence-transformers/LaBSE',\n",
       "  'sha': '931b5f9a111859fa72549cd1a7cb32168ebbe010',\n",
       "  'lastModified': '2022-06-15T19:56:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/LaBSE',\n",
       "  'downloads': 74743,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'etalab-ia/camembert-base-squadFR-fquad-piaf': {'modelId': 'etalab-ia/camembert-base-squadFR-fquad-piaf',\n",
       "  'sha': '63296563d30b341d2cbb3feae651a3545dc1c74d',\n",
       "  'lastModified': '2022-07-04T08:16:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'question-answering',\n",
       "   'fr',\n",
       "   'dataset:piaf',\n",
       "   'dataset:FQuAD',\n",
       "   'dataset:SQuAD-FR',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'etalab-ia',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'etalab-ia/camembert-base-squadFR-fquad-piaf',\n",
       "  'downloads': 74622,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Comment s'appelle le portail open data du gouvernement ?\",\n",
       "    'context': \"Etalab est une administration publique fran√ßaise qui fait notamment office de Chief Data Officer de l'√âtat et coordonne la conception et la mise en ≈ìuvre de sa strat√©gie dans le domaine de la donn√©e (ouverture et partage des donn√©es publiques ou open data, exploitation des donn√©es et intelligence artificielle...). Ainsi, Etalab d√©veloppe et maintient le portail des donn√©es ouvertes du gouvernement fran√ßais data.gouv.fr. Etalab promeut √©galement une plus grande ouverture l'administration sur la soci√©t√© (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l‚Äôinnovation, l‚Äôexp√©rimentation, les m√©thodes de travail ouvertes, agiles et it√©ratives, ainsi que les synergies avec la soci√©t√© civile pour d√©cloisonner l‚Äôadministration et favoriser l‚Äôadoption des meilleures pratiques professionnelles dans le domaine du num√©rique. √Ä ce titre elle √©tudie notamment l‚Äôopportunit√© de recourir √† des technologies en voie de maturation issues du monde de la recherche. Cette entit√© charg√©e de l'innovation au sein de l'administration doit contribuer √† l'am√©lioration du service public gr√¢ce au num√©rique. Elle est rattach√©e √† la Direction interminist√©rielle du num√©rique, dont les missions et l‚Äôorganisation ont √©t√© fix√©es par le d√©cret du 30 octobre 2019.\\u2009 Dirig√© par Laure Lucchesi depuis 2016, elle rassemble une √©quipe pluridisciplinaire d'une trentaine de personnes.\"}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['piaf', 'FQuAD', 'SQuAD-FR'],\n",
       "   'widget': [{'text': \"Comment s'appelle le portail open data du gouvernement ?\",\n",
       "     'context': \"Etalab est une administration publique fran√ßaise qui fait notamment office de Chief Data Officer de l'√âtat et coordonne la conception et la mise en ≈ìuvre de sa strat√©gie dans le domaine de la donn√©e (ouverture et partage des donn√©es publiques ou open data, exploitation des donn√©es et intelligence artificielle...). Ainsi, Etalab d√©veloppe et maintient le portail des donn√©es ouvertes du gouvernement fran√ßais data.gouv.fr. Etalab promeut √©galement une plus grande ouverture l'administration sur la soci√©t√© (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l‚Äôinnovation, l‚Äôexp√©rimentation, les m√©thodes de travail ouvertes, agiles et it√©ratives, ainsi que les synergies avec la soci√©t√© civile pour d√©cloisonner l‚Äôadministration et favoriser l‚Äôadoption des meilleures pratiques professionnelles dans le domaine du num√©rique. √Ä ce titre elle √©tudie notamment l‚Äôopportunit√© de recourir √† des technologies en voie de maturation issues du monde de la recherche. Cette entit√© charg√©e de l'innovation au sein de l'administration doit contribuer √† l'am√©lioration du service public gr√¢ce au num√©rique. Elle est rattach√©e √† la Direction interminist√©rielle du num√©rique, dont les missions et l‚Äôorganisation ont √©t√© fix√©es par le d√©cret du 30 octobre 2019.\\u2009 Dirig√© par Laure Lucchesi depuis 2016, elle rassemble une √©quipe pluridisciplinaire d'une trentaine de personnes.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/xlm-roberta-large-squad2': {'modelId': 'deepset/xlm-roberta-large-squad2',\n",
       "  'sha': '8f1f44ba7cbfe49d8452579c90d0fd0b0ac496da',\n",
       "  'lastModified': '2021-10-21T12:19:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'multilingual',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'deepset/xlm-roberta-large-squad2',\n",
       "  'downloads': 74066,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['question-answering'],\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nyust-eb210/braslab-bert-drcd-384': {'modelId': 'nyust-eb210/braslab-bert-drcd-384',\n",
       "  'sha': 'abb9294fe9c0605d2f498a3228bfc6a30e8d2fbb',\n",
       "  'lastModified': '2021-05-31T14:47:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'zh-tw',\n",
       "   'dataset:DRCD',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'nyust-eb210',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'nyust-eb210/braslab-bert-drcd-384',\n",
       "  'downloads': 73536,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh-tw',\n",
       "   'datasets': 'DRCD',\n",
       "   'tasks': 'Question Answering'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepparag/Aeona': {'modelId': 'deepparag/Aeona',\n",
       "  'sha': '05c943eff79282ab56c3996e53ef7ccb3089f81f',\n",
       "  'lastModified': '2022-06-30T05:09:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'deepparag',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'do_sample': True,\n",
       "     'max_length': 1000,\n",
       "     'no_repeat_ngram_size': 4,\n",
       "     'temperature': 0.85,\n",
       "     'top_k': 100,\n",
       "     'top_p': 0.7}}},\n",
       "  'id': 'deepparag/Aeona',\n",
       "  'downloads': 73475,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://images-ext-2.discordapp.net/external/Wvtx1L98EbA7DR2lpZPbDxDuO4qmKt03nZygATZtXgk/%3Fsize%3D4096/https/cdn.discordapp.com/avatars/931226824753700934/338a9e413bbceaeb9095a29e97d4fac0.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-TinyBERT-L-2-v2': {'modelId': 'cross-encoder/ms-marco-TinyBERT-L-2-v2',\n",
       "  'sha': 'e9ea2688951463fc2791a2ea2ddfce6762900675',\n",
       "  'lastModified': '2021-08-05T08:39:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-TinyBERT-L-2-v2',\n",
       "  'downloads': 73316,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-sentiment-analysis': {'modelId': 'pysentimiento/robertuito-sentiment-analysis',\n",
       "  'sha': 'e3be95c8efad7f480ce8aab2221188ecb78e40f3',\n",
       "  'lastModified': '2022-06-23T13:01:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'twitter',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-sentiment-analysis',\n",
       "  'downloads': 73003,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['twitter', 'sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-large-discriminator': {'modelId': 'google/electra-large-discriminator',\n",
       "  'sha': '96c9a247e8ef7e818408efedfbd5fd2a26aa13ae',\n",
       "  'lastModified': '2021-04-30T07:38:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-large-discriminator',\n",
       "  'downloads': 72327,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'felflare/bert-restore-punctuation': {'modelId': 'felflare/bert-restore-punctuation',\n",
       "  'sha': '954108a105ef1f89f08b71c25d6e33bb89cde724',\n",
       "  'lastModified': '2021-05-24T03:04:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:yelp_polarity',\n",
       "   'transformers',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'felflare',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'felflare/bert-restore-punctuation',\n",
       "  'downloads': 71213,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['punctuation'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['yelp_polarity'],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yjernite/retribert-base-uncased': {'modelId': 'yjernite/retribert-base-uncased',\n",
       "  'sha': 'aeab2b097862fa41e084db47e0e02229649bbe53',\n",
       "  'lastModified': '2021-03-10T02:54:37.000Z',\n",
       "  'tags': ['pytorch', 'retribert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'yjernite',\n",
       "  'config': {'architectures': ['RetriBertModel'], 'model_type': 'retribert'},\n",
       "  'id': 'yjernite/retribert-base-uncased',\n",
       "  'downloads': 70663,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-medium': {'modelId': 'microsoft/DialoGPT-medium',\n",
       "  'sha': '8bada3b953e25ec171dea4e28c52f1e8b546d707',\n",
       "  'lastModified': '2021-05-23T09:11:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-medium',\n",
       "  'downloads': 70452,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext': {'modelId': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
       "  'sha': 'eaa409b6b7c9380a5f2ba7a59aa97712ff30f386',\n",
       "  'lastModified': '2021-09-22T20:09:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2007.15779',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
       "  'downloads': 70287,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[MASK] is a tumor suppressor gene.'}],\n",
       "  'likes': 32,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[MASK] is a tumor suppressor gene.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/reformer-crime-and-punishment': {'modelId': 'google/reformer-crime-and-punishment',\n",
       "  'sha': '0e6c3decb8211d49bf881013425dc8b0448b3f5a',\n",
       "  'lastModified': '2021-02-01T17:53:38.000Z',\n",
       "  'tags': ['pytorch', 'rust', 'reformer', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ReformerModelWithLMHead'],\n",
       "   'model_type': 'reformer',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 100}}},\n",
       "  'id': 'google/reformer-crime-and-punishment',\n",
       "  'downloads': 69746,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-xxlarge-v2': {'modelId': 'albert-xxlarge-v2',\n",
       "  'sha': 'aaec31cf649a4d91a96b11f83eb5b2985eaf8ee5',\n",
       "  'lastModified': '2021-01-13T15:33:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-xxlarge-v2',\n",
       "  'downloads': 69726,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased': {'modelId': 'DeepPavlov/rubert-base-cased',\n",
       "  'sha': '4036cab694767a299f2b9e6492909664d9414229',\n",
       "  'lastModified': '2021-11-23T08:03:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'arxiv:1905.07213',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased',\n",
       "  'downloads': 69071,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-emotion': {'modelId': 'cardiffnlp/twitter-roberta-base-emotion',\n",
       "  'sha': 'dff452c4f42c15a25bd51aff1f1ca5d15ec08c23',\n",
       "  'lastModified': '2022-03-23T14:34:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-emotion',\n",
       "  'downloads': 66608,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-mean-tokens': {'modelId': 'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'sha': '683b927b0b0f77e70b9a7d15f7f7601a515925a9',\n",
       "  'lastModified': '2022-06-15T19:35:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'downloads': 65922,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'feature-extraction',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ROMANCE-en': {'modelId': 'Helsinki-NLP/opus-mt-ROMANCE-en',\n",
       "  'sha': 'dd27a5df7623594b19ab50244084e2beddc2181c',\n",
       "  'lastModified': '2021-09-09T21:25:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'roa',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ROMANCE-en',\n",
       "  'downloads': 65625,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-3b': {'modelId': 't5-3b',\n",
       "  'sha': '89a53f1bc99ac6386b6757c149b150f5819a29f2',\n",
       "  'lastModified': '2022-03-18T17:24:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-3b',\n",
       "  'downloads': 65493,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Narsil/deberta-large-mnli-zero-cls': {'modelId': 'Narsil/deberta-large-mnli-zero-cls',\n",
       "  'sha': '47eecd0a22df5e7d6ad4d9ff6fa4b6f322db5700',\n",
       "  'lastModified': '2021-08-23T13:27:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Narsil',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'Narsil/deberta-large-mnli-zero-cls',\n",
       "  'downloads': 64833,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/swin-tiny-patch4-window7-224': {'modelId': 'microsoft/swin-tiny-patch4-window7-224',\n",
       "  'sha': '83d40fb5b9320b349382208d9e7fe998484e99df',\n",
       "  'lastModified': '2022-05-16T18:24:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'swin',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2103.14030',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['SwinForImageClassification'],\n",
       "   'model_type': 'swin'},\n",
       "  'id': 'microsoft/swin-tiny-patch4-window7-224',\n",
       "  'downloads': 63588,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/mt5-small': {'modelId': 'google/mt5-small',\n",
       "  'sha': 'f03a52d3eaa650878b6f52e443bc4d5b385e786e',\n",
       "  'lastModified': '2022-05-27T15:06:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-small',\n",
       "  'downloads': 63334,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-mpnet-base-dot-v1': {'modelId': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
       "  'sha': '0675a4fa3b0e6e4a7900924c121277f73adcdb2b',\n",
       "  'lastModified': '2021-08-23T18:28:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
       "  'downloads': 62898,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12': {'modelId': 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'c656dbe1fc4d6c7771d93bcaff21b2e7984f64c8',\n",
       "  'lastModified': '2021-09-24T07:46:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'en',\n",
       "   'dataset:PubMed',\n",
       "   'dataset:MIMIC-III',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'bluebert',\n",
       "   'license:cc0-1.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'bionlp',\n",
       "  'config': {},\n",
       "  'id': 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 61300,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['bert', 'bluebert'],\n",
       "   'license': 'cc0-1.0',\n",
       "   'datasets': ['PubMed', 'MIMIC-III']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'arpanghoshal/EmoRoBERTa': {'modelId': 'arpanghoshal/EmoRoBERTa',\n",
       "  'sha': 'ed312eedd348a666a9b151d968a0fb98d73394d0',\n",
       "  'lastModified': '2022-06-20T22:42:02.000Z',\n",
       "  'tags': ['tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:go_emotions',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'arpanghoshal',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'arpanghoshal/EmoRoBERTa',\n",
       "  'downloads': 60661,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['text-classification', 'tensorflow', 'roberta'],\n",
       "   'datasets': ['go_emotions'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dangvantuan/sentence-camembert-large': {'modelId': 'dangvantuan/sentence-camembert-large',\n",
       "  'sha': '046a7d414b0a573af813c0e4d3ff2d0c7ca5c4b3',\n",
       "  'lastModified': '2022-03-11T17:01:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'feature-extraction',\n",
       "   'fr',\n",
       "   'dataset:stsb_multi_mt',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers',\n",
       "   'Text',\n",
       "   'Sentence Similarity',\n",
       "   'Sentence-Embedding',\n",
       "   'camembert-large',\n",
       "   'license:apache-2.0',\n",
       "   'sentence-similarity',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'dangvantuan',\n",
       "  'config': {'architectures': ['CamembertModel'], 'model_type': 'camembert'},\n",
       "  'id': 'dangvantuan/sentence-camembert-large',\n",
       "  'downloads': 60568,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': \"C'est une personne heureuse\",\n",
       "    'sentences': [\"C'est un chien heureux\",\n",
       "     \"C'est une personne tr√®s heureuse\",\n",
       "     \"Aujourd'hui est une journ√©e ensoleill√©e\"]}],\n",
       "  'likes': 6,\n",
       "  'model-index': [{'name': 'sentence-camembert-large by Van Tuan DANG',\n",
       "    'results': [{'task': {'name': 'Sentence-Embedding',\n",
       "       'type': 'Text Similarity'},\n",
       "      'dataset': {'name': 'Text Similarity fr',\n",
       "       'type': 'stsb_multi_mt',\n",
       "       'args': 'fr'},\n",
       "      'metrics': [{'name': 'Test Pearson correlation coefficient',\n",
       "        'type': 'Pearson_correlation_coefficient',\n",
       "        'value': 'xx.xx'}]}]}],\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'fr',\n",
       "   'datasets': ['stsb_multi_mt'],\n",
       "   'tags': ['Text',\n",
       "    'Sentence Similarity',\n",
       "    'Sentence-Embedding',\n",
       "    'camembert-large'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'sentence-camembert-large by Van Tuan DANG',\n",
       "     'results': [{'task': {'name': 'Sentence-Embedding',\n",
       "        'type': 'Text Similarity'},\n",
       "       'dataset': {'name': 'Text Similarity fr',\n",
       "        'type': 'stsb_multi_mt',\n",
       "        'args': 'fr'},\n",
       "       'metrics': [{'name': 'Test Pearson correlation coefficient',\n",
       "         'type': 'Pearson_correlation_coefficient',\n",
       "         'value': 'xx.xx'}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/bert-base-multilingual-cased-ner-hrl': {'modelId': 'Davlan/bert-base-multilingual-cased-ner-hrl',\n",
       "  'sha': '6f69c39cadcdba0ab1401fb1f164964e7557e471',\n",
       "  'lastModified': '2022-06-25T17:01:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'lv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Davlan/bert-base-multilingual-cased-ner-hrl',\n",
       "  'downloads': 60486,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ•ÿ≥ŸÖŸä ŸÖÿ≠ŸÖÿØ Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿ®ÿ±ŸÑŸäŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßÿ±Ÿá Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ŸÑŸÜÿØŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßŸÖŸä Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿßŸÑŸÇÿØÿ≥ ŸÅŸä ŸÅŸÑÿ≥ÿ∑ŸäŸÜ.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'lv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-nl-en': {'modelId': 'Helsinki-NLP/opus-mt-nl-en',\n",
       "  'sha': '642ab6dc2d08ca9b5706ff44191dc443eae738e1',\n",
       "  'lastModified': '2021-09-10T13:59:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'nl',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-nl-en',\n",
       "  'downloads': 60478,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-mul-en': {'modelId': 'Helsinki-NLP/opus-mt-mul-en',\n",
       "  'sha': 'bc0ba94fb12f8b8cf88bd8a925b15ccd5fb94340',\n",
       "  'lastModified': '2020-08-21T14:42:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'es',\n",
       "   'os',\n",
       "   'eo',\n",
       "   'ro',\n",
       "   'fy',\n",
       "   'cy',\n",
       "   'is',\n",
       "   'lb',\n",
       "   'su',\n",
       "   'an',\n",
       "   'sq',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'rm',\n",
       "   'cv',\n",
       "   'ig',\n",
       "   'am',\n",
       "   'eu',\n",
       "   'tr',\n",
       "   'ps',\n",
       "   'af',\n",
       "   'ny',\n",
       "   'ch',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'lt',\n",
       "   'tk',\n",
       "   'sg',\n",
       "   'ar',\n",
       "   'lg',\n",
       "   'bg',\n",
       "   'be',\n",
       "   'ka',\n",
       "   'gd',\n",
       "   'ja',\n",
       "   'si',\n",
       "   'br',\n",
       "   'mh',\n",
       "   'km',\n",
       "   'th',\n",
       "   'ty',\n",
       "   'rw',\n",
       "   'te',\n",
       "   'mk',\n",
       "   'or',\n",
       "   'wo',\n",
       "   'kl',\n",
       "   'mr',\n",
       "   'ru',\n",
       "   'yo',\n",
       "   'hu',\n",
       "   'fo',\n",
       "   'zh',\n",
       "   'ti',\n",
       "   'co',\n",
       "   'ee',\n",
       "   'oc',\n",
       "   'sn',\n",
       "   'mt',\n",
       "   'ts',\n",
       "   'pl',\n",
       "   'gl',\n",
       "   'nb',\n",
       "   'bn',\n",
       "   'tt',\n",
       "   'bo',\n",
       "   'lo',\n",
       "   'id',\n",
       "   'gn',\n",
       "   'nv',\n",
       "   'hy',\n",
       "   'kn',\n",
       "   'to',\n",
       "   'io',\n",
       "   'so',\n",
       "   'vi',\n",
       "   'da',\n",
       "   'fj',\n",
       "   'gv',\n",
       "   'sm',\n",
       "   'nl',\n",
       "   'mi',\n",
       "   'pt',\n",
       "   'hi',\n",
       "   'se',\n",
       "   'as',\n",
       "   'ta',\n",
       "   'et',\n",
       "   'kw',\n",
       "   'ga',\n",
       "   'sv',\n",
       "   'ln',\n",
       "   'na',\n",
       "   'mn',\n",
       "   'gu',\n",
       "   'wa',\n",
       "   'lv',\n",
       "   'jv',\n",
       "   'el',\n",
       "   'my',\n",
       "   'ba',\n",
       "   'it',\n",
       "   'hr',\n",
       "   'ur',\n",
       "   'ce',\n",
       "   'nn',\n",
       "   'fi',\n",
       "   'mg',\n",
       "   'rn',\n",
       "   'xh',\n",
       "   'ab',\n",
       "   'de',\n",
       "   'cs',\n",
       "   'he',\n",
       "   'zu',\n",
       "   'yi',\n",
       "   'ml',\n",
       "   'mul',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-mul-en',\n",
       "  'downloads': 60017,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ca',\n",
       "    'es',\n",
       "    'os',\n",
       "    'eo',\n",
       "    'ro',\n",
       "    'fy',\n",
       "    'cy',\n",
       "    'is',\n",
       "    'lb',\n",
       "    'su',\n",
       "    'an',\n",
       "    'sq',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'rm',\n",
       "    'cv',\n",
       "    'ig',\n",
       "    'am',\n",
       "    'eu',\n",
       "    'tr',\n",
       "    'ps',\n",
       "    'af',\n",
       "    'ny',\n",
       "    'ch',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'lt',\n",
       "    'tk',\n",
       "    'sg',\n",
       "    'ar',\n",
       "    'lg',\n",
       "    'bg',\n",
       "    'be',\n",
       "    'ka',\n",
       "    'gd',\n",
       "    'ja',\n",
       "    'si',\n",
       "    'br',\n",
       "    'mh',\n",
       "    'km',\n",
       "    'th',\n",
       "    'ty',\n",
       "    'rw',\n",
       "    'te',\n",
       "    'mk',\n",
       "    'or',\n",
       "    'wo',\n",
       "    'kl',\n",
       "    'mr',\n",
       "    'ru',\n",
       "    'yo',\n",
       "    'hu',\n",
       "    'fo',\n",
       "    'zh',\n",
       "    'ti',\n",
       "    'co',\n",
       "    'ee',\n",
       "    'oc',\n",
       "    'sn',\n",
       "    'mt',\n",
       "    'ts',\n",
       "    'pl',\n",
       "    'gl',\n",
       "    'nb',\n",
       "    'bn',\n",
       "    'tt',\n",
       "    'bo',\n",
       "    'lo',\n",
       "    'id',\n",
       "    'gn',\n",
       "    'nv',\n",
       "    'hy',\n",
       "    'kn',\n",
       "    'to',\n",
       "    'io',\n",
       "    'so',\n",
       "    'vi',\n",
       "    'da',\n",
       "    'fj',\n",
       "    'gv',\n",
       "    'sm',\n",
       "    'nl',\n",
       "    'mi',\n",
       "    'pt',\n",
       "    'hi',\n",
       "    'se',\n",
       "    'as',\n",
       "    'ta',\n",
       "    'et',\n",
       "    'kw',\n",
       "    'ga',\n",
       "    'sv',\n",
       "    'ln',\n",
       "    'na',\n",
       "    'mn',\n",
       "    'gu',\n",
       "    'wa',\n",
       "    'lv',\n",
       "    'jv',\n",
       "    'el',\n",
       "    'my',\n",
       "    'ba',\n",
       "    'it',\n",
       "    'hr',\n",
       "    'ur',\n",
       "    'ce',\n",
       "    'nn',\n",
       "    'fi',\n",
       "    'mg',\n",
       "    'rn',\n",
       "    'xh',\n",
       "    'ab',\n",
       "    'de',\n",
       "    'cs',\n",
       "    'he',\n",
       "    'zu',\n",
       "    'yi',\n",
       "    'ml',\n",
       "    'mul',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-distilbert': {'modelId': 'hf-internal-testing/tiny-random-distilbert',\n",
       "  'sha': '2ef615d573271690c9822df720b8024148d6715a',\n",
       "  'lastModified': '2021-11-26T16:32:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-distilbert',\n",
       "  'downloads': 59962,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'text-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-large': {'modelId': 'google/pegasus-large',\n",
       "  'sha': '51b039cd8c644561432f7bfbe75e65f720b38f66',\n",
       "  'lastModified': '2021-09-14T07:50:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus',\n",
       "   'task_specific_params': {'summarization_aeslc': {'length_penalty': 0.6,\n",
       "     'max_length': 32,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_arxiv': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_big_patent': {'length_penalty': 0.7,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_billsum': {'length_penalty': 0.6,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_cnn_dailymail': {'length_penalty': 0.8,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_gigaword': {'length_penalty': 0.6,\n",
       "     'max_length': 32,\n",
       "     'max_position_embeddings': 128},\n",
       "    'summarization_large': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_multi_news': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_newsroom': {'length_penalty': 0.8,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_pubmed': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_reddit_tifu': {'length_penalty': 0.6,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_wikihow': {'length_penalty': 0.6,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_xsum': {'length_penalty': 0.8,\n",
       "     'max_length': 64,\n",
       "     'max_position_embeddings': 512}}},\n",
       "  'id': 'google/pegasus-large',\n",
       "  'downloads': 59449,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-small': {'modelId': 'microsoft/DialoGPT-small',\n",
       "  'sha': 'f9c829d0285e7addb0667aeb6e33956916ec6cd0',\n",
       "  'lastModified': '2021-05-23T09:14:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-small',\n",
       "  'downloads': 59436,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'oliverguhr/german-sentiment-bert': {'modelId': 'oliverguhr/german-sentiment-bert',\n",
       "  'sha': 'c5c8dd0c5b966460dce1b7c5851bd90af1d2c6b6',\n",
       "  'lastModified': '2022-07-04T08:59:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'oliverguhr/german-sentiment-bert',\n",
       "  'downloads': 59391,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Das ist gar nicht mal so schlecht'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de'],\n",
       "   'tags': ['sentiment', 'bert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Das ist gar nicht mal so schlecht'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-german-uncased': {'modelId': 'dbmdz/bert-base-german-uncased',\n",
       "  'sha': '26dd62c10449c89b8029c4440855983dfc5a5e83',\n",
       "  'lastModified': '2021-05-19T14:57:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-german-uncased',\n",
       "  'downloads': 59116,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ntu-spml/distilhubert': {'modelId': 'ntu-spml/distilhubert',\n",
       "  'sha': '9c4eece5b1dd98770108a416c101096fb04813de',\n",
       "  'lastModified': '2021-11-05T12:43:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'hubert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2110.01900',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'ntu-spml',\n",
       "  'config': {'architectures': ['HubertModel'], 'model_type': 'hubert'},\n",
       "  'id': 'ntu-spml/distilhubert',\n",
       "  'downloads': 58815,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '1bd90bb33d5c6601f5fbd26d91e955a65059ee55',\n",
       "  'lastModified': '2022-06-15T20:01:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 58115,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-fast': {'modelId': 'flair/ner-english-fast',\n",
       "  'sha': '3d3d35790f78a00ef319939b9004209d1d05f788',\n",
       "  'lastModified': '2021-02-26T15:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-fast',\n",
       "  'downloads': 57903,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'typeform/distilbert-base-uncased-mnli': {'modelId': 'typeform/distilbert-base-uncased-mnli',\n",
       "  'sha': '996dacf8ea284d96ea21f88a345fd7d597de1f1f',\n",
       "  'lastModified': '2022-06-24T15:43:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'typeform',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'typeform/distilbert-base-uncased-mnli',\n",
       "  'downloads': 57675,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilbert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-it-en': {'modelId': 'Helsinki-NLP/opus-mt-it-en',\n",
       "  'sha': '23f2c7f29233a3e0accc900625d65ddf6a49b93e',\n",
       "  'lastModified': '2021-09-10T13:52:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'it',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-it-en',\n",
       "  'downloads': 57087,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Mi chiamo Sarah e vivo a Londra'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-base-cased-v1.1': {'modelId': 'dmis-lab/biobert-base-cased-v1.1',\n",
       "  'sha': '924f12e0c3db7f156a765ad53fb6b11e7afedbc8',\n",
       "  'lastModified': '2020-10-14T07:02:59.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {},\n",
       "  'id': 'dmis-lab/biobert-base-cased-v1.1',\n",
       "  'downloads': 57060,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli': {'modelId': 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'sha': 'ff7d96201d917e7fcc8b5b95f2631602a0777428',\n",
       "  'lastModified': '2020-10-17T02:05:17.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ynie',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'downloads': 56543,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-dot-v5': {'modelId': 'sentence-transformers/msmarco-distilbert-dot-v5',\n",
       "  'sha': '52b2679c1e6789ee4b2d3b81a27a4590a1bc5348',\n",
       "  'lastModified': '2022-06-15T20:15:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-dot-v5',\n",
       "  'downloads': 56396,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-question_encoder-multiset-base': {'modelId': 'facebook/dpr-question_encoder-multiset-base',\n",
       "  'sha': '1b547dba8676a9b96d143a6fffabe21b50553928',\n",
       "  'lastModified': '2020-11-25T16:59:33.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-question_encoder-multiset-base',\n",
       "  'downloads': 54639,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-gpt2': {'modelId': 'sshleifer/tiny-gpt2',\n",
       "  'sha': '5f91d94bd9cd7190a9f3216ff93cd1dd95f2c7be',\n",
       "  'lastModified': '2021-05-23T12:55:11.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'jax', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'sshleifer/tiny-gpt2',\n",
       "  'downloads': 54282,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dccuchile/bert-base-spanish-wwm-cased': {'modelId': 'dccuchile/bert-base-spanish-wwm-cased',\n",
       "  'sha': '56a7647b957a4230fc3f80dafbe80f2ba9b0de73',\n",
       "  'lastModified': '2022-05-31T15:01:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:1904.09077',\n",
       "   'arxiv:1906.01502',\n",
       "   'arxiv:1812.10464',\n",
       "   'arxiv:1901.07291',\n",
       "   'arxiv:1904.02099',\n",
       "   'arxiv:1906.01569',\n",
       "   'arxiv:1908.11828',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dccuchile',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dccuchile/bert-base-spanish-wwm-cased',\n",
       "  'downloads': 53907,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Mi nombre es [MASK] y vivo en Nueva York.'},\n",
       "   {'text': 'El espa√±ol es un idioma muy [MASK] en el mundo.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['masked-lm']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/phobert-base': {'modelId': 'vinai/phobert-base',\n",
       "  'sha': '667b55927a1571811539f27c0f374429a1c75759',\n",
       "  'lastModified': '2022-06-08T04:44:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2003.00744',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/phobert-base',\n",
       "  'downloads': 53792,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-mpnet-base-v1': {'modelId': 'sentence-transformers/all-mpnet-base-v1',\n",
       "  'sha': '5db7848555eaffcf26ac367f5dc9e0711acb2106',\n",
       "  'lastModified': '2021-08-31T07:35:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/all-mpnet-base-v1',\n",
       "  'downloads': 53617,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-base-e2e-qg': {'modelId': 'valhalla/t5-base-e2e-qg',\n",
       "  'sha': 'c652651334cd5516f2bd0f0fb5303a01a678024e',\n",
       "  'lastModified': '2021-06-23T14:40:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '}}},\n",
       "  'id': 'valhalla/t5-base-e2e-qg',\n",
       "  'downloads': 52862,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Python is a programming language. It is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'Python is a programming language. It is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csebuetnlp/mT5_multilingual_XLSum': {'modelId': 'csebuetnlp/mT5_multilingual_XLSum',\n",
       "  'sha': '361416d0a10fe5df7e139081f3b5476fd39c860f',\n",
       "  'lastModified': '2021-10-03T13:14:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'my',\n",
       "   'zh',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'hi',\n",
       "   'ig',\n",
       "   'id',\n",
       "   'ja',\n",
       "   'rn',\n",
       "   'ko',\n",
       "   'ky',\n",
       "   'mr',\n",
       "   'ne',\n",
       "   'om',\n",
       "   'ps',\n",
       "   'fa',\n",
       "   'pcm',\n",
       "   'pt',\n",
       "   'pa',\n",
       "   'ru',\n",
       "   'gd',\n",
       "   'sr',\n",
       "   'si',\n",
       "   'so',\n",
       "   'es',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'ti',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'cy',\n",
       "   'yo',\n",
       "   'dataset:csebuetnlp/xlsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'mT5',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'csebuetnlp',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'csebuetnlp/mT5_multilingual_XLSum',\n",
       "  'downloads': 52364,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.'}],\n",
       "  'likes': 44,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization', 'mT5'],\n",
       "   'datasets': ['csebuetnlp/xlsum'],\n",
       "   'language': ['am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'my',\n",
       "    'zh',\n",
       "    'en',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'hi',\n",
       "    'ig',\n",
       "    'id',\n",
       "    'ja',\n",
       "    'rn',\n",
       "    'ko',\n",
       "    'ky',\n",
       "    'mr',\n",
       "    'ne',\n",
       "    'om',\n",
       "    'ps',\n",
       "    'fa',\n",
       "    'pcm',\n",
       "    'pt',\n",
       "    'pa',\n",
       "    'ru',\n",
       "    'gd',\n",
       "    'sr',\n",
       "    'si',\n",
       "    'so',\n",
       "    'es',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'ti',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'cy',\n",
       "    'yo'],\n",
       "   'licenses': ['cc-by-nc-sa-4.0'],\n",
       "   'widget': [{'text': 'Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-bert': {'modelId': 'hf-internal-testing/tiny-random-bert',\n",
       "  'sha': '9b8c223d42b2188cb49d29af482996f9d0f3e5a6',\n",
       "  'lastModified': '2021-09-17T19:21:17.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-bert',\n",
       "  'downloads': 52119,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/xlm-roberta-base-squad2': {'modelId': 'deepset/xlm-roberta-base-squad2',\n",
       "  'sha': '9ecd20ab82354b46f26dbd6fab132951c34e3435',\n",
       "  'lastModified': '2021-10-21T12:16:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'deepset/xlm-roberta-base-squad2',\n",
       "  'downloads': 51977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-small': {'modelId': 'prajjwal1/bert-small',\n",
       "  'sha': '0ec5f86f27c1a77d704439db5e01c307ea11b9d4',\n",
       "  'lastModified': '2021-10-27T18:31:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-small',\n",
       "  'downloads': 51434,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rsvp-ai/bertserini-bert-base-squad': {'modelId': 'rsvp-ai/bertserini-bert-base-squad',\n",
       "  'sha': '1c93f9f29544f8ce8d6ee99133f91e5bd4dfed36',\n",
       "  'lastModified': '2022-06-23T14:13:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'rsvp-ai',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'rsvp-ai/bertserini-bert-base-squad',\n",
       "  'downloads': 51080,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-2.0-en': {'modelId': 'nghuyong/ernie-2.0-en',\n",
       "  'sha': 'c18a9f28b99a65011e3a6c61e2109f03833a447b',\n",
       "  'lastModified': '2021-05-20T01:42:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1907.12412',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-2.0-en',\n",
       "  'downloads': 50501,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/opt-125m': {'modelId': 'facebook/opt-125m',\n",
       "  'sha': '934b6a077313f3ee660a918a95313f5d0b136c5a',\n",
       "  'lastModified': '2022-06-22T09:52:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-125m',\n",
       "  'downloads': 50319,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/pos-english': {'modelId': 'flair/pos-english',\n",
       "  'sha': '22061e2903f36383754ba0101bc988c432aa4e06',\n",
       "  'lastModified': '2021-03-02T22:20:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/pos-english',\n",
       "  'downloads': 50238,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'I love Berlin.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'I love Berlin.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'classla/bcms-bertic-ner': {'modelId': 'classla/bcms-bertic-ner',\n",
       "  'sha': '4bd46a99b73827a3f6a095ceafa08b6933986dc0',\n",
       "  'lastModified': '2022-02-04T14:26:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'token-classification',\n",
       "   'hr',\n",
       "   'bs',\n",
       "   'sr',\n",
       "   'cnr',\n",
       "   'hbs',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'classla',\n",
       "  'config': {'architectures': ['ElectraForTokenClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'classla/bcms-bertic-ner',\n",
       "  'downloads': 50224,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Zovem se Marko i ≈æivim u Zagrebu. Studirao sam u Beogradu na Filozofskom fakultetu. Obo≈æavam album Moanin.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['hr', 'bs', 'sr', 'cnr', 'hbs'],\n",
       "   'widget': [{'text': 'Zovem se Marko i ≈æivim u Zagrebu. Studirao sam u Beogradu na Filozofskom fakultetu. Obo≈æavam album Moanin.'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/led-base-16384': {'modelId': 'allenai/led-base-16384',\n",
       "  'sha': '25756ed025a94fdf2bc4987af86a58fd999047ec',\n",
       "  'lastModified': '2021-01-11T14:51:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-base-16384',\n",
       "  'downloads': 50216,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-roberta-wwm-ext-large': {'modelId': 'hfl/chinese-roberta-wwm-ext-large',\n",
       "  'sha': 'a25cc9e05974bd9687e528edd516f2cfdb3f5db9',\n",
       "  'lastModified': '2022-03-01T09:15:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-roberta-wwm-ext-large',\n",
       "  'downloads': 50028,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english': {'modelId': 'flair/ner-english',\n",
       "  'sha': '627fd305bf597ea90fa54a50228ccfd4b412caf5',\n",
       "  'lastModified': '2021-03-02T22:11:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english',\n",
       "  'downloads': 49427,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cmarkea/distilcamembert-base-ner': {'modelId': 'cmarkea/distilcamembert-base-ner',\n",
       "  'sha': '21167ca4a0fd71a615e579dc4898c4079e86b014',\n",
       "  'lastModified': '2022-05-24T15:55:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base-ner',\n",
       "  'downloads': 48855,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Boulanger, habitant √† Boulanger et travaillant dans le magasin Boulanger situ√© dans la ville de Boulanger. Boulanger a √©crit le livre √©ponyme Boulanger √©dit√© par la maison d'√©dition Boulanger.\"},\n",
       "   {'text': \"Quentin Jerome Tarantino na√Æt le 27 mars 1963 √† Knoxville, dans le Tennessee. Il est le fils de Connie McHugh, une infirmi√®re, n√©e le 3 septembre 1946, et de Tony Tarantino, acteur et musicien amateur n√© √† New York. Ce dernier est d'origine italienne par son p√®re ; sa m√®re a des ascendances irlandaises et cherokees. Il est pr√©nomm√© d'apr√®s Quint Asper, le personnage jou√© par Burt Reynolds dans la s√©rie Gunsmoke et Quentin Compson, personnage du roman Le Bruit et la Fureur. Son p√®re quitte le domicile familial avant m√™me sa naissance. En 1965, sa m√®re d√©m√©nage √† Torrance, dans la banlieue sud de Los Angeles, et se remarie avec Curtis Zastoupil, un pianiste de bar, qui lui fait d√©couvrir le cin√©ma. Le couple divorce alors que le jeune Quentin a une dizaine d'ann√©es.\"}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Boulanger, habitant √† Boulanger et travaillant dans le magasin Boulanger situ√© dans la ville de Boulanger. Boulanger a √©crit le livre √©ponyme Boulanger √©dit√© par la maison d'√©dition Boulanger.\"},\n",
       "    {'text': \"Quentin Jerome Tarantino na√Æt le 27 mars 1963 √† Knoxville, dans le Tennessee. Il est le fils de Connie McHugh, une infirmi√®re, n√©e le 3 septembre 1946, et de Tony Tarantino, acteur et musicien amateur n√© √† New York. Ce dernier est d'origine italienne par son p√®re ; sa m√®re a des ascendances irlandaises et cherokees. Il est pr√©nomm√© d'apr√®s Quint Asper, le personnage jou√© par Burt Reynolds dans la s√©rie Gunsmoke et Quentin Compson, personnage du roman Le Bruit et la Fureur. Son p√®re quitte le domicile familial avant m√™me sa naissance. En 1965, sa m√®re d√©m√©nage √† Torrance, dans la banlieue sud de Los Angeles, et se remarie avec Curtis Zastoupil, un pianiste de bar, qui lui fait d√©couvrir le cin√©ma. Le couple divorce alors que le jeune Quentin a une dizaine d'ann√©es.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-uncased-contracts': {'modelId': 'nlpaueb/bert-base-uncased-contracts',\n",
       "  'sha': 'f918d2e0cf491ba2c5fcf9f82d5e9603b8c5f3ea',\n",
       "  'lastModified': '2022-04-28T14:43:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-uncased-contracts',\n",
       "  'downloads': 48782,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'This [MASK] Agreement is between General Motors and John Murray.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'This [MASK] Agreement is between General Motors and John Murray.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/deberta-v3-small': {'modelId': 'microsoft/deberta-v3-small',\n",
       "  'sha': '23bfba973812a80178eb6c2c600e85cc461ffc2c',\n",
       "  'lastModified': '2022-01-13T17:59:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-small',\n",
       "  'downloads': 48513,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sagorsarker/bangla-bert-base': {'modelId': 'sagorsarker/bangla-bert-base',\n",
       "  'sha': '315fa6f024884c29b34a3909a016decc2b068222',\n",
       "  'lastModified': '2021-09-22T09:37:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'bn',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1810.04805',\n",
       "   'arxiv:2012.14353',\n",
       "   'arxiv:2104.08613',\n",
       "   'arxiv:2107.03844',\n",
       "   'arxiv:2101.00204',\n",
       "   'transformers',\n",
       "   'bengali',\n",
       "   'bengali-lm',\n",
       "   'bangla',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sagorsarker',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'sagorsarker/bangla-bert-base',\n",
       "  'downloads': 47979,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü [MASK] ‡¶ó‡¶æ‡¶á‡•§'},\n",
       "   {'text': '‡¶Ü‡¶Æ‡¶ø [MASK] ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø‡•§ '}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'bn',\n",
       "   'tags': ['bert', 'bengali', 'bengali-lm', 'bangla'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['common_crawl', 'wikipedia', 'oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'IlyaGusev/mbart_ru_sum_gazeta': {'modelId': 'IlyaGusev/mbart_ru_sum_gazeta',\n",
       "  'sha': '56aba7873fb373f9324f3e4c3880eccbf20c316d',\n",
       "  'lastModified': '2021-12-15T09:58:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'ru-RU',\n",
       "   'dataset:IlyaGusev/gazeta',\n",
       "   'arxiv:2006.11063',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'IlyaGusev',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'IlyaGusev/mbart_ru_sum_gazeta',\n",
       "  'downloads': 47915,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–í—ã—Å–æ—Ç–∞ –±–∞—à–Ω–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 324 –º–µ—Ç—Ä–∞ (1063 —Ñ—É—Ç–∞), –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∞—è –∂–µ –≤—ã—Å–æ—Ç–∞, –∫–∞–∫ —É 81-—ç—Ç–∞–∂–Ω–æ–≥–æ –∑–¥–∞–Ω–∏—è, –∏ —Å–∞–º–æ–µ –≤—ã—Å–æ–∫–æ–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ –≤ –ü–∞—Ä–∏–∂–µ. –ï–≥–æ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ, —Ä–∞–∑–º–µ—Ä–æ–º 125 –º–µ—Ç—Ä–æ–≤ (410 —Ñ—É—Ç–æ–≤) —Å –ª—é–±–æ–π —Å—Ç–æ—Ä–æ–Ω—ã. –í–æ –≤—Ä–µ–º—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –≠–π—Ñ–µ–ª–µ–≤–∞ –±–∞—à–Ω—è –ø—Ä–µ–≤–∑–æ—à–ª–∞ –º–æ–Ω—É–º–µ–Ω—Ç –í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞, —Å—Ç–∞–≤ —Å–∞–º—ã–º –≤—ã—Å–æ–∫–∏–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ–º –≤ –º–∏—Ä–µ, –∏ —ç—Ç–æ—Ç —Ç–∏—Ç—É–ª –æ–Ω–∞ —É–¥–µ—Ä–∂–∏–≤–∞–ª–∞ –≤ —Ç–µ—á–µ–Ω–∏–µ 41 –≥–æ–¥–∞ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–¥–∞–Ω–∏—è –ö—Ä–∞–π—Å–ª–µ—Ä –≤ –ù—å—é-–ô–æ—Ä–∫–µ –≤ 1930 –≥–æ–¥—É. –≠—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–µ –¥–æ—Å—Ç–∏–≥–ª–æ –≤—ã—Å–æ—Ç—ã 300 –º–µ—Ç—Ä–æ–≤. –ò–∑-–∑–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ—â–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω—Ç–µ–Ω–Ω—ã –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –±–∞—à–Ω–∏ –≤ 1957 –≥–æ–¥—É –æ–Ω–∞ —Å–µ–π—á–∞—Å –≤—ã—à–µ –∑–¥–∞–Ω–∏—è –ö—Ä–∞–π—Å–ª–µ—Ä –Ω–∞ 5,2 –º–µ—Ç—Ä–∞ (17 —Ñ—É—Ç–æ–≤). –ó–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ø–µ—Ä–µ–¥–∞—Ç—á–∏–∫–æ–≤, –≠–π—Ñ–µ–ª–µ–≤–∞ –±–∞—à–Ω—è —è–≤–ª—è–µ—Ç—Å—è –≤—Ç–æ—Ä–æ–π —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –≤–æ –§—Ä–∞–Ω—Ü–∏–∏ –ø–æ—Å–ª–µ –≤–∏–∞–¥—É–∫–∞ –ú–∏–π–æ.',\n",
       "    'example_title': '–í–∏–∫–∏–ø–µ–¥–∏—è'},\n",
       "   {'text': '–° 1 —Å–µ–Ω—Ç—è–±—Ä—è –≤ –†–æ—Å—Å–∏–∏ –≤—Å—Ç—É–ø–∞—é—Ç –≤ —Å–∏–ª—É –ø–æ–ø—Ä–∞–≤–∫–∏ –≤ –∑–∞–∫–æ–Ω ¬´–û –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ¬ª ‚Äî —Ç–µ–ø–µ—Ä—å –¥–æ–ª–∂–Ω–∏–∫–∏ —Å–º–æ–≥—É—Ç –æ—Å–≤–æ–±–æ–∂–¥–∞—Ç—å—Å—è –æ—Ç –Ω–µ–ø–æ—Å–∏–ª—å–Ω—ã—Ö –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤–æ –≤–Ω–µ—Å—É–¥–µ–±–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –µ—Å–ª–∏ —Å—É–º–º–∞ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ –º–µ–Ω–µ–µ 50 —Ç—ã—Å. —Ä—É–±–ª–µ–π –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 500 —Ç—ã—Å. —Ä—É–±–ª–µ–π –±–µ–∑ —É—á–µ—Ç–∞ —à—Ç—Ä–∞—Ñ–æ–≤, –ø–µ–Ω–∏, –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∑–∞ –ø—Ä–æ—Å—Ä–æ—á–∫—É –ø–ª–∞—Ç–µ–∂–∞ –∏ –ø—Ä–æ—á–∏—Ö –∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–ª–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏–π. –£ —Ñ–∏–∑–ª–∏—Ü –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π –ø–æ—è–≤–∏–ª–∞—Å—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–π—Ç–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–∞ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —Å—É–¥–∞ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ú–§–¶. –°—É–º–º—É –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∑–∞—è–≤–∏—Ç–µ–ª—é –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ï—Å–ª–∏ –≤—Å–µ —É—Å–ª–æ–≤–∏—è —Å–æ–±–ª—é–¥–µ–Ω—ã, —Å–≤–µ–¥–µ–Ω–∏—è –≤–Ω–µ—Å—É—Ç –≤ –ï–¥–∏–Ω—ã–π —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä –≤ —Ç–µ—á–µ–Ω–∏–µ —Ç—Ä–µ—Ö —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π. –ü—Ä–∏ —ç—Ç–æ–º –Ω–∞ –º–æ–º–µ–Ω—Ç –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–ª–µ–Ω–∏—è –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∑–∞—è–≤–∏—Ç–µ–ª—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–Ω—á–µ–Ω–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤–∑—ã—Å–∫–∞—Ç–µ–ª—é. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –±–∞–Ω–∫—Ä–æ—Ç–∞ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏–º—É—â–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –≤–∑—ã—Å–∫–∞—Ç—å. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω–∞ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–æ–∑–±—É–∂–¥–µ–Ω–æ –¥—Ä—É–≥–æ–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ. –í –ø–µ—Ä–∏–æ–¥ –≤—Å–µ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞—è–≤–∏—Ç–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –±—Ä–∞—Ç—å –∑–∞–π–º—ã, –∫—Ä–µ–¥–∏—Ç—ã, –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ—Ä—É—á–∏—Ç–µ–ª—å—Å—Ç–≤–∞, —Å–æ–≤–µ—Ä—à–∞—Ç—å –∏–Ω—ã–µ –æ–±–µ—Å–ø–µ—á–∏—Ç–µ–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏. –í–Ω–µ—Å—É–¥–µ–±–Ω–æ–µ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ –±—É–¥–µ—Ç –¥–ª–∏—Ç—å—Å—è —à–µ—Å—Ç—å –º–µ—Å—è—Ü–µ–≤, –≤ —Ç–µ—á–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞–∫–∂–µ –±—É–¥–µ—Ç –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –º–æ—Ä–∞—Ç–æ—Ä–∏–π –Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤, –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏ –¥–æ–ª–∂–Ω–∏–∫–∞, –∏ –º–æ—Ä–∞—Ç–æ—Ä–∏–π –æ–± —É–ø–ª–∞—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–µ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ –Ω–µ—É—Å—Ç–æ–µ–∫ –∏ –∏–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏–π; –∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∑—ã—Å–∫–∞–Ω–∏—è (–∫—Ä–æ–º–µ –∞–ª–∏–º–µ–Ω—Ç–æ–≤) —Ç–∞–∫–∂–µ –±—É–¥—É—Ç –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞—è–≤–∏—Ç–µ–ª—è –æ—Å–≤–æ–±–æ–¥—è—Ç –æ—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏ –æ –ø—Ä–∏–∑–Ω–∞–Ω–∏–∏ –µ–≥–æ –±–∞–Ω–∫—Ä–æ—Ç–æ–º, –∞ —ç—Ç–∞ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–µ—Ç—Å—è –±–µ–∑–Ω–∞–¥–µ–∂–Ω–æ–π. –í –ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ –∑–∞ –ø–µ—Ä–≤–æ–µ –ø–æ–ª—É–≥–æ–¥–∏–µ 2020 –≥–æ–¥–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Å—É–¥—ã –ø—Ä–∏–∑–Ω–∞–ª–∏ –±–∞–Ω–∫—Ä–æ—Ç–∞–º–∏ 42,7 —Ç—ã—Å. –≥—Ä–∞–∂–¥–∞–Ω (–≤ —Ç–æ–º —á–∏—Å–ª–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π) ‚Äî –ø–æ –¥–∞–Ω–Ω—ã–º –µ–¥–∏–Ω–æ–≥–æ —Ä–µ–µ—Å—Ç—Ä–∞ ¬´–§–µ–¥—Ä–µ—Å—É—Ä—Å¬ª, —ç—Ç–æ –Ω–∞ 47,2% –±–æ–ª—å—à–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ 2019 –≥–æ–¥–∞. –†–æ—Å—Ç —á–∏—Å–ª–∞ –æ–±–∞–Ω–∫—Ä–æ—Ç–∏–≤—à–∏—Ö—Å—è –≥—Ä–∞–∂–¥–∞–Ω –≤–æ –≤—Ç–æ—Ä–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–µ—Ä–≤—ã–º –∑–∞–º–µ–¥–ª–∏–ª—Å—è ‚Äî —Ç–∞–∫–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞ —Ç–µ–º, —á—Ç–æ –≤ –ø–µ—Ä–∏–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å 19 –º–∞—Ä—Ç–∞ –ø–æ 11 –º–∞—è —Å—É–¥—ã —Ä–µ–¥–∫–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –±–∞–Ω–∫—Ä–æ—Ç–Ω—ã–µ –¥–µ–ª–∞ –∫–æ–º–ø–∞–Ω–∏–π –∏ –º–µ–Ω—å—à–µ, —á–µ–º –æ–±—ã—á–Ω–æ, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –≥—Ä–∞–∂–¥–∞–Ω, –æ–±—ä—è—Å–Ω—è–ª —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ ¬´–§–µ–¥—Ä–µ—Å—É—Ä—Å¬ª –ê–ª–µ–∫—Å–µ–π –Æ—Ö–Ω–∏–Ω. –û–Ω –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç, —á—Ç–æ –≤–æ –≤—Ç–æ—Ä–æ–º –ø–æ–ª—É–≥–æ–¥–∏–∏ –º—ã —É–≤–∏–¥–∏–º —Ä–æ—Å—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è, –∫–æ–≥–¥–∞ —Å—É–¥—ã —Ä–∞—Å—Å–º–æ—Ç—Ä—è—Ç –≤—Å–µ –¥–µ–ª–∞, —á—Ç–æ –Ω–µ —Å–º–æ–≥–ª–∏ —Ä–∞–Ω–µ–µ –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ü–æ –µ–≥–æ –¥–∞–Ω–Ω—ã–º, —É–∂–µ –≤ –∏—é–Ω–µ —á–∏—Å–ª–æ –ª–∏—á–Ω—ã—Ö –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤ –≤—ã—Ä–æ—Å–ª–æ –¥–æ 11,5 —Ç—ã—Å., —á—Ç–æ –≤ –¥–≤–∞ —Ä–∞–∑–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ 2019 –≥–æ–¥–∞.',\n",
       "    'example_title': '–ù–æ–≤–æ—Å—Ç–∏'},\n",
       "   {'text': '–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã. –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–≥—Ä–∞–µ—Ç –≤—Å–µ –±–æ–ª—å—à—É—é  —Ä–æ–ª—å –≤–æ –≤—Å–µ—Ö —Å—Ñ–µ—Ä–∞—Ö –∂–∏–∑–Ω–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞. –í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –æ–±—ä–µ–º –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–º –≤–∏–¥–µ –≤–æ–∑—Ä–æ—Å –Ω–∞—Å—Ç–æ–ª—å–∫–æ, —á—Ç–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —É–≥—Ä–æ–∑–∞ –æ–±–µ—Å—Ü–µ–Ω–∏–≤–∞–Ω–∏—è —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–≤—è–∑–∏ —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø–æ–∏—Å–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π —Å—Ä–µ–¥–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –†–∞–∑–≤–∏—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ —É—Å—É–≥—É–±–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏. –í —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∂–∞—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤‚Äì—Ä–µ—Ñ–µ—Ä–∞—Ç–æ–≤ (–∞–Ω–Ω–æ—Ç–∞—Ü–∏–π). –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞  –ø—Ä–æ–±–ª–µ–º—ã  –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–ø—ã—Ç–∫–∏ –µ–µ —Ä–µ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–ª–∏—Å—å –º–Ω–æ–≥–∏–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏. –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è  –Ω–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–∂–µ –±–æ–ª–µ–µ 50 –ª–µ—Ç –∏ —Å–≤—è–∑–∞–Ω–∞ —Å –∏–º–µ–Ω–∞–º–∏ —Ç–∞–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –∫–∞–∫ –ì.–ü. –õ—É–Ω, –í.–ï. –ë–µ—Ä–∑–æ–Ω, –ò.–ü. C–µ–≤–±–æ, –≠.–§. –°–∫–æ—Ä–æ—Ö–æ–¥—å–∫–æ, –î.–ì. –õ–∞—Ö—É—Ç–∏, –†.–ì. –ü–∏–æ—Ç—Ä–æ–≤—Å–∫–∏–π –∏ –¥—Ä. –ó–∞ —ç—Ç–∏ –≥–æ–¥—ã  –≤—ã—Ä–∞–±–æ—Ç–∞–Ω—ã  –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é –¥–∞–Ω–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–µ—Ç–∫–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ –¥–≤–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ —ç–∫—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑ –ø–µ—Ä–≤–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ¬´–Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö¬ª —Ñ—Ä–∞–∑ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤), —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç; –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –≤—ã–¥–µ–ª–µ–Ω–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ—Ä–æ–∂–¥–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (—Ä–µ—Ñ–µ—Ä–∞—Ç–æ–≤), —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ –æ–±–æ–±—â–∞—é—â–∏—Ö –ø–µ—Ä–≤–∏—á–Ω—ã–µ  –¥–æ–∫—É–º–µ–Ω—Ç—ã.',\n",
       "    'example_title': '–ù–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'ru-RU'],\n",
       "   'tags': ['summarization', 'mbart'],\n",
       "   'datasets': ['IlyaGusev/gazeta'],\n",
       "   'license': 'apache-2.0',\n",
       "   'inference': {'parameters': {'no_repeat_ngram_size': 4}},\n",
       "   'widget': [{'text': '–í—ã—Å–æ—Ç–∞ –±–∞—à–Ω–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 324 –º–µ—Ç—Ä–∞ (1063 —Ñ—É—Ç–∞), –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∞—è –∂–µ –≤—ã—Å–æ—Ç–∞, –∫–∞–∫ —É 81-—ç—Ç–∞–∂–Ω–æ–≥–æ –∑–¥–∞–Ω–∏—è, –∏ —Å–∞–º–æ–µ –≤—ã—Å–æ–∫–æ–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ –≤ –ü–∞—Ä–∏–∂–µ. –ï–≥–æ –æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ, —Ä–∞–∑–º–µ—Ä–æ–º 125 –º–µ—Ç—Ä–æ–≤ (410 —Ñ—É—Ç–æ–≤) —Å –ª—é–±–æ–π —Å—Ç–æ—Ä–æ–Ω—ã. –í–æ –≤—Ä–µ–º—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –≠–π—Ñ–µ–ª–µ–≤–∞ –±–∞—à–Ω—è –ø—Ä–µ–≤–∑–æ—à–ª–∞ –º–æ–Ω—É–º–µ–Ω—Ç –í–∞—à–∏–Ω–≥—Ç–æ–Ω–∞, —Å—Ç–∞–≤ —Å–∞–º—ã–º –≤—ã—Å–æ–∫–∏–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ–º –≤ –º–∏—Ä–µ, –∏ —ç—Ç–æ—Ç —Ç–∏—Ç—É–ª –æ–Ω–∞ —É–¥–µ—Ä–∂–∏–≤–∞–ª–∞ –≤ —Ç–µ—á–µ–Ω–∏–µ 41 –≥–æ–¥–∞ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–¥–∞–Ω–∏—è –ö—Ä–∞–π—Å–ª–µ—Ä –≤ –ù—å—é-–ô–æ—Ä–∫–µ –≤ 1930 –≥–æ–¥—É. –≠—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–µ –¥–æ—Å—Ç–∏–≥–ª–æ –≤—ã—Å–æ—Ç—ã 300 –º–µ—Ç—Ä–æ–≤. –ò–∑-–∑–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–µ—â–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω—Ç–µ–Ω–Ω—ã –Ω–∞ –≤–µ—Ä—à–∏–Ω–µ –±–∞—à–Ω–∏ –≤ 1957 –≥–æ–¥—É –æ–Ω–∞ —Å–µ–π—á–∞—Å –≤—ã—à–µ –∑–¥–∞–Ω–∏—è –ö—Ä–∞–π—Å–ª–µ—Ä –Ω–∞ 5,2 –º–µ—Ç—Ä–∞ (17 —Ñ—É—Ç–æ–≤). –ó–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –ø–µ—Ä–µ–¥–∞—Ç—á–∏–∫–æ–≤, –≠–π—Ñ–µ–ª–µ–≤–∞ –±–∞—à–Ω—è —è–≤–ª—è–µ—Ç—Å—è –≤—Ç–æ—Ä–æ–π —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –≤–æ –§—Ä–∞–Ω—Ü–∏–∏ –ø–æ—Å–ª–µ –≤–∏–∞–¥—É–∫–∞ –ú–∏–π–æ.',\n",
       "     'example_title': '–í–∏–∫–∏–ø–µ–¥–∏—è'},\n",
       "    {'text': '–° 1 —Å–µ–Ω—Ç—è–±—Ä—è –≤ –†–æ—Å—Å–∏–∏ –≤—Å—Ç—É–ø–∞—é—Ç –≤ —Å–∏–ª—É –ø–æ–ø—Ä–∞–≤–∫–∏ –≤ –∑–∞–∫–æ–Ω ¬´–û –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–µ¬ª ‚Äî —Ç–µ–ø–µ—Ä—å –¥–æ–ª–∂–Ω–∏–∫–∏ —Å–º–æ–≥—É—Ç –æ—Å–≤–æ–±–æ–∂–¥–∞—Ç—å—Å—è –æ—Ç –Ω–µ–ø–æ—Å–∏–ª—å–Ω—ã—Ö –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤–æ –≤–Ω–µ—Å—É–¥–µ–±–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –µ—Å–ª–∏ —Å—É–º–º–∞ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ –º–µ–Ω–µ–µ 50 —Ç—ã—Å. —Ä—É–±–ª–µ–π –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 500 —Ç—ã—Å. —Ä—É–±–ª–µ–π –±–µ–∑ —É—á–µ—Ç–∞ —à—Ç—Ä–∞—Ñ–æ–≤, –ø–µ–Ω–∏, –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∑–∞ –ø—Ä–æ—Å—Ä–æ—á–∫—É –ø–ª–∞—Ç–µ–∂–∞ –∏ –ø—Ä–æ—á–∏—Ö –∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–ª–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏–π. –£ —Ñ–∏–∑–ª–∏—Ü –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π –ø–æ—è–≤–∏–ª–∞—Å—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–π—Ç–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–∞ –±–µ–∑ —É—á–∞—Å—Ç–∏—è —Å—É–¥–∞ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ú–§–¶. –°—É–º–º—É –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∑–∞—è–≤–∏—Ç–µ–ª—é –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ï—Å–ª–∏ –≤—Å–µ —É—Å–ª–æ–≤–∏—è —Å–æ–±–ª—é–¥–µ–Ω—ã, —Å–≤–µ–¥–µ–Ω–∏—è –≤–Ω–µ—Å—É—Ç –≤ –ï–¥–∏–Ω—ã–π —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π —Ä–µ–µ—Å—Ç—Ä –≤ —Ç–µ—á–µ–Ω–∏–µ —Ç—Ä–µ—Ö —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π. –ü—Ä–∏ —ç—Ç–æ–º –Ω–∞ –º–æ–º–µ–Ω—Ç –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–ª–µ–Ω–∏—è –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –∑–∞—è–≤–∏—Ç–µ–ª—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–Ω—á–µ–Ω–æ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —Å –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤–∑—ã—Å–∫–∞—Ç–µ–ª—é. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —É –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –±–∞–Ω–∫—Ä–æ—Ç–∞ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏–º—É—â–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–Ω–æ –≤–∑—ã—Å–∫–∞—Ç—å. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω–∞ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–æ–∑–±—É–∂–¥–µ–Ω–æ –¥—Ä—É–≥–æ–µ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ. –í –ø–µ—Ä–∏–æ–¥ –≤—Å–µ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞—è–≤–∏—Ç–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –±—Ä–∞—Ç—å –∑–∞–π–º—ã, –∫—Ä–µ–¥–∏—Ç—ã, –≤—ã–¥–∞–≤–∞—Ç—å –ø–æ—Ä—É—á–∏—Ç–µ–ª—å—Å—Ç–≤–∞, —Å–æ–≤–µ—Ä—à–∞—Ç—å –∏–Ω—ã–µ –æ–±–µ—Å–ø–µ—á–∏—Ç–µ–ª—å–Ω—ã–µ —Å–¥–µ–ª–∫–∏. –í–Ω–µ—Å—É–¥–µ–±–Ω–æ–µ –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ –±—É–¥–µ—Ç –¥–ª–∏—Ç—å—Å—è —à–µ—Å—Ç—å –º–µ—Å—è—Ü–µ–≤, –≤ —Ç–µ—á–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–∞–∫–∂–µ –±—É–¥–µ—Ç –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –º–æ—Ä–∞—Ç–æ—Ä–∏–π –Ω–∞ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤, –æ—Ç–º–µ—á–µ–Ω–Ω—ã—Ö –≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏ –¥–æ–ª–∂–Ω–∏–∫–∞, –∏ –º–æ—Ä–∞—Ç–æ—Ä–∏–π –æ–± —É–ø–ª–∞—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–µ–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–∫—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ –Ω–µ—É—Å—Ç–æ–µ–∫ –∏ –∏–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏–π; –∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∑—ã—Å–∫–∞–Ω–∏—è (–∫—Ä–æ–º–µ –∞–ª–∏–º–µ–Ω—Ç–æ–≤) —Ç–∞–∫–∂–µ –±—É–¥—É—Ç –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ü–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∑–∞—è–≤–∏—Ç–µ–ª—è –æ—Å–≤–æ–±–æ–¥—è—Ç –æ—Ç –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫—Ä–µ–¥–∏—Ç–æ—Ä–æ–≤, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ –∑–∞—è–≤–ª–µ–Ω–∏–∏ –æ –ø—Ä–∏–∑–Ω–∞–Ω–∏–∏ –µ–≥–æ –±–∞–Ω–∫—Ä–æ—Ç–æ–º, –∞ —ç—Ç–∞ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–µ—Ç—Å—è –±–µ–∑–Ω–∞–¥–µ–∂–Ω–æ–π. –í –ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ –∑–∞ –ø–µ—Ä–≤–æ–µ –ø–æ–ª—É–≥–æ–¥–∏–µ 2020 –≥–æ–¥–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Å—É–¥—ã –ø—Ä–∏–∑–Ω–∞–ª–∏ –±–∞–Ω–∫—Ä–æ—Ç–∞–º–∏ 42,7 —Ç—ã—Å. –≥—Ä–∞–∂–¥–∞–Ω (–≤ —Ç–æ–º —á–∏—Å–ª–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π) ‚Äî –ø–æ –¥–∞–Ω–Ω—ã–º –µ–¥–∏–Ω–æ–≥–æ —Ä–µ–µ—Å—Ç—Ä–∞ ¬´–§–µ–¥—Ä–µ—Å—É—Ä—Å¬ª, —ç—Ç–æ –Ω–∞ 47,2% –±–æ–ª—å—à–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ 2019 –≥–æ–¥–∞. –†–æ—Å—Ç —á–∏—Å–ª–∞ –æ–±–∞–Ω–∫—Ä–æ—Ç–∏–≤—à–∏—Ö—Å—è –≥—Ä–∞–∂–¥–∞–Ω –≤–æ –≤—Ç–æ—Ä–æ–º –∫–≤–∞—Ä—Ç–∞–ª–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–µ—Ä–≤—ã–º –∑–∞–º–µ–¥–ª–∏–ª—Å—è ‚Äî —Ç–∞–∫–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞ —Ç–µ–º, —á—Ç–æ –≤ –ø–µ—Ä–∏–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å 19 –º–∞—Ä—Ç–∞ –ø–æ 11 –º–∞—è —Å—É–¥—ã —Ä–µ–¥–∫–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –±–∞–Ω–∫—Ä–æ—Ç–Ω—ã–µ –¥–µ–ª–∞ –∫–æ–º–ø–∞–Ω–∏–π –∏ –º–µ–Ω—å—à–µ, —á–µ–º –æ–±—ã—á–Ω–æ, –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –≥—Ä–∞–∂–¥–∞–Ω, –æ–±—ä—è—Å–Ω—è–ª —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ ¬´–§–µ–¥—Ä–µ—Å—É—Ä—Å¬ª –ê–ª–µ–∫—Å–µ–π –Æ—Ö–Ω–∏–Ω. –û–Ω –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç, —á—Ç–æ –≤–æ –≤—Ç–æ—Ä–æ–º –ø–æ–ª—É–≥–æ–¥–∏–∏ –º—ã —É–≤–∏–¥–∏–º —Ä–æ—Å—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è, –∫–æ–≥–¥–∞ —Å—É–¥—ã —Ä–∞—Å—Å–º–æ—Ç—Ä—è—Ç –≤—Å–µ –¥–µ–ª–∞, —á—Ç–æ –Ω–µ —Å–º–æ–≥–ª–∏ —Ä–∞–Ω–µ–µ –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ü–æ –µ–≥–æ –¥–∞–Ω–Ω—ã–º, —É–∂–µ –≤ –∏—é–Ω–µ —á–∏—Å–ª–æ –ª–∏—á–Ω—ã—Ö –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤ –≤—ã—Ä–æ—Å–ª–æ –¥–æ 11,5 —Ç—ã—Å., —á—Ç–æ –≤ –¥–≤–∞ —Ä–∞–∑–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ 2019 –≥–æ–¥–∞.',\n",
       "     'example_title': '–ù–æ–≤–æ—Å—Ç–∏'},\n",
       "    {'text': '–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã. –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–≥—Ä–∞–µ—Ç –≤—Å–µ –±–æ–ª—å—à—É—é  —Ä–æ–ª—å –≤–æ –≤—Å–µ—Ö —Å—Ñ–µ—Ä–∞—Ö –∂–∏–∑–Ω–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞. –í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –æ–±—ä–µ–º –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–º –≤–∏–¥–µ –≤–æ–∑—Ä–æ—Å –Ω–∞—Å—Ç–æ–ª—å–∫–æ, —á—Ç–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —É–≥—Ä–æ–∑–∞ –æ–±–µ—Å—Ü–µ–Ω–∏–≤–∞–Ω–∏—è —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Å–≤—è–∑–∏ —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø–æ–∏—Å–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–≤–µ–¥–µ–Ω–∏–π —Å—Ä–µ–¥–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤. –†–∞–∑–≤–∏—Ç–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ —É—Å—É–≥—É–±–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏. –í —ç—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∂–∞—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤‚Äì—Ä–µ—Ñ–µ—Ä–∞—Ç–æ–≤ (–∞–Ω–Ω–æ—Ç–∞—Ü–∏–π). –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞  –ø—Ä–æ–±–ª–µ–º—ã  –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–ø—ã—Ç–∫–∏ –µ–µ —Ä–µ—à–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞–ª–∏—Å—å –º–Ω–æ–≥–∏–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏. –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è  –Ω–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–∂–µ –±–æ–ª–µ–µ 50 –ª–µ—Ç –∏ —Å–≤—è–∑–∞–Ω–∞ —Å –∏–º–µ–Ω–∞–º–∏ —Ç–∞–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –∫–∞–∫ –ì.–ü. –õ—É–Ω, –í.–ï. –ë–µ—Ä–∑–æ–Ω, –ò.–ü. C–µ–≤–±–æ, –≠.–§. –°–∫–æ—Ä–æ—Ö–æ–¥—å–∫–æ, –î.–ì. –õ–∞—Ö—É—Ç–∏, –†.–ì. –ü–∏–æ—Ç—Ä–æ–≤—Å–∫–∏–π –∏ –¥—Ä. –ó–∞ —ç—Ç–∏ –≥–æ–¥—ã  –≤—ã—Ä–∞–±–æ—Ç–∞–Ω—ã  –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é –¥–∞–Ω–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–µ—Ç–∫–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ –¥–≤–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ —ç–∫—Å—Ç—Ä–∞–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑ –ø–µ—Ä–≤–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ¬´–Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö¬ª —Ñ—Ä–∞–∑ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤), —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—Ä–∞–∑—É–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç; –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –≤—ã–¥–µ–ª–µ–Ω–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –ø–æ—Ä–æ–∂–¥–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (—Ä–µ—Ñ–µ—Ä–∞—Ç–æ–≤), —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ –æ–±–æ–±—â–∞—é—â–∏—Ö –ø–µ—Ä–≤–∏—á–Ω—ã–µ  –¥–æ–∫—É–º–µ–Ω—Ç—ã.',\n",
       "     'example_title': '–ù–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-roberta-base-v2': {'modelId': 'sentence-transformers/stsb-roberta-base-v2',\n",
       "  'sha': 'b5e9e8dbc4a7d931c766a9113d1a04963a480c06',\n",
       "  'lastModified': '2022-06-15T20:05:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-base-v2',\n",
       "  'downloads': 47592,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-350m': {'modelId': 'facebook/opt-350m',\n",
       "  'sha': '10517ad5b51c8c6e02db7824d8494721d4874488',\n",
       "  'lastModified': '2022-06-16T15:25:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-350m',\n",
       "  'downloads': 47482,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Anjoe/gbert-large': {'modelId': 'Anjoe/gbert-large',\n",
       "  'sha': '0d03ce7b19d93c48c43bf2cd9640af58bcfaa536',\n",
       "  'lastModified': '2022-06-25T17:08:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Anjoe',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Anjoe/gbert-large',\n",
       "  'downloads': 47378,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': [{'name': 'gbert-large', 'results': []}],\n",
       "  'cardData': {'tags': ['generated_from_trainer'],\n",
       "   'model-index': [{'name': 'gbert-large', 'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/sentence-t5-large': {'modelId': 'sentence-transformers/sentence-t5-large',\n",
       "  'sha': '266640df151776ad39f66a2595b81c97ae678195',\n",
       "  'lastModified': '2022-02-09T14:01:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'en',\n",
       "   'arxiv:2108.08877',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['T5EncoderModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'sentence-transformers/sentence-t5-large',\n",
       "  'downloads': 47117,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Tatyana/rubert-base-cased-sentiment-new': {'modelId': 'Tatyana/rubert-base-cased-sentiment-new',\n",
       "  'sha': 'a1ff066aeb2b26b5f1b8d793862e51d77a1090d3',\n",
       "  'lastModified': '2021-05-30T23:12:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ru',\n",
       "   'dataset:Tatyana/ru_sentiment_dataset',\n",
       "   'transformers',\n",
       "   'sentiment'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Tatyana',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Tatyana/rubert-base-cased-sentiment-new',\n",
       "  'downloads': 46869,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '–¢—ã –º–Ω–µ –Ω—Ä–∞–≤–∏—à—å—Å—è. –Ø —Ç–µ–±—è –ª—é–±–ª—é'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['sentiment', 'text-classification'],\n",
       "   'datasets': ['Tatyana/ru_sentiment_dataset']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L3-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L3-v2',\n",
       "  'sha': '055d33c3b40d293cd55714774c68fcc2de81d463',\n",
       "  'lastModified': '2022-06-15T20:08:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L3-v2',\n",
       "  'downloads': 45848,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-fi-en': {'modelId': 'Helsinki-NLP/opus-mt-fi-en',\n",
       "  'sha': '7fb1e75696c8b8930df5afae6bb5d22ffca4ed30',\n",
       "  'lastModified': '2021-01-18T08:32:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'fi',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-fi-en',\n",
       "  'downloads': 45724,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fi', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-roberta': {'modelId': 'hf-internal-testing/tiny-random-roberta',\n",
       "  'sha': '73def02fc9f13169a1ce21ad4602aae38d7cbd5a',\n",
       "  'lastModified': '2021-09-17T19:22:24.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'hf-internal-testing/tiny-random-roberta',\n",
       "  'downloads': 45646,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1': {'modelId': 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       "  'sha': '9800b205abb21a898401af85073e2849699f999b',\n",
       "  'lastModified': '2022-06-10T08:43:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fi',\n",
       "   'arxiv:1912.07076',\n",
       "   'arxiv:1908.04212',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'TurkuNLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       "  'downloads': 45286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fi'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ar-en': {'modelId': 'Helsinki-NLP/opus-mt-ar-en',\n",
       "  'sha': '9c5efffe6f69dcb65d7156f40dfa27b54be34258',\n",
       "  'lastModified': '2021-09-09T21:26:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ar',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ar-en',\n",
       "  'downloads': 45269,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ÿ•ÿ≥ŸÖŸä ŸÖÿ≠ŸÖÿØ Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿ®ÿ±ŸÑŸäŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßÿ±Ÿá Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ŸÑŸÜÿØŸÜ'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-cc25': {'modelId': 'facebook/mbart-large-cc25',\n",
       "  'sha': '2df0e6dd8a0e7f6df056fe4d0d95941a04b64e4f',\n",
       "  'lastModified': '2021-03-10T03:48:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart',\n",
       "   'task_specific_params': {'translation_en_to_ro': {'decoder_start_token_id': 250020}}},\n",
       "  'id': 'facebook/mbart-large-cc25',\n",
       "  'downloads': 45090,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'],\n",
       "   'language': ['en',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-base-parsbert-uncased': {'modelId': 'HooshvareLab/bert-base-parsbert-uncased',\n",
       "  'sha': 'd73a0e2c7492c33bd5819bcdb23eba207404dd19',\n",
       "  'lastModified': '2021-05-18T20:47:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-base-parsbert-uncased',\n",
       "  'downloads': 45084,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepChem/ChemBERTa-10M-MTR': {'modelId': 'DeepChem/ChemBERTa-10M-MTR',\n",
       "  'sha': '53d08e205841e15b77483b42aac0c55bdb1d1822',\n",
       "  'lastModified': '2022-01-20T17:51:35.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepChem',\n",
       "  'config': {'architectures': ['RobertaForRegression'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'DeepChem/ChemBERTa-10M-MTR',\n",
       "  'downloads': 44792,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'RobertaForRegression',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/contriever': {'modelId': 'facebook/contriever',\n",
       "  'sha': '2bd46a25019aeea091fd42d1f0fd4801675cf699',\n",
       "  'lastModified': '2022-01-19T17:23:28.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'arxiv:2112.09118', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Contriever'], 'model_type': 'bert'},\n",
       "  'id': 'facebook/contriever',\n",
       "  'downloads': 44408,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'Contriever',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-sv-en': {'modelId': 'Helsinki-NLP/opus-mt-sv-en',\n",
       "  'sha': 'e4f28e50a1614873bbe8d16d3c48b52e37778ead',\n",
       "  'lastModified': '2021-09-10T14:06:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'sv',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-sv-en',\n",
       "  'downloads': 44343,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-xxl': {'modelId': 'google/t5-v1_1-xxl',\n",
       "  'sha': '6e80045f023a868fdb58e0b697d1ace5fe4880be',\n",
       "  'lastModified': '2020-11-19T19:55:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-xxl',\n",
       "  'downloads': 44096,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vicgalle/xlm-roberta-large-xnli-anli': {'modelId': 'vicgalle/xlm-roberta-large-xnli-anli',\n",
       "  'sha': '81de27372786f6a034c81c4bf1c53ebe9afa10d7',\n",
       "  'lastModified': '2021-03-04T17:05:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:mnli',\n",
       "   'dataset:xnli',\n",
       "   'dataset:anli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'vicgalle',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'vicgalle/xlm-roberta-large-xnli-anli',\n",
       "  'downloads': 44078,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus.',\n",
       "    'candidate_labels': 'violent, peaceful'},\n",
       "   {'text': 'La pel√≠cula empezaba bien pero termin√≥ siendo un desastre.',\n",
       "    'candidate_labels': 'positivo, negativo, neutral'},\n",
       "   {'text': 'La pel√≠cula empez√≥ siendo un desastre pero en general fue bien.',\n",
       "    'candidate_labels': 'positivo, negativo, neutral'},\n",
       "   {'text': '¬øA qui√©n vas a votar en 2020?',\n",
       "    'candidate_labels': 'Europa, elecciones, pol√≠tica, ciencia, deportes'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['zero-shot-classification', 'nli', 'pytorch'],\n",
       "   'datasets': ['mnli', 'xnli', 'anli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus.',\n",
       "     'candidate_labels': 'violent, peaceful'},\n",
       "    {'text': 'La pel√≠cula empezaba bien pero termin√≥ siendo un desastre.',\n",
       "     'candidate_labels': 'positivo, negativo, neutral'},\n",
       "    {'text': 'La pel√≠cula empez√≥ siendo un desastre pero en general fue bien.',\n",
       "     'candidate_labels': 'positivo, negativo, neutral'},\n",
       "    {'text': '¬øA qui√©n vas a votar en 2020?',\n",
       "     'candidate_labels': 'Europa, elecciones, pol√≠tica, ciencia, deportes'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-small-generator': {'modelId': 'google/electra-small-generator',\n",
       "  'sha': 'c04c64e3cca372b13615e71e51bc261f93905212',\n",
       "  'lastModified': '2021-04-29T15:23:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-small-generator',\n",
       "  'downloads': 43598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-electra': {'modelId': 'hf-internal-testing/tiny-random-electra',\n",
       "  'sha': 'e5efa6ccdff6b9cd0c387c90ff4af92686bc0c12',\n",
       "  'lastModified': '2021-09-17T19:22:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'electra', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hf-internal-testing/tiny-random-electra',\n",
       "  'downloads': 43449,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cointegrated/rubert-tiny': {'modelId': 'cointegrated/rubert-tiny',\n",
       "  'sha': 'f191937ca7511ae76b44b06a460f18ab1699c54b',\n",
       "  'lastModified': '2022-01-28T11:42:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'fill-mask',\n",
       "   'embeddings',\n",
       "   'masked-lm',\n",
       "   'tiny',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/rubert-tiny',\n",
       "  'downloads': 43411,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '–ú–∏–Ω–∏–∞—Ç—é—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è [MASK] —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['russian',\n",
       "    'fill-mask',\n",
       "    'pretraining',\n",
       "    'embeddings',\n",
       "    'masked-lm',\n",
       "    'tiny',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '–ú–∏–Ω–∏–∞—Ç—é—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è [MASK] —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-albert': {'modelId': 'hf-internal-testing/tiny-random-albert',\n",
       "  'sha': '213e37e1c19981fbcee4f5f9ebefee8db74c8db1',\n",
       "  'lastModified': '2021-09-17T19:23:59.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'albert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-albert',\n",
       "  'downloads': 43398,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/qnli-distilroberta-base': {'modelId': 'cross-encoder/qnli-distilroberta-base',\n",
       "  'sha': 'c7102de981e15ca7ef131517b94ff770d9e3c166',\n",
       "  'lastModified': '2021-08-05T08:41:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1804.07461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/qnli-distilroberta-base',\n",
       "  'downloads': 43242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vennify/t5-base-grammar-correction': {'modelId': 'vennify/t5-base-grammar-correction',\n",
       "  'sha': '9e4a09d21dca1072a69302df9261289d03c3ed78',\n",
       "  'lastModified': '2022-01-14T16:35:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:jfleg',\n",
       "   'arxiv:1702.04066',\n",
       "   'transformers',\n",
       "   'grammar',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'vennify',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'vennify/t5-base-grammar-correction',\n",
       "  'downloads': 43151,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['grammar', 'text2text-generation'],\n",
       "   'license': 'cc-by-nc-sa-4.0',\n",
       "   'datasets': ['jfleg']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-mpnet-base-cos-v1': {'modelId': 'sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
       "  'sha': 'bd0b4f6d767d5cb937b4c1a9611df492a80e891a',\n",
       "  'lastModified': '2021-08-24T21:07:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
       "  'downloads': 43087,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/graphcodebert-base': {'modelId': 'microsoft/graphcodebert-base',\n",
       "  'sha': '2ff24803553d2274dd118c7ea20e9b37a5804b11',\n",
       "  'lastModified': '2021-07-21T16:26:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/graphcodebert-base',\n",
       "  'downloads': 42870,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad': {'modelId': 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       "  'sha': 'ba9ccd18e456b6c6a63a3ea5b21776f05452d923',\n",
       "  'lastModified': '2021-05-18T16:22:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       "  'downloads': 42567,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v1': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
       "  'sha': '756c7aa7d57c27bd1c71a483367c53966465f450',\n",
       "  'lastModified': '2022-06-15T20:11:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
       "  'downloads': 42314,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-distilroberta-base': {'modelId': 'sshleifer/tiny-distilroberta-base',\n",
       "  'sha': 'd305c58110158c865cb6746c62d4511d4148a934',\n",
       "  'lastModified': '2021-10-22T16:10:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sshleifer/tiny-distilroberta-base',\n",
       "  'downloads': 42077,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-medium': {'modelId': 'prajjwal1/bert-medium',\n",
       "  'sha': 'ce27ec2944bd32b66ed837edb9c77eb7301b8ecc',\n",
       "  'lastModified': '2021-10-27T18:30:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-medium',\n",
       "  'downloads': 41519,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'antoiloui/belgpt2': {'modelId': 'antoiloui/belgpt2',\n",
       "  'sha': 'af72b5d53d2be0e47fac2df7367d7205cd73e8dd',\n",
       "  'lastModified': '2021-05-21T13:21:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'antoiloui',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'antoiloui/belgpt2',\n",
       "  'downloads': 41380,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hier, Elon Musk a'},\n",
       "   {'text': 'Pourquoi a-t-il'},\n",
       "   {'text': 'Tout √† coup, elle'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'license': ['mit'],\n",
       "   'widget': [{'text': 'Hier, Elon Musk a'},\n",
       "    {'text': 'Pourquoi a-t-il'},\n",
       "    {'text': 'Tout √† coup, elle'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-german-cased': {'modelId': 'distilbert-base-german-cased',\n",
       "  'sha': '06b1dc5ba050ddbf462d060df38f906eedb31b01',\n",
       "  'lastModified': '2022-06-03T09:46:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-german-cased',\n",
       "  'downloads': 41176,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased-distilled-squad': {'modelId': 'distilbert-base-uncased-distilled-squad',\n",
       "  'sha': '5fd13b8fe336f632c44f0e02c27cad425c56b3b1',\n",
       "  'lastModified': '2020-12-11T21:24:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tflite',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased-distilled-squad',\n",
       "  'downloads': 40995,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "   {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'widget': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "    {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/legal-bert-base-uncased': {'modelId': 'nlpaueb/legal-bert-base-uncased',\n",
       "  'sha': '15b570cbf88259610b082a167dacc190124f60f6',\n",
       "  'lastModified': '2022-04-28T14:42:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/legal-bert-base-uncased',\n",
       "  'downloads': 40895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/quora-distilroberta-base': {'modelId': 'cross-encoder/quora-distilroberta-base',\n",
       "  'sha': '2f10e5b229ecdb2ca204717607c7635897fd645b',\n",
       "  'lastModified': '2021-08-05T08:41:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/quora-distilroberta-base',\n",
       "  'downloads': 40842,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-small-discriminator': {'modelId': 'hfl/chinese-electra-180g-small-discriminator',\n",
       "  'sha': '19998e79c480fa7f3892b3c035e4362fe497efcf',\n",
       "  'lastModified': '2021-03-03T01:04:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-small-discriminator',\n",
       "  'downloads': 39659,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/ruRoberta-large': {'modelId': 'sberbank-ai/ruRoberta-large',\n",
       "  'sha': '29b46edec511391c384dfd0bbd3892cb72495c5f',\n",
       "  'lastModified': '2021-09-21T19:45:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sberbank-ai/ruRoberta-large',\n",
       "  'downloads': 39573,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç <mask> –∏ —è –∏–Ω–∂–µ–Ω–µ—Ä –∂–∏–≤—É—â–∏–π –≤ –ù—å—é-–ô–æ—Ä–∫–µ.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/model-zoo'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot_small-90M': {'modelId': 'facebook/blenderbot_small-90M',\n",
       "  'sha': 'a2a23a425b397872915db19bdee2522877eddc14',\n",
       "  'lastModified': '2021-12-02T08:09:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'blenderbot-small',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotSmallForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot-small'},\n",
       "  'id': 'facebook/blenderbot_small-90M',\n",
       "  'downloads': 39242,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ThatSkyFox/DialoGPT-small-joshua': {'modelId': 'ThatSkyFox/DialoGPT-small-joshua',\n",
       "  'sha': 'b42987a9651745cfa1112354b16a1c454492045f',\n",
       "  'lastModified': '2021-10-24T17:12:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'ThatSkyFox',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'ThatSkyFox/DialoGPT-small-joshua',\n",
       "  'downloads': 38983,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_bert_bfd': {'modelId': 'Rostlab/prot_bert_bfd',\n",
       "  'sha': '6c5c8a55a52ff08a664dfd584aa1773f125a0487',\n",
       "  'lastModified': '2020-12-11T21:30:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'protein',\n",
       "   'dataset:BFD',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'Rostlab/prot_bert_bfd',\n",
       "  'downloads': 38328,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['BFD']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sberbank-ai/rugpt3large_based_on_gpt2': {'modelId': 'sberbank-ai/rugpt3large_based_on_gpt2',\n",
       "  'sha': 'aa2b602c1939938541eed9283347d6e08536f6f8',\n",
       "  'lastModified': '2021-09-21T19:33:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/rugpt3large_based_on_gpt2',\n",
       "  'downloads': 38289,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –ñ—é–ª—å–µ–Ω –∏'},\n",
       "   {'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –¢–æ–º–∞—Å –∏ –º–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π'},\n",
       "   {'text': '–û–¥–Ω–∞–∂–¥—ã'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/ru-gpts'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-roberta-large': {'modelId': 'princeton-nlp/sup-simcse-roberta-large',\n",
       "  'sha': 'd34da58f734b9cc9e617cc37a2321badffdd0ecf',\n",
       "  'lastModified': '2021-05-20T19:36:20.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'princeton-nlp/sup-simcse-roberta-large',\n",
       "  'downloads': 38244,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/xlm-roberta-large-ner-hrl': {'modelId': 'Davlan/xlm-roberta-large-ner-hrl',\n",
       "  'sha': '690312971788985228c98683d4b816fbf026b346',\n",
       "  'lastModified': '2022-06-27T10:49:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'lv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Davlan/xlm-roberta-large-ner-hrl',\n",
       "  'downloads': 38211,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ÿ•ÿ≥ŸÖŸä ŸÖÿ≠ŸÖÿØ Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿ®ÿ±ŸÑŸäŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßÿ±Ÿá Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ŸÑŸÜÿØŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßŸÖŸä Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿßŸÑŸÇÿØÿ≥ ŸÅŸä ŸÅŸÑÿ≥ÿ∑ŸäŸÜ.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'lv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_base_cased': {'modelId': 'flaubert/flaubert_base_cased',\n",
       "  'sha': '86dac38e2ee7dcefac08dfb2c5c901e8c1cf401e',\n",
       "  'lastModified': '2021-05-19T16:54:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'bert-base',\n",
       "   'flaubert-base',\n",
       "   'cased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_base_cased',\n",
       "  'downloads': 38032,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'bert-base',\n",
       "    'flaubert-base',\n",
       "    'cased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-emotion-analysis': {'modelId': 'pysentimiento/robertuito-emotion-analysis',\n",
       "  'sha': '2a1fb82f525912c23a8187eeea418751049d5056',\n",
       "  'lastModified': '2021-12-10T13:30:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'emotion-analysis',\n",
       "   'twitter'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-emotion-analysis',\n",
       "  'downloads': 37926,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['emotion-analysis', 'twitter']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/bert-base-multilingual-cased-sentence': {'modelId': 'DeepPavlov/bert-base-multilingual-cased-sentence',\n",
       "  'sha': '403febddd8959ecc1a8d140a83d461a1261c7935',\n",
       "  'lastModified': '2021-05-18T18:16:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/bert-base-multilingual-cased-sentence',\n",
       "  'downloads': 37676,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'M-CLIP/M-BERT-Base-ViT-B': {'modelId': 'M-CLIP/M-BERT-Base-ViT-B',\n",
       "  'sha': '5da718394f8f62314bb080b1e989e61f5e3ce026',\n",
       "  'lastModified': '2021-05-18T21:34:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'M-CLIP',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'M-CLIP/M-BERT-Base-ViT-B',\n",
       "  'downloads': 37519,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-xlm-roberta-base': {'modelId': 'cardiffnlp/twitter-xlm-roberta-base',\n",
       "  'sha': '152d26b5e474a09a99599ed33a41b9cf9d85556d',\n",
       "  'lastModified': '2021-04-28T16:24:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'arxiv:2104.12250',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cardiffnlp/twitter-xlm-roberta-base',\n",
       "  'downloads': 37403,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ü§óü§óü§ó<mask>'},\n",
       "   {'text': 'üî•The goal of life is <mask> . üî•'},\n",
       "   {'text': 'Il segreto della vita √® l‚Äô<mask> . ‚ù§Ô∏è'},\n",
       "   {'text': 'Hasta <mask> üëã!'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'widget': [{'text': 'ü§óü§óü§ó<mask>'},\n",
       "    {'text': 'üî•The goal of life is <mask> . üî•'},\n",
       "    {'text': 'Il segreto della vita √® l‚Äô<mask> . ‚ù§Ô∏è'},\n",
       "    {'text': 'Hasta <mask> üëã!'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/byt5-small': {'modelId': 'google/byt5-small',\n",
       "  'sha': 'ce8f3a48ed7676af36476a01fb01f95ea529599c',\n",
       "  'lastModified': '2022-05-27T15:06:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:1907.06292',\n",
       "   'arxiv:2105.13626',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/byt5-small',\n",
       "  'downloads': 36714,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'YituTech/conv-bert-base': {'modelId': 'YituTech/conv-bert-base',\n",
       "  'sha': '5cb451936b5c4a96562d8b146de85f64f9cf2c22',\n",
       "  'lastModified': '2021-02-24T11:26:14.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'convbert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'YituTech',\n",
       "  'config': {'architectures': ['ConvBertModel'], 'model_type': 'convbert'},\n",
       "  'id': 'YituTech/conv-bert-base',\n",
       "  'downloads': 36228,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_fluency_model': {'modelId': 'prithivida/parrot_fluency_model',\n",
       "  'sha': 'e5224ff5b4109cd949ce25b0a6dff8d8cbdec7be',\n",
       "  'lastModified': '2022-06-24T09:54:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'prithivida/parrot_fluency_model',\n",
       "  'downloads': 36009,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_t5_xl_uniref50': {'modelId': 'Rostlab/prot_t5_xl_uniref50',\n",
       "  'sha': 'd604cdc190f7df5186404c8729934f0ee9a4b0e4',\n",
       "  'lastModified': '2021-03-29T11:47:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'protein',\n",
       "   'dataset:UniRef50',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'Rostlab/prot_t5_xl_uniref50',\n",
       "  'downloads': 35826,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['UniRef50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc5cdr-disease': {'modelId': 'dmis-lab/biosyn-sapbert-bc5cdr-disease',\n",
       "  'sha': '53d4525fccf15663f19f0d0846c50286a0a01f1e',\n",
       "  'lastModified': '2021-10-25T14:46:40.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc5cdr-disease',\n",
       "  'downloads': 35676,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'GroNLP/bert-base-dutch-cased': {'modelId': 'GroNLP/bert-base-dutch-cased',\n",
       "  'sha': '484ff5cec2ad42b434537dadd901d9b8e2b64cd2',\n",
       "  'lastModified': '2021-08-25T15:20:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'nl',\n",
       "   'arxiv:1912.09582',\n",
       "   'transformers',\n",
       "   'BERTje',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'GroNLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'GroNLP/bert-base-dutch-cased',\n",
       "  'downloads': 34801,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'nl',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/wietsedv/bertje/master/bertje.png',\n",
       "   'tags': ['BERTje']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-xlarge-mnli': {'modelId': 'microsoft/deberta-xlarge-mnli',\n",
       "  'sha': '5b07a9086c1dbb79981ff7b05b4d1ad83b3af51c',\n",
       "  'lastModified': '2022-06-27T15:47:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-xlarge-mnli',\n",
       "  'downloads': 34578,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'knkarthick/MEETING_SUMMARY': {'modelId': 'knkarthick/MEETING_SUMMARY',\n",
       "  'sha': '1d9f2261609ed4970abf4f3659c080783beaf09e',\n",
       "  'lastModified': '2022-06-29T07:16:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'knkarthick',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'knkarthick/MEETING_SUMMARY',\n",
       "  'downloads': 34335,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Hi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\\nOkay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.\\nUm I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.\\nMm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.\"}],\n",
       "  'likes': 13,\n",
       "  'model-index': [{'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'samsum', 'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 53.8795},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 28.4975},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 44.1899},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 49.4863},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 30.088},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 53.2284},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 28.184},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 44.122},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 49.0301},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 29.9951}]}]},\n",
       "   {'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'xsum', 'type': 'xsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 35.9078},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 14.2497},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 28.1421},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 28.9826},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 32.0167},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.0241},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3715},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 28.1968},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 29.0527},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 31.9933}]}]},\n",
       "   {'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'dialogsum', 'type': 'dialogsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 39.8612},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 16.6917},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 32.2718},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 35.8748},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 41.726},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.9608},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3058},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 29.3261},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 32.9},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 43.086}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bart', 'seq2seq', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI'],\n",
       "   'metrics': ['rouge'],\n",
       "   'widget': [{'text': \"Hi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\\nOkay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.\\nUm I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.\\nMm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.\"}],\n",
       "   'model-index': [{'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'samsum', 'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 53.8795},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 28.4975},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 44.1899},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 49.4863},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 30.088},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 53.2284},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 28.184},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 44.122},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 49.0301},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 29.9951}]}]},\n",
       "    {'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'xsum', 'type': 'xsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 35.9078},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 14.2497},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 28.1421},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 28.9826},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 32.0167},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.0241},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3715},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 28.1968},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 29.0527},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 31.9933}]}]},\n",
       "    {'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'dialogsum', 'type': 'dialogsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 39.8612},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 16.6917},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 32.2718},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 35.8748},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 41.726},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.9608},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3058},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 29.3261},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 32.9},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 43.086}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc5cdr-chemical': {'modelId': 'dmis-lab/biosyn-sapbert-bc5cdr-chemical',\n",
       "  'sha': 'f9b9daf740698ac427bb6532fd456fc18bccdd80',\n",
       "  'lastModified': '2021-10-25T14:47:09.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc5cdr-chemical',\n",
       "  'downloads': 34138,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-mini': {'modelId': 'prajjwal1/bert-mini',\n",
       "  'sha': '5e123abc2480f0c4b4cac186d3b3f09299c258fc',\n",
       "  'lastModified': '2021-10-27T18:27:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-mini',\n",
       "  'downloads': 33926,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'peterchou/simbert-chinese-base': {'modelId': 'peterchou/simbert-chinese-base',\n",
       "  'sha': '6a6ebb9f9d9b2264a8a012f96de01067f304476d',\n",
       "  'lastModified': '2021-06-07T05:21:51.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'peterchou',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'peterchou/simbert-chinese-base',\n",
       "  'downloads': 33339,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'BeIR/query-gen-msmarco-t5-large-v1': {'modelId': 'BeIR/query-gen-msmarco-t5-large-v1',\n",
       "  'sha': '5dd8dd401d24332c17e40015e9792ee31f3ced91',\n",
       "  'lastModified': '2021-06-23T02:12:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'BeIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'BeIR/query-gen-msmarco-t5-large-v1',\n",
       "  'downloads': 33017,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-bart': {'modelId': 'hf-internal-testing/tiny-random-bart',\n",
       "  'sha': 'f2efe525625e508121ef8e13b7c37e6324073378',\n",
       "  'lastModified': '2021-11-18T11:36:51.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bart', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'bart'},\n",
       "  'id': 'hf-internal-testing/tiny-random-bart',\n",
       "  'downloads': 32978,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'amberoad/bert-multilingual-passage-reranking-msmarco': {'modelId': 'amberoad/bert-multilingual-passage-reranking-msmarco',\n",
       "  'sha': 'ed2597214a09ac6a3095b64c1ec49309daab5d9c',\n",
       "  'lastModified': '2021-09-21T16:00:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:msmarco',\n",
       "   'arxiv:1901.04085',\n",
       "   'transformers',\n",
       "   'msmarco',\n",
       "   'passage reranking',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'amberoad',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'amberoad/bert-multilingual-passage-reranking-msmarco',\n",
       "  'downloads': 32935,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'query': 'What is a corporation?',\n",
       "    'passage': 'A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'thumbnail': 'https://amberoad.de/images/logo_text.png',\n",
       "   'tags': ['msmarco', 'multilingual', 'passage reranking'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['msmarco'],\n",
       "   'metrics': ['MRR'],\n",
       "   'widget': [{'query': 'What is a corporation?',\n",
       "     'passage': 'A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-mbart': {'modelId': 'sshleifer/tiny-mbart',\n",
       "  'sha': '9d6b9b3b2774b464bb6b14eda4efe30f82846136',\n",
       "  'lastModified': '2021-08-26T10:55:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'sshleifer/tiny-mbart',\n",
       "  'downloads': 32617,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-german-cased': {'modelId': 'dbmdz/bert-base-german-cased',\n",
       "  'sha': '56c3dce79f5d93e466f3b800d8e57cddfe13a6d4',\n",
       "  'lastModified': '2021-05-19T14:52:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-german-cased',\n",
       "  'downloads': 32109,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/kobert': {'modelId': 'monologg/kobert',\n",
       "  'sha': '8ebf2818cfd85570737d31ed8cd7aaa000e7056c',\n",
       "  'lastModified': '2021-05-19T23:52:30.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'monologg/kobert',\n",
       "  'downloads': 31895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-large': {'modelId': 'google/t5-v1_1-large',\n",
       "  'sha': '314bc112b191ec17b625ba81438dc73d6c23659d',\n",
       "  'lastModified': '2021-06-23T01:59:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-large',\n",
       "  'downloads': 31584,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/nli-mpnet-base-v2': {'modelId': 'sentence-transformers/nli-mpnet-base-v2',\n",
       "  'sha': 'c388b46d029476cd6611aa9ed44d05272bbbacfb',\n",
       "  'lastModified': '2022-06-15T20:14:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/nli-mpnet-base-v2',\n",
       "  'downloads': 30769,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/scibert_scivocab_cased': {'modelId': 'allenai/scibert_scivocab_cased',\n",
       "  'sha': '9be298ced05121c9e6e2b2cb9f508b47b8eae650',\n",
       "  'lastModified': '2021-05-19T11:40:28.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'allenai/scibert_scivocab_cased',\n",
       "  'downloads': 30692,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'beomi/KcELECTRA-base': {'modelId': 'beomi/KcELECTRA-base',\n",
       "  'sha': '686333e78646593e324d6ad5e955dfb6dc9f0f5d',\n",
       "  'lastModified': '2022-06-26T01:49:50.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'electra', 'pretraining', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'beomi',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'beomi/KcELECTRA-base',\n",
       "  'downloads': 30652,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ufal/robeczech-base': {'modelId': 'ufal/robeczech-base',\n",
       "  'sha': 'b154a976e0241a2cfb537600fcf936a8540caeb9',\n",
       "  'lastModified': '2022-04-24T11:33:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'cs',\n",
       "   'arxiv:2105.11314',\n",
       "   'transformers',\n",
       "   'Czech',\n",
       "   'RoBERTa',\n",
       "   '√öFAL',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ufal',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'ufal/robeczech-base',\n",
       "  'downloads': 30637,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'cs',\n",
       "   'tags': ['Czech', 'RoBERTa', '√öFAL'],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Sahajtomar/German_Zeroshot': {'modelId': 'Sahajtomar/German_Zeroshot',\n",
       "  'sha': 'd5b0a26665b8538bcb3faa1e63a634cca4c8ee1b',\n",
       "  'lastModified': '2021-05-18T22:22:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'nli',\n",
       "   'xnli',\n",
       "   'de',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Sahajtomar',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Sahajtomar/German_Zeroshot',\n",
       "  'downloads': 30616,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie',\n",
       "    'candidate_labels': 'Verbrechen,Trag√∂die,Stehlen',\n",
       "    'hypothesis_template': 'In deisem geht es um {}.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['text-classification', 'pytorch', 'nli', 'xnli', 'de'],\n",
       "   'datasets': ['xnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie',\n",
       "     'candidate_labels': 'Verbrechen,Trag√∂die,Stehlen',\n",
       "     'hypothesis_template': 'In deisem geht es um {}.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-large-mnli': {'modelId': 'microsoft/deberta-large-mnli',\n",
       "  'sha': '7296194b9009373def4f7c5dad292651e4b5cf4e',\n",
       "  'lastModified': '2021-05-21T20:07:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-large-mnli',\n",
       "  'downloads': 30606,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/embedding': {'modelId': 'pyannote/embedding',\n",
       "  'sha': '09a3ed256d0fddbf5616fd9fb5db917fcf002708',\n",
       "  'lastModified': '2022-03-23T09:24:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'dataset:voxceleb',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-model',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-recognition',\n",
       "   'speaker-verification',\n",
       "   'speaker-identification',\n",
       "   'speaker-embedding',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/embedding',\n",
       "  'downloads': 30606,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-model',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-recognition',\n",
       "    'speaker-verification',\n",
       "    'speaker-identification',\n",
       "    'speaker-embedding'],\n",
       "   'datasets': ['voxceleb'],\n",
       "   'license': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/mbart-large-50': {'modelId': 'facebook/mbart-large-50',\n",
       "  'sha': 'eab25f78110b11bfbf981249a6204e258f8a3312',\n",
       "  'lastModified': '2022-06-25T17:07:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50',\n",
       "  'downloads': 30094,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ja-en': {'modelId': 'Helsinki-NLP/opus-mt-ja-en',\n",
       "  'sha': '6282eb0555cd0253dc9fac00c5fafb2825ad04b4',\n",
       "  'lastModified': '2021-09-10T13:53:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ja',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ja-en',\n",
       "  'downloads': 30050,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract': {'modelId': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
       "  'sha': '3be15ab62caee5db1d45f923410798cdea920010',\n",
       "  'lastModified': '2021-09-22T20:10:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2007.15779',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
       "  'downloads': 29715,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[MASK] is a tyrosine kinase inhibitor.'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[MASK] is a tyrosine kinase inhibitor.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/T0': {'modelId': 'bigscience/T0',\n",
       "  'sha': '37f8b7565a0c9945db6a0215b0b823a55e337f4f',\n",
       "  'lastModified': '2022-06-21T01:25:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0',\n",
       "  'downloads': 29343,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-small-finetuned-squadv2': {'modelId': 'mrm8488/bert-small-finetuned-squadv2',\n",
       "  'sha': '3ffb743e93b64bc944f778292a71ebac650834ae',\n",
       "  'lastModified': '2021-05-20T00:33:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-small-finetuned-squadv2',\n",
       "  'downloads': 29041,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'snrspeaks/t5-one-line-summary': {'modelId': 'snrspeaks/t5-one-line-summary',\n",
       "  'sha': '62acf01b9c91b2ea3a84b1e83a8ee0557cc3526c',\n",
       "  'lastModified': '2021-06-23T14:20:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:arxiv',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'snrspeaks',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'snrspeaks/t5-one-line-summary',\n",
       "  'downloads': 28720,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production  machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and  handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks.  In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year,  Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing.  In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors  1.7-2.9 times versus production systems.\"}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['arxiv'],\n",
       "   'widget': [{'text': \"summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production  machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and  handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks.  In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year,  Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing.  In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors  1.7-2.9 times versus production systems.\"}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-zh': {'modelId': 'Helsinki-NLP/opus-mt-en-zh',\n",
       "  'sha': '93db7712e4698309ac17a80605adbf54dea5c8ee',\n",
       "  'lastModified': '2021-09-09T21:40:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-zh',\n",
       "  'downloads': 28653,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'zh'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_adequacy_model': {'modelId': 'prithivida/parrot_adequacy_model',\n",
       "  'sha': '87a35bc291d7455cfc86fc5f6a374c92de0156af',\n",
       "  'lastModified': '2022-05-27T02:47:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'prithivida/parrot_adequacy_model',\n",
       "  'downloads': 28567,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/mdeberta-v3-base': {'modelId': 'microsoft/mdeberta-v3-base',\n",
       "  'sha': '7d66e84a399b78accc72e0f61cd6d50f02ee1c2c',\n",
       "  'lastModified': '2022-01-13T19:41:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'multilingual',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'mdeberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/mdeberta-v3-base',\n",
       "  'downloads': 28528,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 32,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['deberta', 'deberta-v3', 'mdeberta'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'julien-c/bert-xsmall-dummy': {'modelId': 'julien-c/bert-xsmall-dummy',\n",
       "  'sha': '9d3811da21adb66feb315118023f528ed10c6b18',\n",
       "  'lastModified': '2021-05-19T20:53:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'julien-c/bert-xsmall-dummy',\n",
       "  'downloads': 28053,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-125M': {'modelId': 'KoboldAI/fairseq-dense-125M',\n",
       "  'sha': 'ee40bc27509c93cd551f2791bb9d462bbab4d450',\n",
       "  'lastModified': '2022-02-01T22:48:03.000Z',\n",
       "  'tags': ['pytorch', 'xglm', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-125M',\n",
       "  'downloads': 27830,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'thatdramebaazguy/roberta-base-squad': {'modelId': 'thatdramebaazguy/roberta-base-squad',\n",
       "  'sha': 'a7a26fd500148b760ca89a87edcd5b5605daab09',\n",
       "  'lastModified': '2022-07-01T19:12:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'English',\n",
       "   'dataset:SQuAD',\n",
       "   'transformers',\n",
       "   'roberta-base',\n",
       "   'qa',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'thatdramebaazguy',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'thatdramebaazguy/roberta-base-squad',\n",
       "  'downloads': 27770,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['SQuAD'],\n",
       "   'language': ['English'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['roberta', 'roberta-base', 'question-answering', 'qa'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-gpt2': {'modelId': 'hf-internal-testing/tiny-random-gpt2',\n",
       "  'sha': '937b4d23b6648f5a1a0d1247b939b26981798903',\n",
       "  'lastModified': '2021-09-17T19:24:03.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'gpt2', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'gpt2'},\n",
       "  'id': 'hf-internal-testing/tiny-random-gpt2',\n",
       "  'downloads': 27601,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/ms-marco-electra-base': {'modelId': 'cross-encoder/ms-marco-electra-base',\n",
       "  'sha': '69c22886dd57c67783a8f48af5b86a35657df8f6',\n",
       "  'lastModified': '2021-08-05T08:40:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['ElectraForSequenceClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'cross-encoder/ms-marco-electra-base',\n",
       "  'downloads': 27376,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'skt/kogpt2-base-v2': {'modelId': 'skt/kogpt2-base-v2',\n",
       "  'sha': 'd0c0df48bf2b2c9350dd855021a5b216f560c0c7',\n",
       "  'lastModified': '2021-09-23T16:29:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'skt',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'skt/kogpt2-base-v2',\n",
       "  'downloads': 27158,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['gpt2'],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-ontonotes-fast': {'modelId': 'flair/ner-english-ontonotes-fast',\n",
       "  'sha': '38a8eb6a720791da55e15962c36a37dd8d8270b2',\n",
       "  'lastModified': '2021-03-02T22:05:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes-fast',\n",
       "  'downloads': 27003,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George Washington won 1 dollar.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George Washington won 1 dollar.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-bert-base-dot-v5': {'modelId': 'sentence-transformers/msmarco-bert-base-dot-v5',\n",
       "  'sha': '668e63a378bc93d76c430af68338e550dc78df09',\n",
       "  'lastModified': '2022-06-15T20:34:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/msmarco-bert-base-dot-v5',\n",
       "  'downloads': 26906,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-3B': {'modelId': 'facebook/blenderbot-3B',\n",
       "  'sha': 'c468b2376f5f49d20624f31383023f2bbd360c8d',\n",
       "  'lastModified': '2021-09-21T19:45:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-3B',\n",
       "  'downloads': 26649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-960h-lv60-self': {'modelId': 'facebook/wav2vec2-large-960h-lv60-self',\n",
       "  'sha': '54074b1c16f4de6a5ad59affb4caa8f2ea03a119',\n",
       "  'lastModified': '2022-05-23T16:13:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2010.11430',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-960h-lv60-self',\n",
       "  'downloads': 26534,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': [{'name': 'wav2vec2-large-960h-lv60',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'wav2vec2-large-960h-lv60',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'uklfr/gottbert-base': {'modelId': 'uklfr/gottbert-base',\n",
       "  'sha': '301ea863069cb7f7226a8bc6b1311b0bc6b7b1d4',\n",
       "  'lastModified': '2021-09-16T15:36:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2012.02110',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uklfr',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'uklfr/gottbert-base',\n",
       "  'downloads': 26503,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'joeddav/bart-large-mnli-yahoo-answers': {'modelId': 'joeddav/bart-large-mnli-yahoo-answers',\n",
       "  'sha': 'd836606b3cf20652cf30283d6884ae26a11e5392',\n",
       "  'lastModified': '2021-06-14T10:44:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:yahoo-answers',\n",
       "   'arxiv:1909.00161',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'joeddav',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'joeddav/bart-large-mnli-yahoo-answers',\n",
       "  'downloads': 26305,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['text-classification', 'pytorch'],\n",
       "   'datasets': ['yahoo-answers'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-large': {'modelId': 'microsoft/deberta-large',\n",
       "  'sha': '822a8791fdac38e8086e2731158047e9b63e4521',\n",
       "  'lastModified': '2022-01-13T17:10:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-large',\n",
       "  'downloads': 26046,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/t5-small-lm-adapt': {'modelId': 'google/t5-small-lm-adapt',\n",
       "  'sha': 'ceece9332ccd73f589b2c764fa0e334c597952d4',\n",
       "  'lastModified': '2021-11-01T13:58:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-small-lm-adapt',\n",
       "  'downloads': 26031,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'chkla/roberta-argument': {'modelId': 'chkla/roberta-argument',\n",
       "  'sha': 'd5480352a5ad33b0135cc1193a62be24396e557a',\n",
       "  'lastModified': '2021-05-20T15:19:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'english',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'chkla',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'chkla/roberta-argument',\n",
       "  'downloads': 25982,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'It has been determined that the amount of greenhouse gases have decreased by almost half because of the prevalence in the utilization of nuclear power.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'english',\n",
       "   'widget': [{'text': 'It has been determined that the amount of greenhouse gases have decreased by almost half because of the prevalence in the utilization of nuclear power.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-v4': {'modelId': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
       "  'sha': '62b749054617919f8d1e8462a987edea4b998e3c',\n",
       "  'lastModified': '2022-06-15T19:32:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
       "  'downloads': 25884,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Vamsi/T5_Paraphrase_Paws': {'modelId': 'Vamsi/T5_Paraphrase_Paws',\n",
       "  'sha': '3bbf07dc42d5ddc9ca77c5589ce7239b0b731832',\n",
       "  'lastModified': '2021-06-23T11:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'paraphrase-generation',\n",
       "   'text-generation',\n",
       "   'Conditional Generation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Vamsi',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Vamsi/T5_Paraphrase_Paws',\n",
       "  'downloads': 25843,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['paraphrase-generation',\n",
       "    'text-generation',\n",
       "    'Conditional Generation'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'unitary/multilingual-toxic-xlm-roberta': {'modelId': 'unitary/multilingual-toxic-xlm-roberta',\n",
       "  'sha': '19f5c53459ec9679c675aeead38cab87cf588944',\n",
       "  'lastModified': '2021-05-06T11:04:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1703.04009',\n",
       "   'arxiv:1905.12516',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'unitary',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'unitary/multilingual-toxic-xlm-roberta',\n",
       "  'downloads': 25836,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'text-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'siebert/sentiment-roberta-large-english': {'modelId': 'siebert/sentiment-roberta-large-english',\n",
       "  'sha': 'acf101658300b9531e65b06504421a0d46033f86',\n",
       "  'lastModified': '2022-05-25T09:28:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:1907.11692',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'twitter',\n",
       "   'reviews',\n",
       "   'siebert'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'siebert',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'siebert/sentiment-roberta-large-english',\n",
       "  'downloads': 25831,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sentiment', 'twitter', 'reviews', 'siebert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large-xsum': {'modelId': 'facebook/bart-large-xsum',\n",
       "  'sha': 'e5a049949143586befac0f09a9716bde79b55e77',\n",
       "  'lastModified': '2021-06-14T07:39:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'facebook/bart-large-xsum',\n",
       "  'downloads': 25814,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'language': ['en'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/ruclip-vit-base-patch32-384': {'modelId': 'sberbank-ai/ruclip-vit-base-patch32-384',\n",
       "  'sha': '1f7f08e5437de5dd5beba7a448983b7e4135891b',\n",
       "  'lastModified': '2022-01-10T00:21:50.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {},\n",
       "  'id': 'sberbank-ai/ruclip-vit-base-patch32-384',\n",
       "  'downloads': 25547,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/qnli-electra-base': {'modelId': 'cross-encoder/qnli-electra-base',\n",
       "  'sha': '4d70c22ec2d12ec7663a70fbe3180a408c980a2a',\n",
       "  'lastModified': '2021-08-05T08:41:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'text-classification',\n",
       "   'arxiv:1804.07461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['ElectraForSequenceClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'cross-encoder/qnli-electra-base',\n",
       "  'downloads': 24800,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/tapas-base': {'modelId': 'google/tapas-base',\n",
       "  'sha': '00456266840bb0a319cd6748ebf7da3caf98816b',\n",
       "  'lastModified': '2021-11-29T10:03:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tapas',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2004.02349',\n",
       "   'arxiv:2010.00571',\n",
       "   'transformers',\n",
       "   'TapasModel',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['TapasModel'], 'model_type': 'tapas'},\n",
       "  'id': 'google/tapas-base',\n",
       "  'downloads': 24754,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapas', 'TapasModel'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc2gn': {'modelId': 'dmis-lab/biosyn-sapbert-bc2gn',\n",
       "  'sha': '28ef41eace90e9aa6a9db372413c145883c72902',\n",
       "  'lastModified': '2022-02-25T13:32:53.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc2gn',\n",
       "  'downloads': 24749,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'r3dhummingbird/DialoGPT-medium-joshua': {'modelId': 'r3dhummingbird/DialoGPT-medium-joshua',\n",
       "  'sha': 'ff22e98bcb70ae1e082f54640c5c3bafd3950125',\n",
       "  'lastModified': '2021-07-19T23:18:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'r3dhummingbird',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'r3dhummingbird/DialoGPT-medium-joshua',\n",
       "  'downloads': 24598,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://raw.githubusercontent.com/RuolinZheng08/twewy-discord-chatbot/main/gif-demo/icon.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ml6team/bert-base-uncased-city-country-ner': {'modelId': 'ml6team/bert-base-uncased-city-country-ner',\n",
       "  'sha': 'e38e683af1174120b192661dbdcbc2358fe56964',\n",
       "  'lastModified': '2022-07-01T07:27:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:Ultra Fine Entity Typing',\n",
       "   'transformers',\n",
       "   'address-NER',\n",
       "   'NER',\n",
       "   'bert-base-uncased',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ml6team',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ml6team/bert-base-uncased-city-country-ner',\n",
       "  'downloads': 24563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Hi, I am Kermit and I live in Berlin'},\n",
       "   {'text': 'It is very difficult to find a house in Berlin, Germany.'},\n",
       "   {'text': 'ML6 is a very cool company from Belgium'},\n",
       "   {'text': 'Samuel ppops in a happy plce called Berlin which happens to be Kazakhstan'},\n",
       "   {'text': 'My family and I visited Montreal, Canada last week and the flight from Amsterdam took 9 hours'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['token-classification', 'address-NER', 'NER', 'bert-base-uncased'],\n",
       "   'datasets': ['Ultra Fine Entity Typing'],\n",
       "   'metrics': ['Precision', 'Recall', 'F1 Score'],\n",
       "   'widget': [{'text': 'Hi, I am Kermit and I live in Berlin'},\n",
       "    {'text': 'It is very difficult to find a house in Berlin, Germany.'},\n",
       "    {'text': 'ML6 is a very cool company from Belgium'},\n",
       "    {'text': 'Samuel ppops in a happy plce called Berlin which happens to be Kazakhstan'},\n",
       "    {'text': 'My family and I visited Montreal, Canada last week and the flight from Amsterdam took 9 hours'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-pl-en': {'modelId': 'Helsinki-NLP/opus-mt-pl-en',\n",
       "  'sha': '361ac28538863fafa2090bf91c36d02b9c596d5b',\n",
       "  'lastModified': '2021-09-10T14:01:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'pl',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-pl-en',\n",
       "  'downloads': 24454,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoichiYasuoka/bert-base-japanese-char-extended': {'modelId': 'KoichiYasuoka/bert-base-japanese-char-extended',\n",
       "  'sha': 'ec39844667602ffc6fc2fa1958ee683b667421f8',\n",
       "  'lastModified': '2022-06-20T22:21:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'masked-lm',\n",
       "   'wikipedia',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KoichiYasuoka',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KoichiYasuoka/bert-base-japanese-char-extended',\n",
       "  'downloads': 24216,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÈÖ∏Á¥†„Éú„É≥„Éô„ÇíÂÖÖ[MASK]„Åô„Çã„ÄÇ'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'tags': ['japanese', 'masked-lm', 'wikipedia'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': 'ÈÖ∏Á¥†„Éú„É≥„Éô„ÇíÂÖÖ[MASK]„Åô„Çã„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-en-de': {'modelId': 'facebook/wmt19-en-de',\n",
       "  'sha': 'b33976783993b11baabc19313275865ee87931e3',\n",
       "  'lastModified': '2020-12-11T21:39:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-en-de',\n",
       "  'downloads': 24076,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-TinyBERT-L6-v2': {'modelId': 'sentence-transformers/paraphrase-TinyBERT-L6-v2',\n",
       "  'sha': '8fe7263a517189c4a11a98f87db8ac964b235b5f',\n",
       "  'lastModified': '2022-06-15T20:12:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-TinyBERT-L6-v2',\n",
       "  'downloads': 23958,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ar': {'modelId': 'Helsinki-NLP/opus-mt-en-ar',\n",
       "  'sha': '12f7bc254b7e475b6377f440d488063d7fb51571',\n",
       "  'lastModified': '2021-02-28T14:15:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ar',\n",
       "  'downloads': 23955,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'ar'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-distilbert-base-cased-distilled-squad': {'modelId': 'sshleifer/tiny-distilbert-base-cased-distilled-squad',\n",
       "  'sha': '33a976c7ab7d41310ea4063d311dbf66c8aaa001',\n",
       "  'lastModified': '2020-05-14T16:54:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sshleifer/tiny-distilbert-base-cased-distilled-squad',\n",
       "  'downloads': 23866,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoichiYasuoka/bert-base-japanese-upos': {'modelId': 'KoichiYasuoka/bert-base-japanese-upos',\n",
       "  'sha': 'e9077f5b327b42da14e3c5d60f529331a68f9eed',\n",
       "  'lastModified': '2022-05-23T21:50:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ja',\n",
       "   'dataset:universal_dependencies',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'pos',\n",
       "   'wikipedia',\n",
       "   'dependency-parsing',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'KoichiYasuoka',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'KoichiYasuoka/bert-base-japanese-upos',\n",
       "  'downloads': 23800,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÂõΩÂ¢É„ÅÆÈï∑„ÅÑ„Éà„É≥„Éç„É´„ÇíÊäú„Åë„Çã„Å®Èõ™ÂõΩ„Åß„ÅÇ„Å£„Åü„ÄÇ'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'tags': ['japanese',\n",
       "    'token-classification',\n",
       "    'pos',\n",
       "    'wikipedia',\n",
       "    'dependency-parsing'],\n",
       "   'datasets': ['universal_dependencies'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'widget': [{'text': 'ÂõΩÂ¢É„ÅÆÈï∑„ÅÑ„Éà„É≥„Éç„É´„ÇíÊäú„Åë„Çã„Å®Èõ™ÂõΩ„Åß„ÅÇ„Å£„Åü„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-1.3b': {'modelId': 'facebook/opt-1.3b',\n",
       "  'sha': 'aa6ac1e23bb9a499be2b7400079cd2a7b8a1309a',\n",
       "  'lastModified': '2022-06-22T09:53:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-1.3b',\n",
       "  'downloads': 23550,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'neuralmind/bert-large-portuguese-cased': {'modelId': 'neuralmind/bert-large-portuguese-cased',\n",
       "  'sha': 'aa302f6ea73b759f7df9cad58bd272127b67ec28',\n",
       "  'lastModified': '2021-05-20T01:31:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'neuralmind',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'neuralmind/bert-large-portuguese-cased',\n",
       "  'downloads': 23484,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['bert', 'pytorch'],\n",
       "   'datasets': ['brWaC']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert-for-token-classification': {'modelId': 'hf-internal-testing/tiny-bert-for-token-classification',\n",
       "  'sha': 'f89ef50d84f2959688279d4b2c09faf823da2069',\n",
       "  'lastModified': '2021-12-16T11:04:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-bert-for-token-classification',\n",
       "  'downloads': 23114,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-cased-finetuned-mrpc': {'modelId': 'bert-base-cased-finetuned-mrpc',\n",
       "  'sha': 'f53cb9cb49541a34be140979efe098073a179b0f',\n",
       "  'lastModified': '2021-05-18T16:08:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-cased-finetuned-mrpc',\n",
       "  'downloads': 23108,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-distilroberta-base': {'modelId': 'cross-encoder/stsb-distilroberta-base',\n",
       "  'sha': '2a387f03597b030ff3dadcef7d73456ce23e3bb7',\n",
       "  'lastModified': '2021-08-05T08:41:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-distilroberta-base',\n",
       "  'downloads': 23067,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mt5-large': {'modelId': 'google/mt5-large',\n",
       "  'sha': 'bdd096d7cf0fc531444a0db2e0a9a209d0a5f8c0',\n",
       "  'lastModified': '2022-05-27T15:06:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-large',\n",
       "  'downloads': 22575,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-english': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-english',\n",
       "  'sha': '5d103dcbb7fec00310f624f8d863bc3a69bf10ad',\n",
       "  'lastModified': '2022-06-22T17:04:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-english',\n",
       "  'downloads': 22574,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 15,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 English by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice en',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'en'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 19.06},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 7.69},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 14.81},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 6.84}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'en'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 27.72},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 11.65},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 20.85},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 11.01}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'en',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 English by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice en',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'en'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 19.06},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 7.69},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 14.81},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 6.84}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'en'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 27.72},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 11.65},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 20.85},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 11.01}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Michau/t5-base-en-generate-headline': {'modelId': 'Michau/t5-base-en-generate-headline',\n",
       "  'sha': 'f526532f788c45b6b6288286e5ef929fa768ef6a',\n",
       "  'lastModified': '2021-06-23T03:17:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Michau',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Michau/t5-base-en-generate-headline',\n",
       "  'downloads': 22504,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/koelectra-base-v3-discriminator': {'modelId': 'monologg/koelectra-base-v3-discriminator',\n",
       "  'sha': '68b30cd259f34a4b5aa8786392612ba2a2617fcc',\n",
       "  'lastModified': '2021-10-20T16:53:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'monologg/koelectra-base-v3-discriminator',\n",
       "  'downloads': 22385,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'license': 'apache-2.0', 'tags': ['korean']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-de-en': {'modelId': 'facebook/wmt19-de-en',\n",
       "  'sha': '80d366f635721148ffa2a0a58591cb672c9b4982',\n",
       "  'lastModified': '2020-12-11T21:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-de-en',\n",
       "  'downloads': 22367,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-mbart': {'modelId': 'hf-internal-testing/tiny-random-mbart',\n",
       "  'sha': '63c7077c54936948aeb5a675e10489c945957824',\n",
       "  'lastModified': '2021-12-28T12:24:41.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'mbart', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'mbart'},\n",
       "  'id': 'hf-internal-testing/tiny-random-mbart',\n",
       "  'downloads': 22271,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'teacookies/autonlp-more_fine_tune_24465520-26265908': {'modelId': 'teacookies/autonlp-more_fine_tune_24465520-26265908',\n",
       "  'sha': 'c935bca0d7748cdbaad58100718074cedf186ae7',\n",
       "  'lastModified': '2021-10-25T09:36:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'unk',\n",
       "   'dataset:teacookies/autonlp-data-more_fine_tune_24465520',\n",
       "   'transformers',\n",
       "   'autonlp',\n",
       "   'co2_eq_emissions',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'teacookies',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'teacookies/autonlp-more_fine_tune_24465520-26265908',\n",
       "  'downloads': 21960,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Who loves AutoNLP?',\n",
       "    'context': 'Everyone loves AutoNLP'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['autonlp', 'question-answering'],\n",
       "   'language': 'unk',\n",
       "   'widget': [{'text': 'Who loves AutoNLP?',\n",
       "     'context': 'Everyone loves AutoNLP'}],\n",
       "   'datasets': ['teacookies/autonlp-data-more_fine_tune_24465520'],\n",
       "   'co2_eq_emissions': 96.32087452115675},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-small': {'modelId': 'google/t5-v1_1-small',\n",
       "  'sha': 'fb7e6cba609f7bab11c614294bc04f82f613c7b1',\n",
       "  'lastModified': '2021-06-23T00:37:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-small',\n",
       "  'downloads': 21570,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-id-en': {'modelId': 'Helsinki-NLP/opus-mt-id-en',\n",
       "  'sha': 'cbdb70ef26d3c5a6585e6a810da1003bd50bb6b3',\n",
       "  'lastModified': '2021-09-09T22:11:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'id',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-id-en',\n",
       "  'downloads': 21462,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-2.0-large-en': {'modelId': 'nghuyong/ernie-2.0-large-en',\n",
       "  'sha': '4770fb35e20abf0e2ed2ba0a70faec4fc55b5d2b',\n",
       "  'lastModified': '2021-05-20T01:45:21.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'jax', 'bert', 'arxiv:1907.12412', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-2.0-large-en',\n",
       "  'downloads': 21317,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'MilaNLProc/feel-it-italian-sentiment': {'modelId': 'MilaNLProc/feel-it-italian-sentiment',\n",
       "  'sha': '10879b7d7ce86e72b40910165b5d31b192d953ee',\n",
       "  'lastModified': '2021-03-19T09:20:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'Italian',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'MilaNLProc',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'MilaNLProc/feel-it-italian-sentiment',\n",
       "  'downloads': 21271,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Mi piaci. Ti amo'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentiment', 'Italian']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/roberta-base-SST-2': {'modelId': 'textattack/roberta-base-SST-2',\n",
       "  'sha': 'a029a4679e8a56a958d932d1132d6a4f68803214',\n",
       "  'lastModified': '2021-05-20T22:11:39.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'textattack/roberta-base-SST-2',\n",
       "  'downloads': 21256,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-medium-finetuned-squadv2': {'modelId': 'mrm8488/bert-medium-finetuned-squadv2',\n",
       "  'sha': '881ce1995ab82387a14f63cf50c845afb8f6f724',\n",
       "  'lastModified': '2021-05-20T00:25:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-medium-finetuned-squadv2',\n",
       "  'downloads': 21242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neox-20b': {'modelId': 'EleutherAI/gpt-neox-20b',\n",
       "  'sha': '364ae95407723fadd1d47b023c1efb92a4d891c3',\n",
       "  'lastModified': '2022-04-07T22:14:56.000Z',\n",
       "  'tags': ['pytorch', 'gpt_neox', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoXForCausalLM'],\n",
       "   'model_type': 'gpt_neox'},\n",
       "  'id': 'EleutherAI/gpt-neox-20b',\n",
       "  'downloads': 21233,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 37,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-emoji': {'modelId': 'cardiffnlp/twitter-roberta-base-emoji',\n",
       "  'sha': 'e7efb0d4f929fce6b1477405d6f59c526e4272ac',\n",
       "  'lastModified': '2021-05-20T14:59:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-emoji',\n",
       "  'downloads': 21106,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'emanjavacas/MacBERTh': {'modelId': 'emanjavacas/MacBERTh',\n",
       "  'sha': 'd709ce9a3cf02bf27e580e65fa50847edf02f64d',\n",
       "  'lastModified': '2022-01-17T16:02:47.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'emanjavacas',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emanjavacas/MacBERTh',\n",
       "  'downloads': 21095,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rasa/LaBSE': {'modelId': 'rasa/LaBSE',\n",
       "  'sha': 'e615b58364f13c7be81e15ccea2ab27a6c483b76',\n",
       "  'lastModified': '2021-05-20T04:01:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'rasa',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'rasa/LaBSE',\n",
       "  'downloads': 21061,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/m2m100_1.2B': {'modelId': 'facebook/m2m100_1.2B',\n",
       "  'sha': '90301acb1353eb6623e48973520b486612a57439',\n",
       "  'lastModified': '2022-05-26T22:26:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'm2m_100',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'ast',\n",
       "   'az',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'ilo',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'lb',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ns',\n",
       "   'oc',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'arxiv:2010.11125',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['M2M100ForConditionalGeneration'],\n",
       "   'model_type': 'm2m_100'},\n",
       "  'id': 'facebook/m2m100_1.2B',\n",
       "  'downloads': 20984,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'ast',\n",
       "    'az',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'ilo',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'lb',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ns',\n",
       "    'oc',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'license': 'mit',\n",
       "   'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shibing624/macbert4csc-base-chinese': {'modelId': 'shibing624/macbert4csc-base-chinese',\n",
       "  'sha': 'a3383e26cc84638663a8681b141a6fdeabf09b72',\n",
       "  'lastModified': '2022-01-29T04:00:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'shibing624',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'shibing624/macbert4csc-base-chinese',\n",
       "  'downloads': 20788,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'tags': ['bert', 'pytorch', 'zh'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tennessejoyce/titlewave-t5-base': {'modelId': 'tennessejoyce/titlewave-t5-base',\n",
       "  'sha': 'fb30007e56801afadefbd4d60cb3b36631dce9e8',\n",
       "  'lastModified': '2021-06-23T14:26:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'tennessejoyce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 40,\n",
       "     'min_length': 5,\n",
       "     'no_repeat_ngram_size': 1,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '}}},\n",
       "  'id': 'tennessejoyce/titlewave-t5-base',\n",
       "  'downloads': 20682,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Example question body.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'cc-by-4.0',\n",
       "   'pipeline_tag': 'summarization',\n",
       "   'widget': [{'text': 'Example question body.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MilaNLProc/feel-it-italian-emotion': {'modelId': 'MilaNLProc/feel-it-italian-emotion',\n",
       "  'sha': '7006790559e68195bfd5ab76b43afee2f131347a',\n",
       "  'lastModified': '2021-03-19T09:21:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'emotion',\n",
       "   'Italian',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'MilaNLProc',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'MilaNLProc/feel-it-italian-emotion',\n",
       "  'downloads': 20677,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Mi piaci. Ti amo'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentiment', 'emotion', 'Italian']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/MiniLM-L12-H384-uncased': {'modelId': 'microsoft/MiniLM-L12-H384-uncased',\n",
       "  'sha': '44acabbec0ef496f6dbc93adadea57f376b7c0ec',\n",
       "  'lastModified': '2021-05-19T23:29:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:2002.10957',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'microsoft/MiniLM-L12-H384-uncased',\n",
       "  'downloads': 20577,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction': {'modelId': 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction',\n",
       "  'sha': 'e680df1f96f17bb3001789b2ba2c78b007c5e3df',\n",
       "  'lastModified': '2022-05-02T13:15:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'token-classification',\n",
       "   'nl',\n",
       "   'dataset:sonar',\n",
       "   'transformers',\n",
       "   'punctuation prediction',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['RobertaForTokenClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction',\n",
       "  'downloads': 20530,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'hervatting van de zitting ik verklaar de zitting van het europees parlement die op vrijdag 17 december werd onderbroken te zijn hervat',\n",
       "    'example_title': 'Euro Parl Sample'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['nl'],\n",
       "   'tags': ['punctuation prediction', 'punctuation'],\n",
       "   'datasets': 'sonar',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'hervatting van de zitting ik verklaar de zitting van het europees parlement die op vrijdag 17 december werd onderbroken te zijn hervat',\n",
       "     'example_title': 'Euro Parl Sample'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5_squad_v1': {'modelId': 'ramsrigouthamg/t5_squad_v1',\n",
       "  'sha': '9555145a47b5794e372b2d0b5a5331cf12e1afb7',\n",
       "  'lastModified': '2021-06-23T13:48:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5_squad_v1',\n",
       "  'downloads': 20513,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tblard/tf-allocine': {'modelId': 'tblard/tf-allocine',\n",
       "  'sha': '9ba1b0f441ba8a67b775a6f920a2b50cc73c38f7',\n",
       "  'lastModified': '2020-12-11T22:02:40.000Z',\n",
       "  'tags': ['tf', 'camembert', 'text-classification', 'fr', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'tblard',\n",
       "  'config': {'architectures': ['TFCamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'tblard/tf-allocine',\n",
       "  'downloads': 20466,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je t'appr√©cie beaucoup. Je t'aime.\"}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'TF_AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ko-en': {'modelId': 'Helsinki-NLP/opus-mt-ko-en',\n",
       "  'sha': '8bf548f19accb8fdc96055608840f5a0c194ec8d',\n",
       "  'lastModified': '2020-08-21T14:42:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ko',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ko-en',\n",
       "  'downloads': 20299,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ko', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yiyanghkust/finbert-esg': {'modelId': 'yiyanghkust/finbert-esg',\n",
       "  'sha': '26eff66d1942e399ca3ed598894cf0a52915985b',\n",
       "  'lastModified': '2022-06-10T23:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'financial-text-analysis',\n",
       "   'esg',\n",
       "   'environmental-social-corporate-governance'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'yiyanghkust',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'yiyanghkust/finbert-esg',\n",
       "  'downloads': 20263,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Rhonda has been volunteering for several years for a variety of charitable community programs. '}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-text-analysis',\n",
       "    'esg',\n",
       "    'environmental-social-corporate-governance'],\n",
       "   'widget': [{'text': 'Rhonda has been volunteering for several years for a variety of charitable community programs. '}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/canine-s': {'modelId': 'google/canine-s',\n",
       "  'sha': '792aaf916e56cb8470fc3162a75f2fa31f96756a',\n",
       "  'lastModified': '2021-08-13T08:23:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'canine',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2103.06874',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['CanineModel'], 'model_type': 'canine'},\n",
       "  'id': 'google/canine-s',\n",
       "  'downloads': 20233,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'onlplab/alephbert-base': {'modelId': 'onlplab/alephbert-base',\n",
       "  'sha': '1745fb3ff5137e41e9eb4d6246e0758f63b93e46',\n",
       "  'lastModified': '2022-06-26T09:32:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'he',\n",
       "   'dataset:oscar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:twitter',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'language model',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'onlplab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'onlplab/alephbert-base',\n",
       "  'downloads': 20113,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['he'],\n",
       "   'tags': ['language model'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['oscar', 'wikipedia', 'twitter']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Shinen': {'modelId': 'KoboldAI/fairseq-dense-13B-Shinen',\n",
       "  'sha': 'c6db29f4afb5ffbdc9e2251ec91914be6fcb4339',\n",
       "  'lastModified': '2022-04-07T09:10:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Shinen',\n",
       "  'downloads': 20062,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/quora-distilbert-multilingual': {'modelId': 'sentence-transformers/quora-distilbert-multilingual',\n",
       "  'sha': '43c541d8cdd793eed04a9c2d66c6f971198681da',\n",
       "  'lastModified': '2022-06-15T20:31:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/quora-distilbert-multilingual',\n",
       "  'downloads': 19919,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/xlm-roberta-base-wikiann-ner': {'modelId': 'Davlan/xlm-roberta-base-wikiann-ner',\n",
       "  'sha': '172d1f30d99d7de340494c6aedd6b6702f6c5021',\n",
       "  'lastModified': '2022-06-27T10:36:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'en',\n",
       "   'es',\n",
       "   'eu',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'mr',\n",
       "   'pa',\n",
       "   'pt',\n",
       "   'sw',\n",
       "   'ur',\n",
       "   'vi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'dataset:wikiann',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Davlan/xlm-roberta-base-wikiann-ner',\n",
       "  'downloads': 19832,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ÿ•ÿ≥ŸÖŸä ŸÖÿ≠ŸÖÿØ Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿ®ÿ±ŸÑŸäŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßÿ±Ÿá Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ŸÑŸÜÿØŸÜ'},\n",
       "   {'text': 'ÿ•ÿ≥ŸÖŸä ÿ≥ÿßŸÖŸä Ÿàÿ£ÿ≥ŸÉŸÜ ŸÅŸä ÿßŸÑŸÇÿØÿ≥ ŸÅŸä ŸÅŸÑÿ≥ÿ∑ŸäŸÜ.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'as',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'en',\n",
       "    'es',\n",
       "    'eu',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'mr',\n",
       "    'pa',\n",
       "    'pt',\n",
       "    'sw',\n",
       "    'ur',\n",
       "    'vi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'multilingual'],\n",
       "   'datasets': ['wikiann']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'julien-c/dummy-unknown': {'modelId': 'julien-c/dummy-unknown',\n",
       "  'sha': '60b8d3fe22aebb024b573f1cca224db3126d10f3',\n",
       "  'lastModified': '2021-05-20T17:31:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'ci',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'julien-c/dummy-unknown',\n",
       "  'downloads': 19815,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['ci']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-cnn_dailymail': {'modelId': 'google/pegasus-cnn_dailymail',\n",
       "  'sha': '811b08dd23ebf40cbe121d5c49b268150604bb8f',\n",
       "  'lastModified': '2021-03-27T08:09:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-cnn_dailymail',\n",
       "  'downloads': 19632,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-token-nq': {'modelId': 'facebook/rag-token-nq',\n",
       "  'sha': 'af32fa164f774a532dfb63c94b2e898e80434643',\n",
       "  'lastModified': '2021-03-12T10:55:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagTokenForGeneration'], 'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-token-nq',\n",
       "  'downloads': 19595,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagTokenForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gelectra-base-germanquad': {'modelId': 'deepset/gelectra-base-germanquad',\n",
       "  'sha': '9e4b56ee16728fecb4efd51596384180c8d43a43',\n",
       "  'lastModified': '2022-02-17T14:11:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'dataset:deepset/germanquad',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/gelectra-base-germanquad',\n",
       "  'downloads': 19592,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den gr√∂√üten Teil des Amazonas-Beckens S√ºdamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der gr√∂√üte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Franz√∂sisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repr√§sentiert mehr als die H√§lfte der verbleibenden Regenw√§lder des Planeten und umfasst den gr√∂√üten und artenreichsten tropischen Regenwald der Welt mit gesch√§tzten 390 Milliarden Einzelb√§umen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germanquad'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'RUCAIBox/mvp': {'modelId': 'RUCAIBox/mvp',\n",
       "  'sha': 'c1d9aeb879f3079101f716f1f6e7109fdd18b4e9',\n",
       "  'lastModified': '2022-06-27T02:27:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mvp',\n",
       "   'en',\n",
       "   'arxiv:2206.12131',\n",
       "   'transformers',\n",
       "   'text-generation',\n",
       "   'text2text-generation',\n",
       "   'summarization',\n",
       "   'conversational',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'RUCAIBox',\n",
       "  'config': {'model_type': 'mvp'},\n",
       "  'id': 'RUCAIBox/mvp',\n",
       "  'downloads': 19543,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n",
       "    'example_title': 'Summarization'},\n",
       "   {'text': 'Given the dialog: do you like dance? [SEP] Yes I do. Did you know Bruce Lee was a cha cha dancer?',\n",
       "    'example_title': 'Dialog'},\n",
       "   {'text': 'Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man',\n",
       "    'example_title': 'Data-to-text'},\n",
       "   {'text': 'Given the story title: I think all public schools should have a uniform dress code.',\n",
       "    'example_title': 'Story Generation'},\n",
       "   {'text': 'Answer the following question: From which country did Angola achieve independence in 1975?',\n",
       "    'example_title': 'Question Answering'},\n",
       "   {'text': 'Generate the question based on the answer: boxing [X_SEP] A bolo punch is a punch used in martial arts . A hook is a punch in boxing .',\n",
       "    'example_title': 'Question Generaion'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'language': ['en'],\n",
       "   'tags': ['text-generation',\n",
       "    'text2text-generation',\n",
       "    'summarization',\n",
       "    'conversational'],\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'widget': [{'text': \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n",
       "     'example_title': 'Summarization'},\n",
       "    {'text': 'Given the dialog: do you like dance? [SEP] Yes I do. Did you know Bruce Lee was a cha cha dancer?',\n",
       "     'example_title': 'Dialog'},\n",
       "    {'text': 'Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man',\n",
       "     'example_title': 'Data-to-text'},\n",
       "    {'text': 'Given the story title: I think all public schools should have a uniform dress code.',\n",
       "     'example_title': 'Story Generation'},\n",
       "    {'text': 'Answer the following question: From which country did Angola achieve independence in 1975?',\n",
       "     'example_title': 'Question Answering'},\n",
       "    {'text': 'Generate the question based on the answer: boxing [X_SEP] A bolo punch is a punch used in martial arts . A hook is a punch in boxing .',\n",
       "     'example_title': 'Question Generaion'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/t5-base-qa-qg-hl': {'modelId': 'valhalla/t5-base-qa-qg-hl',\n",
       "  'sha': '0286be61d8d9de5650fdd21ed8923a7bc226e704',\n",
       "  'lastModified': '2020-12-11T22:03:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-base-qa-qg-hl',\n",
       "  'downloads': 19317,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TalTechNLP/voxlingua107-epaca-tdnn': {'modelId': 'TalTechNLP/voxlingua107-epaca-tdnn',\n",
       "  'sha': 'f35eaf95daf2040cc68ececfd45bbb5e47c44b1c',\n",
       "  'lastModified': '2021-11-04T13:37:27.000Z',\n",
       "  'tags': ['multilingual',\n",
       "   'dataset:VoxLingua107',\n",
       "   'speechbrain',\n",
       "   'audio-classification',\n",
       "   'embeddings',\n",
       "   'Language',\n",
       "   'Identification',\n",
       "   'pytorch',\n",
       "   'ECAPA-TDNN',\n",
       "   'TDNN',\n",
       "   'VoxLingua107',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'audio-classification',\n",
       "  'private': False,\n",
       "  'author': 'TalTechNLP',\n",
       "  'config': {'speechbrain': {'interface': 'EncoderClassifier'}},\n",
       "  'id': 'TalTechNLP/voxlingua107-epaca-tdnn',\n",
       "  'downloads': 19299,\n",
       "  'library_name': 'speechbrain',\n",
       "  'widgetData': [{'example_title': 'English Sample',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/LibriSpeech_61-70968-0000.flac'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'thumbnail': None,\n",
       "   'tags': ['audio-classification',\n",
       "    'speechbrain',\n",
       "    'embeddings',\n",
       "    'Language',\n",
       "    'Identification',\n",
       "    'pytorch',\n",
       "    'ECAPA-TDNN',\n",
       "    'TDNN',\n",
       "    'VoxLingua107'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['VoxLingua107'],\n",
       "   'metrics': ['Accuracy'],\n",
       "   'widget': [{'example_title': 'English Sample',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/LibriSpeech_61-70968-0000.flac'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'obi/deid_bert_i2b2': {'modelId': 'obi/deid_bert_i2b2',\n",
       "  'sha': '8ceb8983df9f0bf75d8c4bac345e157d80b4a5f7',\n",
       "  'lastModified': '2022-02-16T14:41:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'english',\n",
       "   'dataset:I2B2',\n",
       "   'arxiv:1904.03323',\n",
       "   'transformers',\n",
       "   'deidentification',\n",
       "   'medical notes',\n",
       "   'ehr',\n",
       "   'phi',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'obi',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'obi/deid_bert_i2b2',\n",
       "  'downloads': 19288,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'},\n",
       "   {'text': 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home).'},\n",
       "   {'text': 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['english'],\n",
       "   'thumbnail': 'https://www.onebraveidea.org/wp-content/uploads/2019/07/OBI-Logo-Website.png',\n",
       "   'tags': ['deidentification', 'medical notes', 'ehr', 'phi'],\n",
       "   'datasets': ['I2B2'],\n",
       "   'metrics': ['F1', 'Recall', 'AUC'],\n",
       "   'widget': [{'text': 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'},\n",
       "    {'text': 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home).'},\n",
       "    {'text': 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monsoon-nlp/hindi-bert': {'modelId': 'monsoon-nlp/hindi-bert',\n",
       "  'sha': '35f95927136cc5ea05db2fb2af1fb1455f5b310e',\n",
       "  'lastModified': '2020-08-26T22:14:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'feature-extraction',\n",
       "   'hi',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'monsoon-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'electra'},\n",
       "  'id': 'monsoon-nlp/hindi-bert',\n",
       "  'downloads': 19243,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'hi'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'wietsedv/bert-base-dutch-cased-finetuned-sentiment': {'modelId': 'wietsedv/bert-base-dutch-cased-finetuned-sentiment',\n",
       "  'sha': 'c9802e7e1da3cbef9e33a3e745d6af1e923a4c8b',\n",
       "  'lastModified': '2022-06-23T14:05:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'wietsedv',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'wietsedv/bert-base-dutch-cased-finetuned-sentiment',\n",
       "  'downloads': 19141,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-small-ex-discriminator': {'modelId': 'hfl/chinese-electra-180g-small-ex-discriminator',\n",
       "  'sha': '01785e80a5c6601a86bb7cc8d74c20be82646cba',\n",
       "  'lastModified': '2021-03-03T01:25:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-small-ex-discriminator',\n",
       "  'downloads': 19041,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'xlm-mlm-en-2048': {'modelId': 'xlm-mlm-en-2048',\n",
       "  'sha': '081c89fe33722bbc45b410c43faa09d8eeca889a',\n",
       "  'lastModified': '2021-07-08T09:45:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:cc-by-nc-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMWithLMHeadModel'], 'model_type': 'xlm'},\n",
       "  'id': 'xlm-mlm-en-2048',\n",
       "  'downloads': 19039,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris is the <special1> of France.'},\n",
       "   {'text': 'The goal of life is <special1>.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'], 'license': 'cc-by-nc-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-large-v2': {'modelId': 'albert-large-v2',\n",
       "  'sha': 'c76159dc6b4d18f16d303451ae64b4f34a7d0d63',\n",
       "  'lastModified': '2021-01-13T15:35:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-large-v2',\n",
       "  'downloads': 18970,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Filosofas/DialoGPT-medium-PALPATINE': {'modelId': 'Filosofas/DialoGPT-medium-PALPATINE',\n",
       "  'sha': '321b76cbcf40d9c9efa7776ba1eb80be7946211a',\n",
       "  'lastModified': '2022-02-08T11:50:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Filosofas',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Filosofas/DialoGPT-medium-PALPATINE',\n",
       "  'downloads': 18934,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-clm-ende-1024': {'modelId': 'xlm-clm-ende-1024',\n",
       "  'sha': '413761297209305ce09cdcf26596b0adef5cc87d',\n",
       "  'lastModified': '2021-07-08T09:59:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMWithLMHeadModel'], 'model_type': 'xlm'},\n",
       "  'id': 'xlm-clm-ende-1024',\n",
       "  'downloads': 18928,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris is the <special1> of France.'},\n",
       "   {'text': 'The goal of life is <special1>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-greek-uncased-v1': {'modelId': 'nlpaueb/bert-base-greek-uncased-v1',\n",
       "  'sha': 'ec2b8f88dd215b5246f2f850413d5bff90d7540d',\n",
       "  'lastModified': '2022-03-02T16:32:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'el',\n",
       "   'arxiv:2008.12014',\n",
       "   'transformers',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-greek-uncased-v1',\n",
       "  'downloads': 18787,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Œ£ŒÆŒºŒµœÅŒ± ŒµŒØŒΩŒ±Œπ ŒºŒπŒ± [MASK] ŒºŒ≠œÅŒ±.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'el',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'thumbnail': 'https://github.com/nlpaueb/GreekBERT/raw/master/greek-bert-logo.png',\n",
       "   'widget': [{'text': 'Œ£ŒÆŒºŒµœÅŒ± ŒµŒØŒΩŒ±Œπ ŒºŒπŒ± [MASK] ŒºŒ≠œÅŒ±.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jean-Baptiste/camembert-ner': {'modelId': 'Jean-Baptiste/camembert-ner',\n",
       "  'sha': 'dbec8489a1c44ecad9da8a9185115bccabd799fe',\n",
       "  'lastModified': '2022-04-04T01:13:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'Jean-Baptiste/camembert-ner',\n",
       "  'downloads': 18776,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je m'appelle jean-baptiste et je vis √† montr√©al\"},\n",
       "   {'text': 'george washington est all√© √† washington'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Je m'appelle jean-baptiste et je vis √† montr√©al\"},\n",
       "    {'text': 'george washington est all√© √† washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'setu4993/smaller-LaBSE': {'modelId': 'setu4993/smaller-LaBSE',\n",
       "  'sha': 'abd4e324cf0850b32f1dbf4b08fad6022ab47c0b',\n",
       "  'lastModified': '2021-12-05T06:13:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'ko',\n",
       "   'nl',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ru',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'zh',\n",
       "   'dataset:CommonCrawl',\n",
       "   'dataset:Wikipedia',\n",
       "   'arxiv:2010.05609',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'multilingual',\n",
       "   'google',\n",
       "   'sentence-similarity',\n",
       "   'labse',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'setu4993',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'setu4993/smaller-LaBSE',\n",
       "  'downloads': 18751,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'ko',\n",
       "    'nl',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ru',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'zh'],\n",
       "   'tags': ['bert',\n",
       "    'sentence_embedding',\n",
       "    'multilingual',\n",
       "    'google',\n",
       "    'sentence-similarity',\n",
       "    'labse'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['CommonCrawl', 'Wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-30b': {'modelId': 'facebook/opt-30b',\n",
       "  'sha': '463007d7da4e87fe962909a027811a8c0b32ede8',\n",
       "  'lastModified': '2022-06-23T16:42:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-30b',\n",
       "  'downloads': 18728,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 68,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-da-en': {'modelId': 'Helsinki-NLP/opus-mt-da-en',\n",
       "  'sha': '8971eb3839ec41bddd060128b9b83038bb43fd96',\n",
       "  'lastModified': '2021-09-09T21:29:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'da',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-da-en',\n",
       "  'downloads': 18710,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-xlsr-53': {'modelId': 'facebook/wav2vec2-large-xlsr-53',\n",
       "  'sha': 'c3f9d884181a224a6ac87bf8885c84d1cff3384f',\n",
       "  'lastModified': '2022-03-18T16:11:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'multilingual',\n",
       "   'dataset:common_voice',\n",
       "   'arxiv:2006.13979',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-xlsr-53',\n",
       "  'downloads': 18649,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'datasets': ['common_voice'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'indobenchmark/indobert-base-p1': {'modelId': 'indobenchmark/indobert-base-p1',\n",
       "  'sha': 'c2cd0b51ddce6580eb35263b39b0a1e5fb0a39e2',\n",
       "  'lastModified': '2021-05-19T20:22:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'id',\n",
       "   'dataset:Indo4B',\n",
       "   'arxiv:2009.05387',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indobenchmark',\n",
       "   'indonlu',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'indobenchmark',\n",
       "  'config': {'architectures': ['BertModel'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'indobenchmark/indobert-base-p1',\n",
       "  'downloads': 18610,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indobenchmark', 'indonlu'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['Indo4B']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v3-xsmall': {'modelId': 'microsoft/deberta-v3-xsmall',\n",
       "  'sha': 'a4c41b18332abc2f8df65cd0528883373cf09923',\n",
       "  'lastModified': '2022-01-13T18:28:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-xsmall',\n",
       "  'downloads': 18443,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rinna/japanese-gpt-1b': {'modelId': 'rinna/japanese-gpt-1b',\n",
       "  'sha': 'a3c6e8478d5afa92fe5174b984555e01fe378cd3',\n",
       "  'lastModified': '2022-02-18T04:46:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:c4',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'gpt',\n",
       "   'lm',\n",
       "   'nlp',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'rinna/japanese-gpt-1b',\n",
       "  'downloads': 18277,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Ë•øÁî∞ÂπæÂ§öÈÉé„ÅØ„ÄÅ'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-pretrained-models/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'gpt', 'text-generation', 'lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia', 'c4'],\n",
       "   'widget': [{'text': 'Ë•øÁî∞ÂπæÂ§öÈÉé„ÅØ„ÄÅ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ctrl': {'modelId': 'ctrl',\n",
       "  'sha': '07e9f73d6b0d0dda24506e8ad9ad4fb5cf87f4c9',\n",
       "  'lastModified': '2021-09-01T08:15:13.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'ctrl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'model_type': 'ctrl'},\n",
       "  'id': 'ctrl',\n",
       "  'downloads': 18240,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english': {'modelId': 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english',\n",
       "  'sha': '1dfc430017617e79fd19a2b60b30d6b217f64d28',\n",
       "  'lastModified': '2021-05-20T07:12:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english',\n",
       "  'downloads': 18137,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/processor_with_lm': {'modelId': 'hf-internal-testing/processor_with_lm',\n",
       "  'sha': '5477bdaf3c221237d0859ebcd6c6aa49e4a7d804',\n",
       "  'lastModified': '2022-01-18T13:19:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'hf-internal-testing/processor_with_lm',\n",
       "  'downloads': 18132,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'moussaKam/mbarthez': {'modelId': 'moussaKam/mbarthez',\n",
       "  'sha': '303813ade47c885979189a169c1449669fdc546f',\n",
       "  'lastModified': '2021-11-15T13:01:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'fr',\n",
       "   'arxiv:2010.12321',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'fill-mask',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'moussaKam',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'moussaKam/mbarthez',\n",
       "  'downloads': 18117,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'language': ['fr'],\n",
       "   'license': 'apache-2.0',\n",
       "   'pipeline_tag': 'fill-mask'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext': {'modelId': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext',\n",
       "  'sha': 'c1f013fb438445557fa71a012928e233a9c5c777',\n",
       "  'lastModified': '2021-05-24T09:59:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2010.11784',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cambridgeltl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext',\n",
       "  'downloads': 18098,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Maltehb/danish-bert-botxo': {'modelId': 'Maltehb/danish-bert-botxo',\n",
       "  'sha': '565d9bd5ca0872bec0b2d7af7887607a96416c2f',\n",
       "  'lastModified': '2021-11-12T08:34:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'da',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:dindebat.dk',\n",
       "   'dataset:hestenettet.dk',\n",
       "   'dataset:danish OpenSubtitles',\n",
       "   'transformers',\n",
       "   'danish',\n",
       "   'masked-lm',\n",
       "   'Certainly',\n",
       "   'license:cc-by-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Maltehb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'Maltehb/danish-bert-botxo',\n",
       "  'downloads': 18095,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'K√∏benhavn er [MASK] i Danmark.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'da',\n",
       "   'tags': ['danish', 'bert', 'masked-lm', 'Certainly'],\n",
       "   'license': 'cc-by-4.0',\n",
       "   'datasets': ['common_crawl',\n",
       "    'wikipedia',\n",
       "    'dindebat.dk',\n",
       "    'hestenettet.dk',\n",
       "    'danish OpenSubtitles'],\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': 'K√∏benhavn er [MASK] i Danmark.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-ru-en': {'modelId': 'facebook/wmt19-ru-en',\n",
       "  'sha': 'd145f3063a3e25e43c04c9aa64de38999b3fb2cd',\n",
       "  'lastModified': '2020-12-11T21:40:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-ru-en',\n",
       "  'downloads': 17988,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –í–æ–ª—å—Ñ–≥–∞–Ω–≥ –∏ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª–∏–Ω–µ'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-base-chinese': {'modelId': 'ckiplab/albert-base-chinese',\n",
       "  'sha': 'ed9a51e41fcf0cb4dec5aa3cbd7cdeb40b3e0099',\n",
       "  'lastModified': '2022-05-10T03:28:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-base-chinese',\n",
       "  'downloads': 17981,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-german-large': {'modelId': 'flair/ner-german-large',\n",
       "  'sha': 'd8943c40a867161a5a5b7ce91f31adaea1c3a424',\n",
       "  'lastModified': '2021-05-08T15:36:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'de',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-german-large',\n",
       "  'downloads': 17976,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington ging nach Washington'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'de',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington ging nach Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'allenai/led-large-16384': {'modelId': 'allenai/led-large-16384',\n",
       "  'sha': '04472a9a5d3af2efe700dda11da6063c68cd27a4',\n",
       "  'lastModified': '2021-01-11T14:51:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-large-16384',\n",
       "  'downloads': 17967,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-hi-en': {'modelId': 'Helsinki-NLP/opus-mt-hi-en',\n",
       "  'sha': '48acb8543e96768bc9fea5b3813d0e46fe1a45f3',\n",
       "  'lastModified': '2021-09-09T22:09:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'hi',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-hi-en',\n",
       "  'downloads': 17928,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-lv60': {'modelId': 'facebook/wav2vec2-large-lv60',\n",
       "  'sha': '0cde644b64dac88d8416bec1c92a4099b850ba0b',\n",
       "  'lastModified': '2021-12-28T12:45:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-lv60',\n",
       "  'downloads': 17866,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'cardiffnlp/twitter-roberta-base': {'modelId': 'cardiffnlp/twitter-roberta-base',\n",
       "  'sha': 'aafc670a946810bfad6966fcabeb5b37b6388930',\n",
       "  'lastModified': '2021-05-20T15:13:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base',\n",
       "  'downloads': 17824,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/clip-ViT-B-32-multilingual-v1': {'modelId': 'sentence-transformers/clip-ViT-B-32-multilingual-v1',\n",
       "  'sha': '200b64f20b3cef15ade0d31b1392519a46024087',\n",
       "  'lastModified': '2022-06-15T20:17:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:2004.09813',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/clip-ViT-B-32-multilingual-v1',\n",
       "  'downloads': 17767,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-xl': {'modelId': 'google/t5-v1_1-xl',\n",
       "  'sha': 'a9e51c46bd6f3893213c51edf9498be6f0426797',\n",
       "  'lastModified': '2020-11-19T19:55:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-xl',\n",
       "  'downloads': 17640,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/bart-base-chinese': {'modelId': 'fnlp/bart-base-chinese',\n",
       "  'sha': 'cac82280f9cafd2e7bb76fc32123717e335397ba',\n",
       "  'lastModified': '2021-10-31T15:05:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'Chinese',\n",
       "   'seq2seq',\n",
       "   'BART'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/bart-base-chinese',\n",
       "  'downloads': 17598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['text2text-generation', 'Chinese', 'seq2seq', 'BART'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KB/bert-base-swedish-cased': {'modelId': 'KB/bert-base-swedish-cased',\n",
       "  'sha': '81c7baa04742a30cb6732c181e678721868cb42e',\n",
       "  'lastModified': '2022-06-07T16:31:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KB',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KB/bert-base-swedish-cased',\n",
       "  'downloads': 17480,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KB/bert-base-swedish-cased-ner': {'modelId': 'KB/bert-base-swedish-cased-ner',\n",
       "  'sha': 'e1c9ae76afa22ce28d2097310ab95312d73e4e3a',\n",
       "  'lastModified': '2022-06-07T16:34:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'KB',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'KB/bert-base-swedish-cased-ner',\n",
       "  'downloads': 17321,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-large-japanese': {'modelId': 'cl-tohoku/bert-large-japanese',\n",
       "  'sha': '0f5fc4dcd523bed677d208cfa7279c80b1e8e8dc',\n",
       "  'lastModified': '2021-09-23T13:45:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-large-japanese',\n",
       "  'downloads': 17254,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/rugpt3small_based_on_gpt2': {'modelId': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
       "  'sha': 'f2f7c585b05a16726efe8974586e10b4d5939082',\n",
       "  'lastModified': '2021-09-21T19:30:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
       "  'downloads': 17176,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –ñ—é–ª—å–µ–Ω –∏'},\n",
       "   {'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –¢–æ–º–∞—Å –∏ –º–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π'},\n",
       "   {'text': '–û–¥–Ω–∞–∂–¥—ã'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/ru-gpts'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/overlapped-speech-detection': {'modelId': 'pyannote/overlapped-speech-detection',\n",
       "  'sha': '77439be79bd6ddddaabfe0916c146742265e9614',\n",
       "  'lastModified': '2022-03-23T09:23:32.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/overlapped-speech-detection',\n",
       "  'downloads': 17130,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hfl/chinese-macbert-large': {'modelId': 'hfl/chinese-macbert-large',\n",
       "  'sha': '1cf2677c782975600ce58e2961656b1b29eddbae',\n",
       "  'lastModified': '2021-05-19T19:14:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-macbert-large',\n",
       "  'downloads': 16989,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-base-patch16-224': {'modelId': 'facebook/deit-base-patch16-224',\n",
       "  'sha': '7d4fdb12f05852e952cf14805435477bc2e4f99e',\n",
       "  'lastModified': '2022-06-23T07:49:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'facebook/deit-base-patch16-224',\n",
       "  'downloads': 16740,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification'],\n",
       "   'datasets': ['imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'avichr/heBERT_sentiment_analysis': {'modelId': 'avichr/heBERT_sentiment_analysis',\n",
       "  'sha': '022c0d00fc26288c25c0b9f5389d7f0991f93de2',\n",
       "  'lastModified': '2021-12-31T16:08:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'avichr',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'avichr/heBERT_sentiment_analysis',\n",
       "  'downloads': 16654,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-base-lm-adapt': {'modelId': 'google/t5-base-lm-adapt',\n",
       "  'sha': '82aa560c46d415609fa3403f4e94d2c1a90923af',\n",
       "  'lastModified': '2021-11-01T14:01:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-base-lm-adapt',\n",
       "  'downloads': 16589,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'climatebert/distilroberta-base-climate-f': {'modelId': 'climatebert/distilroberta-base-climate-f',\n",
       "  'sha': '046b11a4b8c9db2c04a1f170bd48f863e0d2cf47',\n",
       "  'lastModified': '2022-03-09T16:49:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2110.12010',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'climatebert',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'climatebert/distilroberta-base-climate-f',\n",
       "  'downloads': 16577,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-distilbert-cos-v1': {'modelId': 'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
       "  'sha': 'd3aaaee2b4f73c64bae6f6bb4433f8b50d620b4f',\n",
       "  'lastModified': '2021-08-23T18:22:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
       "  'downloads': 16329,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/muril-base-cased': {'modelId': 'google/muril-base-cased',\n",
       "  'sha': 'afd9f36c7923d54e97903922ff1b260d091d202f',\n",
       "  'lastModified': '2022-06-10T13:33:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:2103.10730',\n",
       "   'arxiv:1810.04805',\n",
       "   'arxiv:1911.02116',\n",
       "   'arxiv:2003.11080',\n",
       "   'arxiv:2009.05166',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'google/muril-base-cased',\n",
       "  'downloads': 16319,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SpanBERT/spanbert-base-cased': {'modelId': 'SpanBERT/spanbert-base-cased',\n",
       "  'sha': 'b436fe68816aa04256692ce7e27711bf6be15513',\n",
       "  'lastModified': '2021-05-19T11:30:27.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SpanBERT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SpanBERT/spanbert-base-cased',\n",
       "  'downloads': 16313,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'textattack/bert-base-uncased-ag-news': {'modelId': 'textattack/bert-base-uncased-ag-news',\n",
       "  'sha': 'fe417ad660b1657142f66353a184dc0c7e6d2e48',\n",
       "  'lastModified': '2021-05-20T07:40:21.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-ag-news',\n",
       "  'downloads': 16243,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/sentence_bert': {'modelId': 'deepset/sentence_bert',\n",
       "  'sha': '496b9b39b227f03c4053a9f5fdac1616773b5112',\n",
       "  'lastModified': '2021-05-19T15:34:03.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers', 'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'deepset/sentence_bert',\n",
       "  'downloads': 16098,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Seznam/small-e-czech': {'modelId': 'Seznam/small-e-czech',\n",
       "  'sha': '0e933d0b8d8dde2b3f5cc76444e221c0f407536c',\n",
       "  'lastModified': '2022-05-11T10:46:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'cs',\n",
       "   'arxiv:2003.10555',\n",
       "   'arxiv:2112.01810',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'Seznam',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'Seznam/small-e-czech',\n",
       "  'downloads': 15901,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'cs', 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ckiplab/bert-base-chinese-ws': {'modelId': 'ckiplab/bert-base-chinese-ws',\n",
       "  'sha': '60c22ced1c0ec221242906e8f9fbdf90fb560b77',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-ws',\n",
       "  'downloads': 15892,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàë‰ΩèÂú®Âä†Â∑û‰ºØÂÖãÂà©„ÄÇ'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/voice-activity-detection': {'modelId': 'pyannote/voice-activity-detection',\n",
       "  'sha': '1ba76fd68b766cdff0fb95ace5699757838ea6cb',\n",
       "  'lastModified': '2022-03-23T09:24:05.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'voice-activity-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/voice-activity-detection',\n",
       "  'downloads': 15608,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'voice-activity-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dmis-lab/biobert-base-cased-v1.2': {'modelId': 'dmis-lab/biobert-base-cased-v1.2',\n",
       "  'sha': '67c9c25b46986521ca33df05d8540da1210b3256',\n",
       "  'lastModified': '2021-06-24T02:54:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'dmis-lab/biobert-base-cased-v1.2',\n",
       "  'downloads': 15570,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-tiny-finetuned-sms-spam-detection': {'modelId': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
       "  'sha': '49541ae4a47b52d4b2e6d5a3a1875c5e35aebeb1',\n",
       "  'lastModified': '2021-05-20T00:40:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sms_spam',\n",
       "   'transformers',\n",
       "   'sms',\n",
       "   'spam',\n",
       "   'detection'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
       "  'downloads': 15525,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sms', 'spam', 'detection'],\n",
       "   'datasets': ['sms_spam'],\n",
       "   'widget': [{'text': 'Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-tr-en': {'modelId': 'Helsinki-NLP/opus-mt-tr-en',\n",
       "  'sha': '3252b40d8b9dead8012364425fd00db1a26abf85',\n",
       "  'lastModified': '2021-09-11T10:49:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'tr',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-tr-en',\n",
       "  'downloads': 15510,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SZTAKI-HLT/hubert-base-cc': {'modelId': 'SZTAKI-HLT/hubert-base-cc',\n",
       "  'sha': 'f9b2da95ca4080247005d54b81a41bf98c65acb5',\n",
       "  'lastModified': '2021-05-19T11:29:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'hu',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SZTAKI-HLT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SZTAKI-HLT/hubert-base-cc',\n",
       "  'downloads': 15476,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'hu',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_crawl', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-albert-small-v2': {'modelId': 'sentence-transformers/paraphrase-albert-small-v2',\n",
       "  'sha': 'e31b7cd6bb1c99f3fae6aa41bc89da1c8306c94f',\n",
       "  'lastModified': '2022-06-21T14:56:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'albert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['AlbertModel'], 'model_type': 'albert'},\n",
       "  'id': 'sentence-transformers/paraphrase-albert-small-v2',\n",
       "  'downloads': 15398,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/bern2-ner': {'modelId': 'dmis-lab/bern2-ner',\n",
       "  'sha': 'e09c2e59b9e90cdf3f5c55e3923fe2c2034070a9',\n",
       "  'lastModified': '2021-10-27T06:15:12.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['RoBERTaMultiNER2'], 'model_type': 'roberta'},\n",
       "  'id': 'dmis-lab/bern2-ner',\n",
       "  'downloads': 15396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'RoBERTaMultiNER2',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'samrawal/bert-base-uncased_clinical-ner': {'modelId': 'samrawal/bert-base-uncased_clinical-ner',\n",
       "  'sha': 'db93d0fda8da893ad16484174f11ebc1ecb00a49',\n",
       "  'lastModified': '2022-05-28T15:56:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'samrawal',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'samrawal/bert-base-uncased_clinical-ner',\n",
       "  'downloads': 15268,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlmv3-base': {'modelId': 'microsoft/layoutlmv3-base',\n",
       "  'sha': '3ef8cc27ab279a6ef5b2b7841d4ce6e7386b1825',\n",
       "  'lastModified': '2022-05-03T07:30:33.000Z',\n",
       "  'tags': ['pytorch', 'layoutlmv3', 'arxiv:2204.08387', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv3'},\n",
       "  'id': 'microsoft/layoutlmv3-base',\n",
       "  'downloads': 15229,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'clips/mfaq': {'modelId': 'clips/mfaq',\n",
       "  'sha': 'a49f0160e4d170a3d12e90ace3990fd6a50e35aa',\n",
       "  'lastModified': '2021-10-15T06:21:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'cs',\n",
       "   'da',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'id',\n",
       "   'it',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sv',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'dataset:clips/mfaq',\n",
       "   'arxiv:2109.12870',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'clips',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'clips/mfaq',\n",
       "  'downloads': 15207,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': '<Q>How many models can I host on HuggingFace?',\n",
       "    'sentences': ['<A>All plans come with unlimited private models and datasets.',\n",
       "     '<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.',\n",
       "     '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.']}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'language': ['cs',\n",
       "    'da',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'id',\n",
       "    'it',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sv',\n",
       "    'tr',\n",
       "    'vi'],\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'datasets': ['clips/mfaq'],\n",
       "   'widget': {'source_sentence': '<Q>How many models can I host on HuggingFace?',\n",
       "    'sentences': ['<A>All plans come with unlimited private models and datasets.',\n",
       "     '<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.',\n",
       "     '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.']}},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'PlanTL-GOB-ES/roberta-base-bne': {'modelId': 'PlanTL-GOB-ES/roberta-base-bne',\n",
       "  'sha': 'f4cc2aff5eaa2e1ad5add20a740d8578c833574a',\n",
       "  'lastModified': '2022-04-06T14:40:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'dataset:bne',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2107.07253',\n",
       "   'transformers',\n",
       "   'national library of spain',\n",
       "   'spanish',\n",
       "   'bne',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'PlanTL-GOB-ES',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'PlanTL-GOB-ES/roberta-base-bne',\n",
       "  'downloads': 15131,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Este a√±o las campanadas de La Sexta las presentar√° <mask>.'},\n",
       "   {'text': 'David Broncano es un presentador de La <mask>.'},\n",
       "   {'text': 'Gracias a los datos de la BNE se ha podido <mask> este modelo del lenguaje.'},\n",
       "   {'text': 'Hay base legal dentro del marco <mask> actual.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['national library of spain', 'spanish', 'bne'],\n",
       "   'datasets': ['bne'],\n",
       "   'metrics': ['ppl'],\n",
       "   'widget': [{'text': 'Este a√±o las campanadas de La Sexta las presentar√° <mask>.'},\n",
       "    {'text': 'David Broncano es un presentador de La <mask>.'},\n",
       "    {'text': 'Gracias a los datos de la BNE se ha podido <mask> este modelo del lenguaje.'},\n",
       "    {'text': 'Hay base legal dentro del marco <mask> actual.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gbert-base-germandpr-question_encoder': {'modelId': 'deepset/gbert-base-germandpr-question_encoder',\n",
       "  'sha': 'dcbc13b5d22e49f58549a54dc7f30edec6153c39',\n",
       "  'lastModified': '2021-10-21T12:17:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dpr',\n",
       "   'feature-extraction',\n",
       "   'de',\n",
       "   'dataset:deepset/germandpr',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/gbert-base-germandpr-question_encoder',\n",
       "  'downloads': 15058,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germandpr'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/infoxlm-large': {'modelId': 'microsoft/infoxlm-large',\n",
       "  'sha': 'd616d637f0720deda963cebbfc630657d2b7d3ae',\n",
       "  'lastModified': '2021-08-04T11:43:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2007.07834',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'microsoft/infoxlm-large',\n",
       "  'downloads': 15039,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510': {'modelId': 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510',\n",
       "  'sha': '8498b019a97bdf693fa138efcf7e118f099e2c4e',\n",
       "  'lastModified': '2021-12-05T12:12:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'dataset:mC4',\n",
       "   'dataset:UD_Japanese_BCCWJ r2.8',\n",
       "   'dataset:GSK2014-A(2019)',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'spaCy',\n",
       "   'ELECTRA',\n",
       "   'GiNZA',\n",
       "   'mC4',\n",
       "   'UD_Japanese-BCCWJ',\n",
       "   'GSK2014-A',\n",
       "   'MIT',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'megagonlabs',\n",
       "  'config': {'architectures': ['ElectraModel'], 'model_type': 'electra'},\n",
       "  'id': 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510',\n",
       "  'downloads': 15035,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/megagonlabs/ginza/static/docs/images/GiNZA_logo_4c_s.png',\n",
       "   'tags': ['PyTorch',\n",
       "    'Transformers',\n",
       "    'spaCy',\n",
       "    'ELECTRA',\n",
       "    'GiNZA',\n",
       "    'mC4',\n",
       "    'UD_Japanese-BCCWJ',\n",
       "    'GSK2014-A',\n",
       "    'ja',\n",
       "    'MIT'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['mC4', 'UD_Japanese_BCCWJ r2.8', 'GSK2014-A(2019)'],\n",
       "   'metrics': ['UAS', 'LAS', 'UPOS']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/distilbart-mnli-12-6': {'modelId': 'valhalla/distilbart-mnli-12-6',\n",
       "  'sha': 'f14383bf830237f338e7d597955f156590fddc2e',\n",
       "  'lastModified': '2021-06-14T10:32:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-6',\n",
       "  'downloads': 14818,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlnet-large-cased': {'modelId': 'xlnet-large-cased',\n",
       "  'sha': '09792d2c42dfb606155f1bea8873260b23887edd',\n",
       "  'lastModified': '2021-09-16T09:45:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1906.08237',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'xlnet-large-cased',\n",
       "  'downloads': 14799,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'wietsedv/bert-base-dutch-cased': {'modelId': 'wietsedv/bert-base-dutch-cased',\n",
       "  'sha': '2d09de2a6f34a25cb194c39e6281c5fbef317032',\n",
       "  'lastModified': '2021-05-20T09:12:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'wietsedv',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'wietsedv/bert-base-dutch-cased',\n",
       "  'downloads': 14705,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uclanlp/plbart-base': {'modelId': 'uclanlp/plbart-base',\n",
       "  'sha': 'cf5287241fcff3819f6ade49635dc2d77efee032',\n",
       "  'lastModified': '2021-11-09T17:07:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'plbart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uclanlp',\n",
       "  'config': {'architectures': ['PLBartForConditionalGeneration'],\n",
       "   'model_type': 'plbart'},\n",
       "  'id': 'uclanlp/plbart-base',\n",
       "  'downloads': 14682,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-Neo-2.7B-Shinen': {'modelId': 'KoboldAI/GPT-Neo-2.7B-Shinen',\n",
       "  'sha': '551fbd85138d4f29589ab07000cca813cb8a62ea',\n",
       "  'lastModified': '2022-03-20T18:49:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'KoboldAI/GPT-Neo-2.7B-Shinen',\n",
       "  'downloads': 14649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huawei-noah/TinyBERT_General_4L_312D': {'modelId': 'huawei-noah/TinyBERT_General_4L_312D',\n",
       "  'sha': '34707a33cd59a94ecde241ac209bf35103691b43',\n",
       "  'lastModified': '2021-05-19T20:03:32.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'arxiv:1909.10351', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'huawei-noah',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'huawei-noah/TinyBERT_General_4L_312D',\n",
       "  'downloads': 14565,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/electra-large-generator': {'modelId': 'google/electra-large-generator',\n",
       "  'sha': 'bbb1b8938d38e9f5dbfaaecc869320388b4fefe2',\n",
       "  'lastModified': '2021-04-30T07:44:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-large-generator',\n",
       "  'downloads': 14513,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/rubert-tiny2': {'modelId': 'cointegrated/rubert-tiny2',\n",
       "  'sha': 'a53f0afc4b34c94012191a043c52e2271fa23f27',\n",
       "  'lastModified': '2022-06-30T14:26:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'fill-mask',\n",
       "   'embeddings',\n",
       "   'masked-lm',\n",
       "   'tiny',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/rubert-tiny2',\n",
       "  'downloads': 14443,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '–ú–∏–Ω–∏–∞—Ç—é—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è [MASK] —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['russian',\n",
       "    'fill-mask',\n",
       "    'pretraining',\n",
       "    'embeddings',\n",
       "    'masked-lm',\n",
       "    'tiny',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '–ú–∏–Ω–∏–∞—Ç—é—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è [MASK] —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/tct_colbert-v2-hnp-msmarco': {'modelId': 'castorini/tct_colbert-v2-hnp-msmarco',\n",
       "  'sha': '3b46a821282996e0ada304e4bcc5d659712972a8',\n",
       "  'lastModified': '2021-08-12T01:05:56.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'castorini/tct_colbert-v2-hnp-msmarco',\n",
       "  'downloads': 14361,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli': {'modelId': 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli',\n",
       "  'sha': '33e5835aad1c32ac8707971141b65c3fc5ff1904',\n",
       "  'lastModified': '2022-02-25T19:07:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jbetker',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli',\n",
       "  'downloads': 14291,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'unitary/toxic-bert': {'modelId': 'unitary/toxic-bert',\n",
       "  'sha': '5cc53435803a6e6f1ac8e4b243910d3bf26803ff',\n",
       "  'lastModified': '2021-06-07T15:20:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:1703.04009',\n",
       "   'arxiv:1905.12516',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'unitary',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'unitary/toxic-bert',\n",
       "  'downloads': 14276,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/german-gpt2': {'modelId': 'dbmdz/german-gpt2',\n",
       "  'sha': 'f0edef6d975b1338bae533502e1dae74974cb2d2',\n",
       "  'lastModified': '2021-10-22T08:58:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'dbmdz/german-gpt2',\n",
       "  'downloads': 14234,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Heute ist sehr sch√∂nes Wetter in'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'Heute ist sehr sch√∂nes Wetter in'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hustvl/yolos-tiny': {'modelId': 'hustvl/yolos-tiny',\n",
       "  'sha': '3686e65df0c914833fc8cbeca745a33b374c499b',\n",
       "  'lastModified': '2022-06-27T08:37:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'yolos',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2106.00666',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'hustvl',\n",
       "  'config': {'architectures': ['YolosForObjectDetection'],\n",
       "   'model_type': 'yolos'},\n",
       "  'id': 'hustvl/yolos-tiny',\n",
       "  'downloads': 14078,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'facebook/detr-resnet-101': {'modelId': 'facebook/detr-resnet-101',\n",
       "  'sha': '1a655091c08729eecf4fc5063c27fa5ea82aeaa3',\n",
       "  'lastModified': '2022-06-27T08:30:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForObjectDetection'],\n",
       "   'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-101',\n",
       "  'downloads': 13867,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/bigbird-pegasus-large-arxiv': {'modelId': 'google/bigbird-pegasus-large-arxiv',\n",
       "  'sha': '9d87686a36f9c732db7deeae1dcac5fb085d0b90',\n",
       "  'lastModified': '2022-06-29T20:40:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bigbird_pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:scientific_papers',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdPegasusForConditionalGeneration'],\n",
       "   'model_type': 'bigbird_pegasus'},\n",
       "  'id': 'google/bigbird-pegasus-large-arxiv',\n",
       "  'downloads': 13783,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': [{'name': 'google/bigbird-pegasus-large-arxiv',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'scientific_papers',\n",
       "       'type': 'scientific_papers',\n",
       "       'config': 'pubmed',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 36.0276,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 13.4166,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 21.9612,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 29.648,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 2.774355173110962,\n",
       "        'verified': True},\n",
       "       {'name': 'meteor', 'type': 'meteor', 'value': 0.2824, 'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 209.2537,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['scientific_papers'],\n",
       "   'tags': ['summarization'],\n",
       "   'model-index': [{'name': 'google/bigbird-pegasus-large-arxiv',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'scientific_papers',\n",
       "        'type': 'scientific_papers',\n",
       "        'config': 'pubmed',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 36.0276,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 13.4166,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 21.9612,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 29.648,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 2.774355173110962,\n",
       "         'verified': True},\n",
       "        {'name': 'meteor',\n",
       "         'type': 'meteor',\n",
       "         'value': 0.2824,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 209.2537,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/bart-tiny-random': {'modelId': 'sshleifer/bart-tiny-random',\n",
       "  'sha': '69bce9237e4fa10ea015446395ec0108067890cf',\n",
       "  'lastModified': '2021-06-14T07:44:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'max_length': 10,\n",
       "     'min_length': 2,\n",
       "     'num_beams': 1}}},\n",
       "  'id': 'sshleifer/bart-tiny-random',\n",
       "  'downloads': 13764,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'transfo-xl-wt103': {'modelId': 'transfo-xl-wt103',\n",
       "  'sha': 'fe38e218e1949dbfa50ce7a8931019cc8dd20f38',\n",
       "  'lastModified': '2020-12-09T18:29:59.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'transfo-xl', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['TransfoXLLMHeadModel'],\n",
       "   'model_type': 'transfo-xl',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'transfo-xl-wt103',\n",
       "  'downloads': 13575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'j-hartmann/sentiment-roberta-large-english-3-classes': {'modelId': 'j-hartmann/sentiment-roberta-large-english-3-classes',\n",
       "  'sha': 'f995433eb6d79d26702ab9335bfde472a9933ee4',\n",
       "  'lastModified': '2022-02-06T12:26:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'twitter'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'j-hartmann',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'j-hartmann/sentiment-roberta-large-english-3-classes',\n",
       "  'downloads': 13568,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Oh no. This is bad..'},\n",
       "   {'text': 'To be or not to be.'},\n",
       "   {'text': 'Oh Happy Day'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['roberta', 'sentiment', 'twitter'],\n",
       "   'widget': [{'text': 'Oh no. This is bad..'},\n",
       "    {'text': 'To be or not to be.'},\n",
       "    {'text': 'Oh Happy Day'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli': {'modelId': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli',\n",
       "  'sha': '4954f857233ce526f1f1f061cfbecdd23ec1019f',\n",
       "  'lastModified': '2022-02-08T21:39:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta-v2',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:anli',\n",
       "   'dataset:fever',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'MoritzLaurer',\n",
       "  'config': {'architectures': ['DebertaV2ForSequenceClassification'],\n",
       "   'model_type': 'deberta-v2'},\n",
       "  'id': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli',\n",
       "  'downloads': 13477,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text-classification', 'zero-shot-classification'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'datasets': ['multi_nli', 'anli', 'fever'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/PRIMERA': {'modelId': 'allenai/PRIMERA',\n",
       "  'sha': '550385ac1f15be0309fddaa429d72a87ed60aa6b',\n",
       "  'lastModified': '2022-06-25T16:04:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'allenai/PRIMERA',\n",
       "  'downloads': 13474,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/distilrubert-base-cased-conversational': {'modelId': 'DeepPavlov/distilrubert-base-cased-conversational',\n",
       "  'sha': '9b4f8c20bdc51934ef2ef586ef9afee85549cccb',\n",
       "  'lastModified': '2022-05-06T11:58:43.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-base-cased-conversational',\n",
       "  'downloads': 13454,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hf-internal-testing/tiny-random-vit': {'modelId': 'hf-internal-testing/tiny-random-vit',\n",
       "  'sha': '1870c862512fd2c5c46337626d3fec558aa816f3',\n",
       "  'lastModified': '2022-03-02T15:34:35.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'vit', 'image-classification', 'transformers'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'hf-internal-testing/tiny-random-vit',\n",
       "  'downloads': 13444,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'HooshvareLab/albert-fa-zwnj-base-v2': {'modelId': 'HooshvareLab/albert-fa-zwnj-base-v2',\n",
       "  'sha': '2315c0d4ddd6bd68f7703c5651978706bf8aff37',\n",
       "  'lastModified': '2021-03-16T16:36:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'HooshvareLab/albert-fa-zwnj-base-v2',\n",
       "  'downloads': 13281,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ≤ŸÜÿØ⁄Ø€å €å⁄© ÿ≥ŸàÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿß€åŸÜ ⁄©Ÿá ⁄Ü⁄ØŸàŸÜŸá [MASK] ⁄©ŸÜ€åŸÖ Ÿæÿßÿ≥ÿÆ ÿß€åŸÜ ÿ≥ŸàÿßŸÑ!'},\n",
       "   {'text': 'ÿ≤ŸÜÿØ⁄Ø€å ÿßÿ≤ ŸÖÿ±⁄Ø Ÿæÿ±ÿ≥€åÿØ: ⁄Üÿ±ÿß ŸáŸÖŸá ŸÖŸÜ ÿ±ÿß [MASK] ÿØÿßÿ±ŸÜÿØ ÿßŸÖÿß ÿßÿ≤ ÿ™Ÿà ŸÖÿ™ŸÜŸÅÿ±ŸÜÿØÿü'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/albert-base-v2-yelp-polarity': {'modelId': 'textattack/albert-base-v2-yelp-polarity',\n",
       "  'sha': 'bbb5fb3997de43eedb58f7c74b8fbd63c719b5dd',\n",
       "  'lastModified': '2020-07-06T16:37:10.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'textattack/albert-base-v2-yelp-polarity',\n",
       "  'downloads': 13236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/roberta-large-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/roberta-large-nli-stsb-mean-tokens',\n",
       "  'sha': '768fca01ac32ae924414f7128af28ea1d9dfcada',\n",
       "  'lastModified': '2022-06-15T20:56:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-large-nli-stsb-mean-tokens',\n",
       "  'downloads': 13219,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpconnect/vit-gpt2-image-captioning': {'modelId': 'nlpconnect/vit-gpt2-image-captioning',\n",
       "  'sha': '27b41be193be4c2dc238990bad1c8d874b272a83',\n",
       "  'lastModified': '2022-07-01T07:38:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'image-captioning',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'nlpconnect',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'nlpconnect/vit-gpt2-image-captioning',\n",
       "  'downloads': 13178,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['image-to-text', 'image-captioning'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rakib/roberta-base-on-cuad': {'modelId': 'Rakib/roberta-base-on-cuad',\n",
       "  'sha': 'bc6033499692e08cef629b94b5dad636df956b24',\n",
       "  'lastModified': '2021-07-03T18:10:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'Rakib',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'Rakib/roberta-base-on-cuad',\n",
       "  'downloads': 13065,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese': {'modelId': 'ckiplab/bert-base-chinese',\n",
       "  'sha': 'efe27bb4a9373384e0120ffe1cf327714ceb61bf',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese',\n",
       "  'downloads': 13044,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two': {'modelId': 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two',\n",
       "  'sha': '7b1a724a178c639a4b3446c0ff8f13d19be4f471',\n",
       "  'lastModified': '2022-06-24T09:45:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:hatexplain',\n",
       "   'arxiv:2012.10289',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Hate-speech-CNERG',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two',\n",
       "  'downloads': 13043,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['hatexplain']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/tapas-base-finetuned-wtq': {'modelId': 'google/tapas-base-finetuned-wtq',\n",
       "  'sha': '69ceee21288ad8e3ffd045c02ce9c9ee15424abe',\n",
       "  'lastModified': '2021-11-29T13:02:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tapas',\n",
       "   'table-question-answering',\n",
       "   'en',\n",
       "   'dataset:wtq',\n",
       "   'arxiv:2004.02349',\n",
       "   'arxiv:2010.00571',\n",
       "   'arxiv:1508.00305',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'table-question-answering',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['TapasForQuestionAnswering'],\n",
       "   'model_type': 'tapas'},\n",
       "  'id': 'google/tapas-base-finetuned-wtq',\n",
       "  'downloads': 12926,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'How many stars does the transformers repository have?',\n",
       "    'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'],\n",
       "     'Stars': [36542, 4512, 3934],\n",
       "     'Contributors': [651, 77, 34],\n",
       "     'Programming language': ['Python',\n",
       "      'Python',\n",
       "      'Rust, Python and NodeJS']}}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapas'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wtq']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTableQuestionAnswering',\n",
       "   'pipeline_tag': 'table-question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-yelp-polarity': {'modelId': 'textattack/bert-base-uncased-yelp-polarity',\n",
       "  'sha': 'a4d0a85ea6c1d5bb944dcc12ea5c918863e469a4',\n",
       "  'lastModified': '2021-05-20T07:49:07.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-yelp-polarity',\n",
       "  'downloads': 12693,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dumitrescustefan/bert-base-romanian-cased-v1': {'modelId': 'dumitrescustefan/bert-base-romanian-cased-v1',\n",
       "  'sha': '9718c77b8a4f402f3d2a9202e9c918f7fdcdcceb',\n",
       "  'lastModified': '2021-11-02T15:25:55.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'ro', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dumitrescustefan',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dumitrescustefan/bert-base-romanian-cased-v1',\n",
       "  'downloads': 12687,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ro'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es': {'modelId': 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
       "  'sha': 'a8842522e00a16b382c875ecb2da2dd8cf7cf5b6',\n",
       "  'lastModified': '2022-03-30T20:37:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
       "  'downloads': 12598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '¬øD√≥nde vivo?',\n",
       "    'context': 'Me llamo Wolfgang y vivo en Berlin'},\n",
       "   {'text': '¬øQui√©n invent√≥ el submarino?',\n",
       "    'context': 'Isaac Peral fue un murciano que invent√≥ el submarino'},\n",
       "   {'text': '¬øCu√°ntas personas hablan espa√±ol?',\n",
       "    'context': 'El espa√±ol es el segundo idioma m√°s hablado del mundo con m√°s de 442 millones de hablantes'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'thumbnail': 'https://i.imgur.com/jgBdimh.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-large': {'modelId': 'vinai/bertweet-large',\n",
       "  'sha': '67477168d449ccc8abb725e2123a0d6e44f27f4b',\n",
       "  'lastModified': '2022-06-08T04:43:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-large',\n",
       "  'downloads': 12551,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/xtremedistil-l6-h256-uncased': {'modelId': 'microsoft/xtremedistil-l6-h256-uncased',\n",
       "  'sha': '8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0',\n",
       "  'lastModified': '2021-08-05T17:49:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2106.04563',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/xtremedistil-l6-h256-uncased',\n",
       "  'downloads': 12540,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-nl': {'modelId': 'Helsinki-NLP/opus-mt-en-nl',\n",
       "  'sha': 'cd888e7c566d69f91f675d7a12d26613d1c4d826',\n",
       "  'lastModified': '2021-09-09T21:38:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'nl',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-nl',\n",
       "  'downloads': 12503,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'elastic/distilbert-base-uncased-finetuned-conll03-english': {'modelId': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "  'sha': '0e98652673725eab6929978aeb28d8dffc614818',\n",
       "  'lastModified': '2022-06-24T09:30:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'elastic',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "  'downloads': 12485,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': [{'name': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "    'results': [{'task': {'type': 'token-classification',\n",
       "       'name': 'Token Classification'},\n",
       "      'dataset': {'name': 'conll2003',\n",
       "       'type': 'conll2003',\n",
       "       'config': 'conll2003',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9854480753649896,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9880928983228512,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9895677847945542,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9888297915932504,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.06707527488470078,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['conll2003'],\n",
       "   'model-index': [{'name': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "     'results': [{'task': {'type': 'token-classification',\n",
       "        'name': 'Token Classification'},\n",
       "       'dataset': {'name': 'conll2003',\n",
       "        'type': 'conll2003',\n",
       "        'config': 'conll2003',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9854480753649896,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9880928983228512,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9895677847945542,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9888297915932504,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.06707527488470078,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pin/senda': {'modelId': 'pin/senda',\n",
       "  'sha': '0ad8d1953f6a3a27a432a20b957d7e1129cdcbbc',\n",
       "  'lastModified': '2021-08-20T11:00:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'da',\n",
       "   'transformers',\n",
       "   'danish',\n",
       "   'sentiment',\n",
       "   'polarity',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pin',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'pin/senda',\n",
       "  'downloads': 12438,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Sikke en dejlig dag det er i dag'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'da',\n",
       "   'tags': ['danish', 'bert', 'sentiment', 'polarity'],\n",
       "   'license': 'cc-by-4.0',\n",
       "   'widget': [{'text': 'Sikke en dejlig dag det er i dag'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-pegasus-large-bigpatent': {'modelId': 'google/bigbird-pegasus-large-bigpatent',\n",
       "  'sha': '623321f538339e475269fdf79a258a5a7b796f4c',\n",
       "  'lastModified': '2021-06-03T18:26:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bigbird_pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:big_patent',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdPegasusForConditionalGeneration'],\n",
       "   'model_type': 'bigbird_pegasus'},\n",
       "  'id': 'google/bigbird-pegasus-large-bigpatent',\n",
       "  'downloads': 12344,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['big_patent'],\n",
       "   'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/longformer-large-4096': {'modelId': 'allenai/longformer-large-4096',\n",
       "  'sha': 'cfa97f5f8c58c219bfea4da030a0259d5dbb28c4',\n",
       "  'lastModified': '2021-03-10T02:31:17.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'longformer', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-large-4096',\n",
       "  'downloads': 12320,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openclimatefix/nowcasting_cnn_v2': {'modelId': 'openclimatefix/nowcasting_cnn_v2',\n",
       "  'sha': '5b9c2a9194869afae91651ffdf2ba1de1952d866',\n",
       "  'lastModified': '2022-05-25T10:41:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'transformers',\n",
       "   'nowcasting',\n",
       "   'forecasting',\n",
       "   'timeseries',\n",
       "   'remote-sensing',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'openclimatefix',\n",
       "  'config': {},\n",
       "  'id': 'openclimatefix/nowcasting_cnn_v2',\n",
       "  'downloads': 12186,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['nowcasting', 'forecasting', 'timeseries', 'remote-sensing']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ru': {'modelId': 'Helsinki-NLP/opus-mt-en-ru',\n",
       "  'sha': 'b4544f727b37dc7b186fa5d5a99baa74cd2f4128',\n",
       "  'lastModified': '2021-09-09T21:38:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ru',\n",
       "  'downloads': 12174,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanfordnlp/stanza-fr': {'modelId': 'stanfordnlp/stanza-fr',\n",
       "  'sha': 'f7ed3c2077c9f65e409975e8f1ba491be850b093',\n",
       "  'lastModified': '2022-05-28T08:08:45.000Z',\n",
       "  'tags': ['fr', 'stanza', 'token-classification', 'license:apache-2.0'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'stanfordnlp',\n",
       "  'config': None,\n",
       "  'id': 'stanfordnlp/stanza-fr',\n",
       "  'downloads': 12113,\n",
       "  'library_name': 'stanza',\n",
       "  'widgetData': [{'text': 'Mon nom est Wolfgang et je vis √† Berlin'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['stanza', 'token-classification'],\n",
       "   'library_name': 'stanza',\n",
       "   'language': 'fr',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/t5-large-ssm-nq': {'modelId': 'google/t5-large-ssm-nq',\n",
       "  'sha': '000abff88a3e622c77c59e4a83fb985e79c7c6e6',\n",
       "  'lastModified': '2021-06-23T01:35:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:natural_questions',\n",
       "   'arxiv:2002.08909',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-large-ssm-nq',\n",
       "  'downloads': 12041,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4', 'wikipedia', 'natural_questions'],\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gbert-base-germandpr-ctx_encoder': {'modelId': 'deepset/gbert-base-germandpr-ctx_encoder',\n",
       "  'sha': '12ac6df80a8a3a3301464a306cd412d01f43c082',\n",
       "  'lastModified': '2021-10-21T12:17:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dpr',\n",
       "   'de',\n",
       "   'dataset:deepset/germandpr',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/gbert-base-germandpr-ctx_encoder',\n",
       "  'downloads': 11997,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germandpr'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Recognai/bert-base-spanish-wwm-cased-xnli': {'modelId': 'Recognai/bert-base-spanish-wwm-cased-xnli',\n",
       "  'sha': '6219d7e4cc59999010c795ac26d2e014102e24ba',\n",
       "  'lastModified': '2021-10-15T15:55:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Recognai',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Recognai/bert-base-spanish-wwm-cased-xnli',\n",
       "  'downloads': 11961,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'El autor se perfila, a los 50 a√±os de su muerte, como uno de los grandes de su siglo',\n",
       "    'candidate_labels': 'cultura, sociedad, economia, salud, deportes'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'tags': ['zero-shot-classification', 'nli', 'pytorch'],\n",
       "   'datasets': ['xnli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'El autor se perfila, a los 50 a√±os de su muerte, como uno de los grandes de su siglo',\n",
       "     'candidate_labels': 'cultura, sociedad, economia, salud, deportes'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/codebert-base-mlm': {'modelId': 'microsoft/codebert-base-mlm',\n",
       "  'sha': '5c927614b8750b556dcf569cf8a211fbe20f688a',\n",
       "  'lastModified': '2021-05-20T17:47:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2002.08155',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/codebert-base-mlm',\n",
       "  'downloads': 11933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shahrukhx01/bert-mini-finetune-question-detection': {'modelId': 'shahrukhx01/bert-mini-finetune-question-detection',\n",
       "  'sha': 'c9088454657626f5e370b3a1ec993fdcad81aaf6',\n",
       "  'lastModified': '2021-07-11T14:27:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'neural-search-query-classification',\n",
       "   'neural-search'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'shahrukhx01',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'shahrukhx01/bert-mini-finetune-question-detection',\n",
       "  'downloads': 11877,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'keyword query.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['neural-search-query-classification', 'neural-search'],\n",
       "   'widget': [{'text': 'keyword query.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-xxlarge-v1': {'modelId': 'albert-xxlarge-v1',\n",
       "  'sha': '431c690c9f508b1cbcba8f475974833ac646d41c',\n",
       "  'lastModified': '2021-01-13T15:32:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-xxlarge-v1',\n",
       "  'downloads': 11875,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-uncased': {'modelId': 'dbmdz/bert-base-turkish-uncased',\n",
       "  'sha': '0582a4e05fd7ec5aa6b265d4bc4c81438d951593',\n",
       "  'lastModified': '2021-05-19T15:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-uncased',\n",
       "  'downloads': 11807,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'indolem/indobert-base-uncased': {'modelId': 'indolem/indobert-base-uncased',\n",
       "  'sha': 'b6663c19a819c04798e7a93d681f9bc34ed57b4a',\n",
       "  'lastModified': '2021-09-17T04:06:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'id',\n",
       "   'dataset:220M words (IndoWiki, IndoWC, News)]',\n",
       "   'arxiv:2011.00677',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indolem',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'indolem',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'indolem/indobert-base-uncased',\n",
       "  'downloads': 11791,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indolem'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['220M words (IndoWiki, IndoWC, News)]']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-wav2vec2': {'modelId': 'hf-internal-testing/tiny-random-wav2vec2',\n",
       "  'sha': '44e60a7cad2409b873242c874476c0c8ce8e98b0',\n",
       "  'lastModified': '2021-10-06T10:02:54.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'wav2vec2', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'wav2vec2'},\n",
       "  'id': 'hf-internal-testing/tiny-random-wav2vec2',\n",
       "  'downloads': 11790,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'beomi/kcbert-base': {'modelId': 'beomi/kcbert-base',\n",
       "  'sha': '1715c455b12198ef178917232788fc682f1f218f',\n",
       "  'lastModified': '2021-05-19T12:29:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'beomi',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'beomi/kcbert-base',\n",
       "  'downloads': 11664,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-bert-wwm': {'modelId': 'hfl/chinese-bert-wwm',\n",
       "  'sha': 'ab0aa81da273504efc8540aa4d0bbaa3016a1bb5',\n",
       "  'lastModified': '2021-05-19T19:07:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-bert-wwm',\n",
       "  'downloads': 11648,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vumichien/wav2vec2-large-xlsr-japanese-hiragana': {'modelId': 'vumichien/wav2vec2-large-xlsr-japanese-hiragana',\n",
       "  'sha': '4110cbb24231daf76321af85b829a1baa686d289',\n",
       "  'lastModified': '2021-06-18T11:22:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ja',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'vumichien',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'vumichien/wav2vec2-large-xlsr-japanese-hiragana',\n",
       "  'downloads': 11641,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Japanese Hiragana by Chien Vu',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice Japanese',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'ja'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 24.74},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 10.99}]}]}],\n",
       "  'cardData': {'language': 'ja',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['wer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Japanese Hiragana by Chien Vu',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice Japanese',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'ja'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 24.74},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 10.99}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'allenai/longformer-large-4096-finetuned-triviaqa': {'modelId': 'allenai/longformer-large-4096-finetuned-triviaqa',\n",
       "  'sha': '4a10c0999bd77b29f6fd122663787c770afa197e',\n",
       "  'lastModified': '2021-03-10T02:31:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'longformer',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LongformerForQuestionAnswering'],\n",
       "   'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-large-4096-finetuned-triviaqa',\n",
       "  'downloads': 11598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ine-en': {'modelId': 'Helsinki-NLP/opus-mt-ine-en',\n",
       "  'sha': 'bd46b80a8dfef9dcd1a1201bbe2807bdec97ad9b',\n",
       "  'lastModified': '2020-08-21T14:42:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'es',\n",
       "   'os',\n",
       "   'ro',\n",
       "   'fy',\n",
       "   'cy',\n",
       "   'sc',\n",
       "   'is',\n",
       "   'yi',\n",
       "   'lb',\n",
       "   'an',\n",
       "   'sq',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'rm',\n",
       "   'ps',\n",
       "   'af',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'lt',\n",
       "   'bg',\n",
       "   'be',\n",
       "   'gd',\n",
       "   'si',\n",
       "   'en',\n",
       "   'br',\n",
       "   'mk',\n",
       "   'or',\n",
       "   'mr',\n",
       "   'ru',\n",
       "   'fo',\n",
       "   'co',\n",
       "   'oc',\n",
       "   'pl',\n",
       "   'gl',\n",
       "   'nb',\n",
       "   'bn',\n",
       "   'id',\n",
       "   'hy',\n",
       "   'da',\n",
       "   'gv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'hi',\n",
       "   'as',\n",
       "   'kw',\n",
       "   'ga',\n",
       "   'sv',\n",
       "   'gu',\n",
       "   'wa',\n",
       "   'lv',\n",
       "   'el',\n",
       "   'it',\n",
       "   'hr',\n",
       "   'ur',\n",
       "   'nn',\n",
       "   'de',\n",
       "   'cs',\n",
       "   'ine',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ine-en',\n",
       "  'downloads': 11557,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ca',\n",
       "    'es',\n",
       "    'os',\n",
       "    'ro',\n",
       "    'fy',\n",
       "    'cy',\n",
       "    'sc',\n",
       "    'is',\n",
       "    'yi',\n",
       "    'lb',\n",
       "    'an',\n",
       "    'sq',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'rm',\n",
       "    'ps',\n",
       "    'af',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'lt',\n",
       "    'bg',\n",
       "    'be',\n",
       "    'gd',\n",
       "    'si',\n",
       "    'en',\n",
       "    'br',\n",
       "    'mk',\n",
       "    'or',\n",
       "    'mr',\n",
       "    'ru',\n",
       "    'fo',\n",
       "    'co',\n",
       "    'oc',\n",
       "    'pl',\n",
       "    'gl',\n",
       "    'nb',\n",
       "    'bn',\n",
       "    'id',\n",
       "    'hy',\n",
       "    'da',\n",
       "    'gv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'hi',\n",
       "    'as',\n",
       "    'kw',\n",
       "    'ga',\n",
       "    'sv',\n",
       "    'gu',\n",
       "    'wa',\n",
       "    'lv',\n",
       "    'el',\n",
       "    'it',\n",
       "    'hr',\n",
       "    'ur',\n",
       "    'nn',\n",
       "    'de',\n",
       "    'cs',\n",
       "    'ine'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/electra-large-discriminator-finetuned-conll03-english': {'modelId': 'dbmdz/electra-large-discriminator-finetuned-conll03-english',\n",
       "  'sha': '7c0a483cbc7c8c27759f5fc38fe0261ce6bda31e',\n",
       "  'lastModified': '2020-12-09T18:30:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['ElectraForTokenClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'dbmdz/electra-large-discriminator-finetuned-conll03-english',\n",
       "  'downloads': 11449,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '9c71022cc69f395526255204c7af8bea1cc252d8',\n",
       "  'lastModified': '2022-06-15T20:21:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 11407,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/roberta-large-cola-krishna2020': {'modelId': 'cointegrated/roberta-large-cola-krishna2020',\n",
       "  'sha': '8386814a366a824280df5690a810fe038d7a270b',\n",
       "  'lastModified': '2021-11-11T05:13:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.05700',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cointegrated/roberta-large-cola-krishna2020',\n",
       "  'downloads': 11400,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/codet5-base': {'modelId': 'Salesforce/codet5-base',\n",
       "  'sha': '4078456db09ba972a3532827a0b5df4da172323c',\n",
       "  'lastModified': '2021-11-23T09:53:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:2109.00859',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'codet5',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Salesforce/codet5-base',\n",
       "  'downloads': 11383,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['codet5'],\n",
       "   'datasets': ['code_search_net'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-bat-en': {'modelId': 'Helsinki-NLP/opus-mt-bat-en',\n",
       "  'sha': '760a39c69d5158182c320e47986306e24e96fa1f',\n",
       "  'lastModified': '2021-01-18T07:48:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'bat',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-bat-en',\n",
       "  'downloads': 11353,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['lt', 'lv', 'bat', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/roberta-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/roberta-base-nli-stsb-mean-tokens',\n",
       "  'sha': '903ef0c8897802c3209d82aa46b1c897ac56cf28',\n",
       "  'lastModified': '2022-06-15T20:49:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 11323,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/cpt-large': {'modelId': 'fnlp/cpt-large',\n",
       "  'sha': 'd3f5c8f08d530c93c96b85be27bd79efea80052d',\n",
       "  'lastModified': '2021-10-29T07:11:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'text2text-generation',\n",
       "   'text-classification',\n",
       "   'Summarization',\n",
       "   'Chinese',\n",
       "   'CPT',\n",
       "   'BART',\n",
       "   'BERT',\n",
       "   'seq2seq'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/cpt-large',\n",
       "  'downloads': 11295,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ÊàëÂñúÊ¨¢‰Ω†„ÄÇ ÊàëÁà±‰Ω†'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['fill-mask',\n",
       "    'text2text-generation',\n",
       "    'fill-mask',\n",
       "    'text-classification',\n",
       "    'Summarization',\n",
       "    'Chinese',\n",
       "    'CPT',\n",
       "    'BART',\n",
       "    'BERT',\n",
       "    'seq2seq'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KBLab/bert-base-swedish-cased': {'modelId': 'KBLab/bert-base-swedish-cased',\n",
       "  'sha': '8feda44cdf94516823ee4803c6ef1037aacd224e',\n",
       "  'lastModified': '2021-05-18T21:23:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KBLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KBLab/bert-base-swedish-cased',\n",
       "  'downloads': 11294,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'GanjinZero/UMLSBert_ENG': {'modelId': 'GanjinZero/UMLSBert_ENG',\n",
       "  'sha': '1e4841546c6384cefa47192146a7bd368d509849',\n",
       "  'lastModified': '2022-04-27T08:18:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'biomedical',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'GanjinZero',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'GanjinZero/UMLSBert_ENG',\n",
       "  'downloads': 11260,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'biomedical']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'asafaya/bert-base-arabic': {'modelId': 'asafaya/bert-base-arabic',\n",
       "  'sha': 'b61c328d130eabc1bb1b3b4d1448410a961da888',\n",
       "  'lastModified': '2021-05-19T00:05:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:oscar',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'asafaya',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'asafaya/bert-base-arabic',\n",
       "  'downloads': 11258,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ®ÿßÿ±Ÿäÿ≥ [MASK] ŸÅÿ±ŸÜÿ≥ÿß.'},\n",
       "   {'text': 'ŸÅŸÑÿ≥ŸÅÿ© ÿßŸÑÿ≠Ÿäÿßÿ© ŸáŸä [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar', 'datasets': ['oscar', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hiiamsid/sentence_similarity_spanish_es': {'modelId': 'hiiamsid/sentence_similarity_spanish_es',\n",
       "  'sha': '2817cf8566982a08b43bc4d6f74924010bc56f65',\n",
       "  'lastModified': '2021-10-18T03:52:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'es',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'hiiamsid',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hiiamsid/sentence_similarity_spanish_es',\n",
       "  'downloads': 11243,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'Esa es una persona feliz',\n",
       "    'sentences': ['Ese es un perro feliz',\n",
       "     'Esa es una persona muy feliz',\n",
       "     'Hoy es un d√≠a soleado']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': ['es'],\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/wavlm-large': {'modelId': 'microsoft/wavlm-large',\n",
       "  'sha': 'c1423ed94bb01d80a3f5ce5bc39f6026a0f4828c',\n",
       "  'lastModified': '2022-02-02T21:21:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wavlm',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1912.07875',\n",
       "   'arxiv:2106.06909',\n",
       "   'arxiv:2101.00390',\n",
       "   'arxiv:2110.13900',\n",
       "   'transformers',\n",
       "   'speech'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['WavLMModel'], 'model_type': 'wavlm'},\n",
       "  'id': 'microsoft/wavlm-large',\n",
       "  'downloads': 11207,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['speech'], 'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'iarfmoose/bert-base-cased-qa-evaluator': {'modelId': 'iarfmoose/bert-base-cased-qa-evaluator',\n",
       "  'sha': 'edfb0e29d78453325a95d9a61d4d26d3598e402b',\n",
       "  'lastModified': '2021-05-19T20:15:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'iarfmoose',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'iarfmoose/bert-base-cased-qa-evaluator',\n",
       "  'downloads': 11040,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'epwalsh/bert-xsmall-dummy': {'modelId': 'epwalsh/bert-xsmall-dummy',\n",
       "  'sha': 'd36cc494a54ac76cac8c237866fe8ce540c879a6',\n",
       "  'lastModified': '2021-05-19T16:30:53.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'epwalsh',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'epwalsh/bert-xsmall-dummy',\n",
       "  'downloads': 11027,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-roberta-base': {'modelId': 'sentence-transformers/stsb-roberta-base',\n",
       "  'sha': '20afd582a7bbc12014f37486dbef9b0c990f91bd',\n",
       "  'lastModified': '2022-06-15T20:36:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-base',\n",
       "  'downloads': 10954,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-covid19-base-uncased': {'modelId': 'vinai/bertweet-covid19-base-uncased',\n",
       "  'sha': 'fd00afc23cbc3c3dba662f913d549453f91cb4d4',\n",
       "  'lastModified': '2022-06-08T04:41:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-covid19-base-uncased',\n",
       "  'downloads': 10913,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/t5-base-japanese': {'modelId': 'sonoisa/t5-base-japanese',\n",
       "  'sha': 'c4ef50c4b768cbbb13848677f88989d0020fddc2',\n",
       "  'lastModified': '2021-10-04T01:31:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:oscar',\n",
       "   'dataset:cc100',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'seq2seq',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['T5Model'], 'model_type': 't5'},\n",
       "  'id': 'sonoisa/t5-base-japanese',\n",
       "  'downloads': 10839,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'tags': ['t5', 'text2text-generation', 'seq2seq'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia', 'oscar', 'cc100']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-uk-ru': {'modelId': 'Helsinki-NLP/opus-mt-uk-ru',\n",
       "  'sha': '0dbf7d9872d43f0599f41259b927e012f84b87fe',\n",
       "  'lastModified': '2020-08-21T14:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'uk',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-uk-ru',\n",
       "  'downloads': 10833,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω–µ –∑–≤–∞—Ç–∏ –í–æ–ª—å—Ñ“ë–∞–Ω“ë —ñ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª—ñ–Ω—ñ.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['uk', 'ru'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eugenesiow/bart-paraphrase': {'modelId': 'eugenesiow/bart-paraphrase',\n",
       "  'sha': '561b9d9631d608b8c63c01ecb64b5f030cabdd73',\n",
       "  'lastModified': '2021-09-13T10:02:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:quora',\n",
       "   'dataset:paws',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'paraphrase',\n",
       "   'seq2seq',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'eugenesiow',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'eugenesiow/bart-paraphrase',\n",
       "  'downloads': 10822,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['transformers', 'bart', 'paraphrase', 'seq2seq'],\n",
       "   'datasets': ['quora', 'paws']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k': {'modelId': 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k',\n",
       "  'sha': '98b76c842d1fae9868f74b331b298a92eee3c12e',\n",
       "  'lastModified': '2021-09-23T15:48:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri2Mix',\n",
       "   'dataset:sep_noisy',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k',\n",
       "  'downloads': 10805,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri2Mix', 'sep_noisy'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Salesforce/codet5-small': {'modelId': 'Salesforce/codet5-small',\n",
       "  'sha': 'a642dc934e5475185369d09ac07091dfe72a31fc',\n",
       "  'lastModified': '2021-11-23T09:45:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:2109.00859',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'codet5',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Salesforce/codet5-small',\n",
       "  'downloads': 10803,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['codet5'],\n",
       "   'datasets': ['code_search_net'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k': {'modelId': 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k',\n",
       "  'sha': 'e1ef95ab7a037950f3a606b9a56760cf94701d3d',\n",
       "  'lastModified': '2021-09-23T15:48:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri2Mix',\n",
       "   'dataset:sep_clean',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k',\n",
       "  'downloads': 10757,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri2Mix', 'sep_clean'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-tas-b': {'modelId': 'sentence-transformers/msmarco-distilbert-base-tas-b',\n",
       "  'sha': '1de916fb3ec96493f57c9f0349a0d9e338ed64c2',\n",
       "  'lastModified': '2022-06-15T21:37:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-tas-b',\n",
       "  'downloads': 10652,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/tinyroberta-squad2': {'modelId': 'deepset/tinyroberta-squad2',\n",
       "  'sha': '45149c89ea8692c8441820a3b11deb7adaaa2967',\n",
       "  'lastModified': '2022-05-27T08:43:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'arxiv:1909.10351',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/tinyroberta-squad2',\n",
       "  'downloads': 10560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-roberta-base': {'modelId': 'princeton-nlp/sup-simcse-roberta-base',\n",
       "  'sha': '4bf73c6b5df517f74188c5e9ec159b2208c89c08',\n",
       "  'lastModified': '2021-05-20T19:33:45.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'princeton-nlp/sup-simcse-roberta-base',\n",
       "  'downloads': 10523,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/prophetnet-large-uncased': {'modelId': 'microsoft/prophetnet-large-uncased',\n",
       "  'sha': 'f8218576b32128d7623ad24f3f25dce10f3d1b01',\n",
       "  'lastModified': '2021-03-04T20:24:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'prophetnet',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2001.04063',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['ProphetNetForConditionalGeneration'],\n",
       "   'model_type': 'prophetnet',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'microsoft/prophetnet-large-uncased',\n",
       "  'downloads': 10505,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-2.7b': {'modelId': 'facebook/opt-2.7b',\n",
       "  'sha': 'c9c15109b9dac40871c063892227d45b85cb3952',\n",
       "  'lastModified': '2022-06-22T09:54:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-2.7b',\n",
       "  'downloads': 10435,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-imdb': {'modelId': 'textattack/bert-base-uncased-imdb',\n",
       "  'sha': 'c70b9f391af2067f7eff69a03940218bba9b8d39',\n",
       "  'lastModified': '2021-05-20T07:42:02.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-imdb',\n",
       "  'downloads': 10431,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/sentence-bert-base-ja-mean-tokens': {'modelId': 'sonoisa/sentence-bert-base-ja-mean-tokens',\n",
       "  'sha': 'c5db458007569cea1374d4b5766193832c3fc285',\n",
       "  'lastModified': '2021-12-14T11:43:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'sentence-transformers',\n",
       "   'sentence-bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'sonoisa/sentence-bert-base-ja-mean-tokens',\n",
       "  'downloads': 10417,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'sentence-bert',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sberbank-ai/ruT5-base': {'modelId': 'sberbank-ai/ruT5-base',\n",
       "  'sha': '8940daf014ad31b5b619cb07429dd50c884882f1',\n",
       "  'lastModified': '2021-09-21T19:41:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'sberbank-ai/ruT5-base',\n",
       "  'downloads': 10415,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/model-zoo'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-Neo-2.7B-Horni': {'modelId': 'KoboldAI/GPT-Neo-2.7B-Horni',\n",
       "  'sha': '6024c30cd1984b12fa35c50e1490c73e42cf4823',\n",
       "  'lastModified': '2021-12-30T11:43:31.000Z',\n",
       "  'tags': ['pytorch', 'gpt_neo', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'KoboldAI/GPT-Neo-2.7B-Horni',\n",
       "  'downloads': 10397,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-cnn-6-6': {'modelId': 'sshleifer/distilbart-cnn-6-6',\n",
       "  'sha': 'd2fde4ca965ba893255479612e4b801aa6500029',\n",
       "  'lastModified': '2021-06-14T07:53:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'sshleifer/distilbart-cnn-6-6',\n",
       "  'downloads': 10392,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/T0pp': {'modelId': 'bigscience/T0pp',\n",
       "  'sha': 'cd850304a7a82b39522a4a9b36f55c287ed72995',\n",
       "  'lastModified': '2022-06-21T01:20:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0pp',\n",
       "  'downloads': 10365,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 251,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dandelin/vilt-b32-mlm': {'modelId': 'dandelin/vilt-b32-mlm',\n",
       "  'sha': '7051201a80eb7fd4f78a1a39f0e507a0204517ff',\n",
       "  'lastModified': '2022-01-23T09:42:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vilt',\n",
       "   'fill-mask',\n",
       "   'arxiv:2102.03334',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dandelin',\n",
       "  'config': {'architectures': ['ViltForMaskedLM'], 'model_type': 'vilt'},\n",
       "  'id': 'dandelin/vilt-b32-mlm',\n",
       "  'downloads': 10318,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/bert_uncased_L-4_H-256_A-4': {'modelId': 'google/bert_uncased_L-4_H-256_A-4',\n",
       "  'sha': '387825ce42dbb39b87911cdf8e383ee3b25184f8',\n",
       "  'lastModified': '2021-05-19T17:30:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-4_H-256_A-4',\n",
       "  'downloads': 10286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': 'e614409446a9b8cc7eb7ac1087e11af9e99ab895',\n",
       "  'lastModified': '2022-06-15T20:39:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 10230,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cahya/xlm-roberta-large-indonesian-NER': {'modelId': 'cahya/xlm-roberta-large-indonesian-NER',\n",
       "  'sha': 'd0ef1c27f757b1c21ab299ccfb25fe858ac77ed4',\n",
       "  'lastModified': '2020-09-23T15:55:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'cahya',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cahya/xlm-roberta-large-indonesian-NER',\n",
       "  'downloads': 10188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-french-europeana-cased': {'modelId': 'dbmdz/bert-base-french-europeana-cased',\n",
       "  'sha': 'b895c3cf291f7bf4c15639078a6bee0b3e272c5b',\n",
       "  'lastModified': '2021-09-13T21:03:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'historic french',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-french-europeana-cased',\n",
       "  'downloads': 10120,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'tags': ['historic french']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/roberta-base-squad2-distilled': {'modelId': 'deepset/roberta-base-squad2-distilled',\n",
       "  'sha': '299ce1fd69aa868bb76820746a82b73f4ac4d463',\n",
       "  'lastModified': '2022-01-06T16:19:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-base-squad2-distilled',\n",
       "  'downloads': 10118,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert': {'modelId': 'hf-internal-testing/tiny-bert',\n",
       "  'sha': '8a4db81d7e7ef2296d71ceb206e048c5734ce42f',\n",
       "  'lastModified': '2021-07-08T18:23:09.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {},\n",
       "  'id': 'hf-internal-testing/tiny-bert',\n",
       "  'downloads': 10038,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/stsb-roberta-large': {'modelId': 'sentence-transformers/stsb-roberta-large',\n",
       "  'sha': 'b6cd86898ba049a6d160dab42e298f677c5e63b6',\n",
       "  'lastModified': '2022-06-15T20:28:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-large',\n",
       "  'downloads': 10012,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-xsum-12-6': {'modelId': 'sshleifer/distilbart-xsum-12-6',\n",
       "  'sha': '5b2e376c845c201ddc34ec0e55fd1ad9890ba5ee',\n",
       "  'lastModified': '2021-06-14T07:58:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'sshleifer/distilbart-xsum-12-6',\n",
       "  'downloads': 10004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-128k-cased': {'modelId': 'dbmdz/bert-base-turkish-128k-cased',\n",
       "  'sha': 'ee962e2ecfaafa8cf708fa961f5cbc4346b1c367',\n",
       "  'lastModified': '2021-05-19T15:10:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-128k-cased',\n",
       "  'downloads': 9986,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/wav2vec2-large-it-voxpopuli': {'modelId': 'facebook/wav2vec2-large-it-voxpopuli',\n",
       "  'sha': '06983d0205d75ad2b6ff6b31ef0cff420091ec85',\n",
       "  'lastModified': '2021-07-06T02:18:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'it',\n",
       "   'arxiv:2101.00390',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'automatic-speech-recognition',\n",
       "   'voxpopuli',\n",
       "   'license:cc-by-nc-4.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-it-voxpopuli',\n",
       "  'downloads': 9952,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'voxpopuli'],\n",
       "   'license': 'cc-by-nc-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'textattack/bert-base-uncased-SST-2': {'modelId': 'textattack/bert-base-uncased-SST-2',\n",
       "  'sha': '95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2',\n",
       "  'lastModified': '2021-05-20T07:37:12.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-SST-2',\n",
       "  'downloads': 9790,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-qg-hl': {'modelId': 'valhalla/t5-small-qg-hl',\n",
       "  'sha': '9fdee3255929ba5f0b9d45e76e1e0184f664a368',\n",
       "  'lastModified': '2021-06-23T14:43:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-small-qg-hl',\n",
       "  'downloads': 9781,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '<hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'Python is a programming language. It is developed by <hl> Guido Van Rossum <hl>. </s>'},\n",
       "   {'text': 'Simple is better than <hl> complex <hl>. </s>'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': '<hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'Python is a programming language. It is developed by <hl> Guido Van Rossum <hl>. </s>'},\n",
       "    {'text': 'Simple is better than <hl> complex <hl>. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-hate': {'modelId': 'cardiffnlp/twitter-roberta-base-hate',\n",
       "  'sha': '82cfd645130b125b8e8de63d90b0376620a0bfbd',\n",
       "  'lastModified': '2021-05-20T15:02:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-hate',\n",
       "  'downloads': 9705,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pdelobelle/robbert-v2-dutch-base': {'modelId': 'pdelobelle/robbert-v2-dutch-base',\n",
       "  'sha': 'e28720e1a6cdf68ed3418c67a1964392905a7c8a',\n",
       "  'lastModified': '2022-05-19T20:45:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'nl',\n",
       "   'dataset:oscar',\n",
       "   'dataset:oscar (NL)',\n",
       "   'dataset:dbrd',\n",
       "   'dataset:lassy-ud',\n",
       "   'dataset:europarl-mono',\n",
       "   'dataset:conll2002',\n",
       "   'arxiv:2001.06286',\n",
       "   'arxiv:2004.02814',\n",
       "   'arxiv:2010.13652',\n",
       "   'arxiv:2101.05716',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2001.02943',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'Dutch',\n",
       "   'Flemish',\n",
       "   'RoBERTa',\n",
       "   'RobBERT',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'pdelobelle',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'pdelobelle/robbert-v2-dutch-base',\n",
       "  'downloads': 9696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Hallo, ik ben RobBERT, een <mask> taalmodel van de KU Leuven.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'nl',\n",
       "   'thumbnail': 'https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo.png',\n",
       "   'tags': ['Dutch', 'Flemish', 'RoBERTa', 'RobBERT'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['oscar',\n",
       "    'oscar (NL)',\n",
       "    'dbrd',\n",
       "    'lassy-ud',\n",
       "    'europarl-mono',\n",
       "    'conll2002'],\n",
       "   'widget': [{'text': 'Hallo, ik ben RobBERT, een <mask> taalmodel van de KU Leuven.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-base': {'modelId': 'studio-ousia/luke-base',\n",
       "  'sha': '7438924defd9f3c2018d63c16073bf4bcb6a70aa',\n",
       "  'lastModified': '2022-04-13T08:59:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'luke',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1906.08237',\n",
       "   'arxiv:1903.07785',\n",
       "   'arxiv:2002.01808',\n",
       "   'transformers',\n",
       "   'named entity recognition',\n",
       "   'entity typing',\n",
       "   'relation classification',\n",
       "   'question answering',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForMaskedLM'], 'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-base',\n",
       "  'downloads': 9687,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://github.com/studio-ousia/luke/raw/master/resources/luke_logo.png',\n",
       "   'tags': ['luke',\n",
       "    'named entity recognition',\n",
       "    'entity typing',\n",
       "    'relation classification',\n",
       "    'question answering'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/Multilingual-MiniLM-L12-H384': {'modelId': 'microsoft/Multilingual-MiniLM-L12-H384',\n",
       "  'sha': 'f8a8e5023cbd4f94f1debed2578c65964dd4846b',\n",
       "  'lastModified': '2021-06-01T14:33:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:2002.10957',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1911.02116',\n",
       "   'arxiv:1910.07475',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'microsoft/Multilingual-MiniLM-L12-H384',\n",
       "  'downloads': 9686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/nq-distilbert-base-v1': {'modelId': 'sentence-transformers/nq-distilbert-base-v1',\n",
       "  'sha': 'a3dd10344d84c37c1d4a8be5d5021317900a2d19',\n",
       "  'lastModified': '2022-06-15T21:49:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/nq-distilbert-base-v1',\n",
       "  'downloads': 9682,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-base-discriminator': {'modelId': 'hfl/chinese-electra-180g-base-discriminator',\n",
       "  'sha': '693c1a7e58307777ad4cdf6b80b47c777c028572',\n",
       "  'lastModified': '2021-03-03T01:26:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-base-discriminator',\n",
       "  'downloads': 9601,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/gbert-large': {'modelId': 'deepset/gbert-large',\n",
       "  'sha': 'f6bca479ebb46e62ac99c03282a5030139e302f4',\n",
       "  'lastModified': '2022-02-17T14:05:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OPUS',\n",
       "   'dataset:OpenLegalData',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:2010.10906',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'deepset/gbert-large',\n",
       "  'downloads': 9530,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['wikipedia', 'OPUS', 'OpenLegalData', 'oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'google/bert_uncased_L-4_H-512_A-8': {'modelId': 'google/bert_uncased_L-4_H-512_A-8',\n",
       "  'sha': '606e4d55252882ac25ba1f1d1a182075830f5a90',\n",
       "  'lastModified': '2021-05-19T17:30:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-4_H-512_A-8',\n",
       "  'downloads': 9500,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'NbAiLab/nb-bert-base': {'modelId': 'NbAiLab/nb-bert-base',\n",
       "  'sha': '82b194c0b3ea1fcad65f1eceee04adb26f9f71ac',\n",
       "  'lastModified': '2021-11-26T12:02:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'no',\n",
       "   'transformers',\n",
       "   'norwegian',\n",
       "   'license:cc-by-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'NbAiLab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'NbAiLab/nb-bert-base',\n",
       "  'downloads': 9499,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'P√• biblioteket kan du [MASK] en bok.'},\n",
       "   {'text': 'Dette er et [MASK] eksempel.'},\n",
       "   {'text': 'Av og til kan en spr√•kmodell gi et [MASK] resultat.'},\n",
       "   {'text': 'Som ansat f√•r du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'no',\n",
       "   'license': 'cc-by-4.0',\n",
       "   'tags': ['norwegian', 'bert'],\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': 'P√• biblioteket kan du [MASK] en bok.'},\n",
       "    {'text': 'Dette er et [MASK] eksempel.'},\n",
       "    {'text': 'Av og til kan en spr√•kmodell gi et [MASK] resultat.'},\n",
       "    {'text': 'Som ansat f√•r du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'seyonec/ChemBERTa_zinc250k_v2_40k': {'modelId': 'seyonec/ChemBERTa_zinc250k_v2_40k',\n",
       "  'sha': '460f88413c1b572509b46fbd957a4296ea71e19f',\n",
       "  'lastModified': '2021-05-20T20:57:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'seyonec',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'seyonec/ChemBERTa_zinc250k_v2_40k',\n",
       "  'downloads': 9489,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Nerys': {'modelId': 'KoboldAI/fairseq-dense-13B-Nerys',\n",
       "  'sha': '2dea07a2017f755e1906cf6f0bf16fd13ff3e814',\n",
       "  'lastModified': '2022-06-25T11:22:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Nerys',\n",
       "  'downloads': 9459,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mfeb/albert-xxlarge-v2-squad2': {'modelId': 'mfeb/albert-xxlarge-v2-squad2',\n",
       "  'sha': 'e734d3529f402760605a24af13e02f6f092e96a0',\n",
       "  'lastModified': '2020-04-24T16:10:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mfeb',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'mfeb/albert-xxlarge-v2-squad2',\n",
       "  'downloads': 9452,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csarron/bert-base-uncased-squad-v1': {'modelId': 'csarron/bert-base-uncased-squad-v1',\n",
       "  'sha': '31129bdd485e06a6bec8fd2e045256369db34b0b',\n",
       "  'lastModified': '2021-05-19T14:32:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'bert-base',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'csarron',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'csarron/bert-base-uncased-squad-v1',\n",
       "  'downloads': 9426,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "   {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': None,\n",
       "   'license': 'mit',\n",
       "   'tags': ['question-answering', 'bert', 'bert-base'],\n",
       "   'datasets': ['squad'],\n",
       "   'metrics': ['squad'],\n",
       "   'widget': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "    {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/distilrubert-tiny-cased-conversational': {'modelId': 'DeepPavlov/distilrubert-tiny-cased-conversational',\n",
       "  'sha': 'a70de709fe33d6879bd82337162bdd2ea19442bd',\n",
       "  'lastModified': '2022-06-28T17:10:33.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-tiny-cased-conversational',\n",
       "  'downloads': 9315,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Yanhao/simcse-bert-for-patent': {'modelId': 'Yanhao/simcse-bert-for-patent',\n",
       "  'sha': 'e46b5526806e0a2cc868ea06ffff6e113fb62a20',\n",
       "  'lastModified': '2022-05-04T21:32:00.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'Yanhao',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'Yanhao/simcse-bert-for-patent',\n",
       "  'downloads': 9307,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/trocr-base-printed': {'modelId': 'microsoft/trocr-base-printed',\n",
       "  'sha': '191a64dd5078db39e975a76b798b6fd026a96fa6',\n",
       "  'lastModified': '2022-07-01T07:35:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'arxiv:2109.10282',\n",
       "   'transformers',\n",
       "   'trocr',\n",
       "   'image-to-text'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'microsoft/trocr-base-printed',\n",
       "  'downloads': 9263,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['trocr', 'image-to-text']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/dit-base-finetuned-rvlcdip': {'modelId': 'microsoft/dit-base-finetuned-rvlcdip',\n",
       "  'sha': '27841b0cfbb6d626f7b53c4fd4daaf41a45ea5f5',\n",
       "  'lastModified': '2022-03-08T10:39:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'beit',\n",
       "   'image-classification',\n",
       "   'arxiv:2203.02378',\n",
       "   'transformers',\n",
       "   'dit',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BeitForImageClassification'],\n",
       "   'model_type': 'beit'},\n",
       "  'id': 'microsoft/dit-base-finetuned-rvlcdip',\n",
       "  'downloads': 9234,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/coca_cola_advertisement.png',\n",
       "    'example_title': 'Advertisement'},\n",
       "   {'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/scientific_publication.png',\n",
       "    'example_title': 'Scientific publication'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['dit', 'vision', 'image-classification'],\n",
       "   'widget': [{'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/coca_cola_advertisement.png',\n",
       "     'example_title': 'Advertisement'},\n",
       "    {'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/scientific_publication.png',\n",
       "     'example_title': 'Scientific publication'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'dmis-lab/biobert-large-cased-v1.1-squad': {'modelId': 'dmis-lab/biobert-large-cased-v1.1-squad',\n",
       "  'sha': '2b17f30cda1efcbe0d6ab3b977856c7898f934b1',\n",
       "  'lastModified': '2021-05-19T16:01:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biobert-large-cased-v1.1-squad',\n",
       "  'downloads': 9207,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese-ner': {'modelId': 'ckiplab/bert-base-chinese-ner',\n",
       "  'sha': '50c5afc0a0131e8ab93f54d9ebf9575af04c22d5',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-ner',\n",
       "  'downloads': 9203,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàë‰ΩèÂú®Âä†Â∑û‰ºØÂÖãÂà©„ÄÇ'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Babelscape/rebel-large': {'modelId': 'Babelscape/rebel-large',\n",
       "  'sha': 'd24237e8ab9c1ad2cbdf53fd54b0d7cda1da8018',\n",
       "  'lastModified': '2022-05-27T10:20:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:Babelscape/rebel-dataset',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'relation-extraction',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Babelscape',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'relation_extraction': {'length_penalty': 0,\n",
       "     'max_length': 256,\n",
       "     'min_length': 12,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'Babelscape/rebel-large',\n",
       "  'downloads': 9200,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic'}],\n",
       "  'likes': 19,\n",
       "  'model-index': [{'name': 'REBEL',\n",
       "    'results': [{'task': {'name': 'Relation Extraction',\n",
       "       'type': 'Relation-Extraction'},\n",
       "      'dataset': {'name': 'CoNLL04', 'type': 'CoNLL04'},\n",
       "      'metrics': [{'name': 'RE+ Macro F1',\n",
       "        'type': 're+ macro f1',\n",
       "        'value': 76.65}]},\n",
       "     {'task': {'name': 'Relation Extraction', 'type': 'Relation-Extraction'},\n",
       "      'dataset': {'name': 'NYT', 'type': 'NYT'},\n",
       "      'metrics': [{'name': 'F1', 'type': 'f1', 'value': 93.4}]}]}],\n",
       "  'cardData': {'language': ['en'],\n",
       "   'widget': [{'text': 'Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic'}],\n",
       "   'tags': ['seq2seq', 'relation-extraction'],\n",
       "   'datasets': ['Babelscape/rebel-dataset'],\n",
       "   'model-index': [{'name': 'REBEL',\n",
       "     'results': [{'task': {'name': 'Relation Extraction',\n",
       "        'type': 'Relation-Extraction'},\n",
       "       'dataset': {'name': 'CoNLL04', 'type': 'CoNLL04'},\n",
       "       'metrics': [{'name': 'RE+ Macro F1',\n",
       "         'type': 're+ macro f1',\n",
       "         'value': 76.65}]},\n",
       "      {'task': {'name': 'Relation Extraction', 'type': 'Relation-Extraction'},\n",
       "       'dataset': {'name': 'NYT', 'type': 'NYT'},\n",
       "       'metrics': [{'name': 'F1', 'type': 'f1', 'value': 93.4}]}]}],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ALINEAR/albert-japanese-v2': {'modelId': 'ALINEAR/albert-japanese-v2',\n",
       "  'sha': '102cec3c2b7bc8483cd9281b6a029279861df66d',\n",
       "  'lastModified': '2020-05-04T13:20:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ALINEAR',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ALINEAR/albert-japanese-v2',\n",
       "  'downloads': 9152,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking': {'modelId': 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking',\n",
       "  'sha': 'f39736041df2a9460ef1525cc9052c3fa39bebc2',\n",
       "  'lastModified': '2022-06-15T22:01:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking',\n",
       "  'downloads': 9132,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Hate-speech-CNERG/bert-base-uncased-hatexplain': {'modelId': 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n",
       "  'sha': 'e487c81b768c7532bf474bd5e486dedea4cf3848',\n",
       "  'lastModified': '2021-05-25T09:53:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:hatexplain',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Hate-speech-CNERG',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n",
       "  'downloads': 9026,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['hatexplain']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-hate-speech': {'modelId': 'pysentimiento/robertuito-hate-speech',\n",
       "  'sha': '272493f45c85fd9b6590716d0206443f2ce79731',\n",
       "  'lastModified': '2021-12-02T21:50:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'twitter',\n",
       "   'hate-speech'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-hate-speech',\n",
       "  'downloads': 8825,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['twitter', 'hate-speech']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'papluca/xlm-roberta-base-language-detection': {'modelId': 'papluca/xlm-roberta-base-language-detection',\n",
       "  'sha': 'f793746a8fff7a83f266fa0df91064727c8c76a2',\n",
       "  'lastModified': '2021-11-25T12:41:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'papluca',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'papluca/xlm-roberta-base-language-detection',\n",
       "  'downloads': 8709,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "    'results': []}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'metrics': ['accuracy', 'f1'],\n",
       "   'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "     'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dlb/electra-base-portuguese-uncased-brwac': {'modelId': 'dlb/electra-base-portuguese-uncased-brwac',\n",
       "  'sha': '60cd1eb5aa58c9468200271ae1b4f55cd1ee2036',\n",
       "  'lastModified': '2021-12-10T12:33:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pt',\n",
       "   'dataset:brwac',\n",
       "   'transformers',\n",
       "   'electra',\n",
       "   'pretraining'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dlb',\n",
       "  'config': {},\n",
       "  'id': 'dlb/electra-base-portuguese-uncased-brwac',\n",
       "  'downloads': 8689,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'tags': ['electra', 'pretraining', 'pytorch'],\n",
       "   'datasets': ['brwac']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'DeepPavlov/distilrubert-tiny-cased-conversational-v1': {'modelId': 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
       "  'sha': '2033d0d1de807e8181ebfa0e53d2a8e526412b0f',\n",
       "  'lastModified': '2022-05-06T11:57:05.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
       "  'downloads': 8686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ArthurZ/flax-tiny-random-bert-sharded': {'modelId': 'ArthurZ/flax-tiny-random-bert-sharded',\n",
       "  'sha': '52b09e4c2b7081b8027dc2d05c14f79c90bd501f',\n",
       "  'lastModified': '2022-06-17T16:08:48.000Z',\n",
       "  'tags': ['jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'ArthurZ',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'ArthurZ/flax-tiny-random-bert-sharded',\n",
       "  'downloads': 8684,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-fa-base-uncased': {'modelId': 'HooshvareLab/bert-fa-base-uncased',\n",
       "  'sha': 'a04aa40c97bcdde570ae11986a534542c2995a62',\n",
       "  'lastModified': '2021-05-18T21:02:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'bert-fa',\n",
       "   'bert-persian',\n",
       "   'persian-lm',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-fa-base-uncased',\n",
       "  'downloads': 8662,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ≤ŸÜÿØ⁄Ø€å €å⁄© ÿ≥ŸàÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿß€åŸÜ ⁄©Ÿá ⁄Ü⁄ØŸàŸÜŸá [MASK] ⁄©ŸÜ€åŸÖ Ÿæÿßÿ≥ÿÆ ÿß€åŸÜ ÿ≥ŸàÿßŸÑ!'},\n",
       "   {'text': 'ÿ≤ŸÜÿØ⁄Ø€å ÿßÿ≤ ŸÖÿ±⁄Ø Ÿæÿ±ÿ≥€åÿØ: ⁄Üÿ±ÿß ŸáŸÖŸá ŸÖŸÜ ÿ±ÿß [MASK] ÿØÿßÿ±ŸÜÿØ ÿßŸÖÿß ÿßÿ≤ ÿ™Ÿà ŸÖÿ™ŸÜŸÅÿ±ŸÜÿØÿü'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa',\n",
       "   'tags': ['bert-fa', 'bert-persian', 'persian-lm'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese-pos': {'modelId': 'ckiplab/bert-base-chinese-pos',\n",
       "  'sha': 'c3f173670d4793f00ce5d23381cbeffa17e4e197',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-pos',\n",
       "  'downloads': 8629,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàë‰ΩèÂú®Âä†Â∑û‰ºØÂÖãÂà©„ÄÇ'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jhgan/ko-sroberta-multitask': {'modelId': 'jhgan/ko-sroberta-multitask',\n",
       "  'sha': 'ab957ae6a91e99c4cad36d52063a2a9cf1bf4419',\n",
       "  'lastModified': '2022-06-13T16:34:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'jhgan',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'jhgan/ko-sroberta-multitask',\n",
       "  'downloads': 8578,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'language': 'ko'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kssteven/ibert-roberta-base': {'modelId': 'kssteven/ibert-roberta-base',\n",
       "  'sha': '4f98e9110b04a8958444d3af8ed39287834fbb90',\n",
       "  'lastModified': '2021-11-22T10:09:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'ibert',\n",
       "   'fill-mask',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2101.01321',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'kssteven',\n",
       "  'config': {'architectures': ['IBertForMaskedLM'], 'model_type': 'ibert'},\n",
       "  'id': 'kssteven/ibert-roberta-base',\n",
       "  'downloads': 8550,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/infoxlm-base': {'modelId': 'microsoft/infoxlm-base',\n",
       "  'sha': 'c67f260d5635cdeef35864fd2ce369d24eca1b34',\n",
       "  'lastModified': '2021-08-04T11:42:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2007.07834',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'microsoft/infoxlm-base',\n",
       "  'downloads': 8539,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TODBERT/TOD-BERT-JNT-V1': {'modelId': 'TODBERT/TOD-BERT-JNT-V1',\n",
       "  'sha': '903797e92f97b5e61a1142636b2d604682a1032c',\n",
       "  'lastModified': '2021-05-18T22:44:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'TODBERT',\n",
       "  'config': {'architectures': ['BertForMaskedLM'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'TODBERT/TOD-BERT-JNT-V1',\n",
       "  'downloads': 8532,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'M-CLIP/M-BERT-Distil-40': {'modelId': 'M-CLIP/M-BERT-Distil-40',\n",
       "  'sha': 'ff20c09c1a088589cb65a169d165b5ddcbe792ca',\n",
       "  'lastModified': '2021-03-21T15:39:15.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'M-CLIP',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'M-CLIP/M-BERT-Distil-40',\n",
       "  'downloads': 8527,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-russian': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n",
       "  'sha': 'ce7dbfba0ef183421eded7358a6643f030f8c4cb',\n",
       "  'lastModified': '2022-06-22T17:05:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ru',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n",
       "  'downloads': 8518,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Russian by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice ru',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'ru'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 13.3},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 2.88},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 9.57},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.24}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'ru'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 40.22},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 14.8},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 33.61},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 13.5}]}]}],\n",
       "  'cardData': {'language': 'ru',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'ru',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Russian by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice ru',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'ru'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 13.3},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 2.88},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 9.57},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.24}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'ru'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 40.22},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 14.8},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 33.61},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 13.5}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'klue/roberta-base': {'modelId': 'klue/roberta-base',\n",
       "  'sha': '67dd433d36ebc66a42c9aaa85abcf8d2620e41d9',\n",
       "  'lastModified': '2021-10-20T16:10:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'klue/roberta-base',\n",
       "  'downloads': 8517,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert/camembert-large': {'modelId': 'camembert/camembert-large',\n",
       "  'sha': 'df7dbf53dd70551faa6b4ec45deb4a566445c7cc',\n",
       "  'lastModified': '2020-12-11T21:35:25.000Z',\n",
       "  'tags': ['pytorch', 'camembert', 'fr', 'arxiv:1911.03894', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'camembert',\n",
       "  'config': {'model_type': 'camembert'},\n",
       "  'id': 'camembert/camembert-large',\n",
       "  'downloads': 8484,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ai4bharat/indic-bert': {'modelId': 'ai4bharat/indic-bert',\n",
       "  'sha': '97ae2d6440dbd1a2698540223dc00b43075c69c9',\n",
       "  'lastModified': '2021-04-12T09:06:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'en',\n",
       "   'dataset:AI4Bharat IndicNLP Corpora',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'ai4bharat',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'ai4bharat/indic-bert',\n",
       "  'downloads': 8483,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['AI4Bharat IndicNLP Corpora']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'iarfmoose/t5-base-question-generator': {'modelId': 'iarfmoose/t5-base-question-generator',\n",
       "  'sha': '1bfc9d4b2b0078e0b65cf40c6e2e2e974fbab6b0',\n",
       "  'lastModified': '2022-02-24T08:41:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'iarfmoose',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'iarfmoose/t5-base-question-generator',\n",
       "  'downloads': 8483,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-128k-uncased': {'modelId': 'dbmdz/bert-base-turkish-128k-uncased',\n",
       "  'sha': 'f5287aecee60f0c597c11c34341cb92d31c0e71b',\n",
       "  'lastModified': '2021-05-19T15:13:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-128k-uncased',\n",
       "  'downloads': 8391,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sberbank-ai/mGPT': {'modelId': 'sberbank-ai/mGPT',\n",
       "  'sha': '9f49a85776d5ec166120ea81719987fe0f643574',\n",
       "  'lastModified': '2022-04-21T18:06:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'az',\n",
       "   'sw',\n",
       "   'af',\n",
       "   'ar',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bxr',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'cv',\n",
       "   'hy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'es',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hu',\n",
       "   'kk',\n",
       "   'id',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'ka',\n",
       "   'ky',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mn',\n",
       "   'ml',\n",
       "   'os',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'sah',\n",
       "   'ru',\n",
       "   'tg',\n",
       "   'sv',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tk',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'tl',\n",
       "   'tt',\n",
       "   'tyv',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'vi',\n",
       "   'uz',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'xal',\n",
       "   'dataset:mc4',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2112.10668',\n",
       "   'arxiv:2204.07580',\n",
       "   'transformers',\n",
       "   'multilingual',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'gpt3',\n",
       "   'Deepspeed',\n",
       "   'Megatron',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/mGPT',\n",
       "  'downloads': 8362,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 53,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'language': ['en',\n",
       "    'az',\n",
       "    'sw',\n",
       "    'af',\n",
       "    'ar',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bxr',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'cv',\n",
       "    'hy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'es',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hu',\n",
       "    'kk',\n",
       "    'id',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'ka',\n",
       "    'ky',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mn',\n",
       "    'ml',\n",
       "    'os',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'sah',\n",
       "    'ru',\n",
       "    'tg',\n",
       "    'sv',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tk',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'tl',\n",
       "    'tt',\n",
       "    'tyv',\n",
       "    'uk',\n",
       "    'en',\n",
       "    'ur',\n",
       "    'vi',\n",
       "    'uz',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'xal'],\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'tags': ['multilingual',\n",
       "    'PyTorch',\n",
       "    'Transformers',\n",
       "    'gpt3',\n",
       "    'gpt2',\n",
       "    'Deepspeed',\n",
       "    'Megatron'],\n",
       "   'datasets': ['mc4', 'wikipedia'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/mgpt'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large': {'modelId': 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large',\n",
       "  'sha': '160deb78aca30f63754e512a93337ce8013a32ca',\n",
       "  'lastModified': '2021-06-20T19:03:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large',\n",
       "  'downloads': 8355,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200': {'modelId': 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200',\n",
       "  'sha': 'e2568c3f7868d8aa3aaabcf28fa100d10d54c170',\n",
       "  'lastModified': '2022-01-29T06:03:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'hi',\n",
       "   'arxiv:2107.07402',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'Harveenchadha',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200',\n",
       "  'downloads': 8339,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': [{'name': 'Wav2Vec2 Vakyansh Hindi Model by Harveen Chadha',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice hi',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'hi'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 33.17}]}]}],\n",
       "  'cardData': {'language': 'hi',\n",
       "   'metrics': ['wer'],\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'speech'],\n",
       "   'license': 'mit',\n",
       "   'model-index': [{'name': 'Wav2Vec2 Vakyansh Hindi Model by Harveen Chadha',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice hi',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'hi'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 33.17}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'nghuyong/ernie-1.0': {'modelId': 'nghuyong/ernie-1.0',\n",
       "  'sha': 'b06176bf30ecf544330ab008933c9ac1012f1a6d',\n",
       "  'lastModified': '2021-05-20T01:40:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'zh',\n",
       "   'arxiv:1904.09223',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-1.0',\n",
       "  'downloads': 8337,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli': {'modelId': 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'sha': '5b605abab9b75bc87ab66cfc049ef58d9d64b8ed',\n",
       "  'lastModified': '2021-05-20T23:17:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'dataset:snli',\n",
       "   'dataset:anli',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:multi_nli_mismatch',\n",
       "   'dataset:fever',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ynie',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'downloads': 8331,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['snli',\n",
       "    'anli',\n",
       "    'multi_nli',\n",
       "    'multi_nli_mismatch',\n",
       "    'fever'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kamalkraj/deberta-base': {'modelId': 'kamalkraj/deberta-base',\n",
       "  'sha': 'e6f7f722f429327e65a625ccd1f1ab9b1bfa9abc',\n",
       "  'lastModified': '2021-08-08T09:12:57.000Z',\n",
       "  'tags': ['tf',\n",
       "   'deberta',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'kamalkraj',\n",
       "  'config': {'architectures': ['DebertaModel'], 'model_type': 'deberta'},\n",
       "  'id': 'kamalkraj/deberta-base',\n",
       "  'downloads': 8320,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-small-distilled-patch16-224': {'modelId': 'facebook/deit-small-distilled-patch16-224',\n",
       "  'sha': '167101edd3b696f0030f70bfc752bc6408a0153d',\n",
       "  'lastModified': '2022-03-18T14:38:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DeiTForImageClassificationWithTeacher'],\n",
       "   'model_type': 'deit'},\n",
       "  'id': 'facebook/deit-small-distilled-patch16-224',\n",
       "  'downloads': 8263,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking': {'modelId': 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       "  'sha': 'fa5374ac8a5b2c6d3f5f8e156a9892cdf687201d',\n",
       "  'lastModified': '2021-09-23T13:45:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       "  'downloads': 8164,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰ªôÂè∞„ÅØ„Äå[MASK]„ÅÆÈÉΩ„Äç„Å®Âëº„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '‰ªôÂè∞„ÅØ„Äå[MASK]„ÅÆÈÉΩ„Äç„Å®Âëº„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-6.7b': {'modelId': 'facebook/opt-6.7b',\n",
       "  'sha': '8dc17cdd7b9381612e631064e569f4142d776d88',\n",
       "  'lastModified': '2022-06-24T05:22:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-6.7b',\n",
       "  'downloads': 8150,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/tapex-base': {'modelId': 'microsoft/tapex-base',\n",
       "  'sha': '968109c940c8b270a3eaec1532d596ba6c923b6a',\n",
       "  'lastModified': '2022-05-17T08:25:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2107.07653',\n",
       "   'transformers',\n",
       "   'tapex',\n",
       "   'table-question-answering',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'table-question-answering',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'microsoft/tapex-base',\n",
       "  'downloads': 8143,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'How many stars does the transformers repository have?',\n",
       "    'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'],\n",
       "     'Stars': [36542, 4512, 3934],\n",
       "     'Contributors': [651, 77, 34],\n",
       "     'Programming language': ['Python',\n",
       "      'Python',\n",
       "      'Rust, Python and NodeJS']}}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapex', 'table-question-answering'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-uncased-eurlex': {'modelId': 'nlpaueb/bert-base-uncased-eurlex',\n",
       "  'sha': '77adac5a840314c4d688f565a925db1bdd2e87e7',\n",
       "  'lastModified': '2022-04-28T14:44:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-uncased-eurlex',\n",
       "  'downloads': 8008,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'vblagoje/bart_lfqa': {'modelId': 'vblagoje/bart_lfqa',\n",
       "  'sha': '5493d5be6812cdb4835e004ce17ea2082cc25b03',\n",
       "  'lastModified': '2022-02-14T15:54:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:vblagoje/lfqa',\n",
       "   'dataset:vblagoje/lfqa_support_docs',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'vblagoje',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'vblagoje/bart_lfqa',\n",
       "  'downloads': 7991,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['vblagoje/lfqa', 'vblagoje/lfqa_support_docs'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-many-to-one-mmt': {'modelId': 'facebook/mbart-large-50-many-to-one-mmt',\n",
       "  'sha': 'aadfc3b9db11b773f823ad936d5d683c470e7683',\n",
       "  'lastModified': '2022-05-26T22:28:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart',\n",
       "   'task_specific_params': {'translation_en_to_ro': {'decoder_start_token_id': 250020}}},\n",
       "  'id': 'facebook/mbart-large-50-many-to-one-mmt',\n",
       "  'downloads': 7960,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ahotrod/electra_large_discriminator_squad2_512': {'modelId': 'ahotrod/electra_large_discriminator_squad2_512',\n",
       "  'sha': 'fe230d41e74248eceb366a91e4f8572f358f201c',\n",
       "  'lastModified': '2020-12-11T21:31:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ahotrod',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'ahotrod/electra_large_discriminator_squad2_512',\n",
       "  'downloads': 7903,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ctl/wav2vec2-large-xlsr-cantonese': {'modelId': 'ctl/wav2vec2-large-xlsr-cantonese',\n",
       "  'sha': '6a6119ab39ec2a0c8d16edfbf91db45334540315',\n",
       "  'lastModified': '2021-07-06T01:16:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh-HK',\n",
       "   'yue',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'ctl',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'ctl/wav2vec2-large-xlsr-cantonese',\n",
       "  'downloads': 7876,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'wav2vec2-large-xlsr-cantonese',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-HK',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-HK'},\n",
       "      'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 15.36}]}]}],\n",
       "  'cardData': {'language': ['zh-HK', 'yue'],\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'wav2vec2-large-xlsr-cantonese',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-HK',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-HK'},\n",
       "       'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 15.36}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'sshleifer/tiny-marian-en-de': {'modelId': 'sshleifer/tiny-marian-en-de',\n",
       "  'sha': '7a6b5b34785930445aeb20a9a34543b72de6e267',\n",
       "  'lastModified': '2020-06-25T02:27:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'sshleifer/tiny-marian-en-de',\n",
       "  'downloads': 7872,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-1B-distill': {'modelId': 'facebook/blenderbot-1B-distill',\n",
       "  'sha': 'a10c0a628734b4cd46da57520bd35c7ca8965201',\n",
       "  'lastModified': '2021-06-17T12:02:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-1B-distill',\n",
       "  'downloads': 7829,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-sequence-nq': {'modelId': 'facebook/rag-sequence-nq',\n",
       "  'sha': 'c0d9c6ceda8a69c78091abb7aa734a97b75b89fd',\n",
       "  'lastModified': '2021-03-12T11:04:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagSequenceForGeneration'],\n",
       "   'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-sequence-nq',\n",
       "  'downloads': 7783,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagSequenceForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Grossmend/rudialogpt3_medium_based_on_gpt2': {'modelId': 'Grossmend/rudialogpt3_medium_based_on_gpt2',\n",
       "  'sha': 'a2f8ac89182e36e352ea921de30cf2b0e9b30b89',\n",
       "  'lastModified': '2021-08-02T13:43:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Grossmend',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'Grossmend/rudialogpt3_medium_based_on_gpt2',\n",
       "  'downloads': 7779,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-xlm-r-multilingual': {'modelId': 'sentence-transformers/stsb-xlm-r-multilingual',\n",
       "  'sha': 'bc1a68705f2e397259207e96349a36ccbc7e6493',\n",
       "  'lastModified': '2022-06-15T21:42:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/stsb-xlm-r-multilingual',\n",
       "  'downloads': 7631,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-90M': {'modelId': 'facebook/blenderbot-90M',\n",
       "  'sha': '3e5344952a74d2017762fa8428c45edd07f3dea7',\n",
       "  'lastModified': '2021-03-12T06:17:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot-small',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotSmallForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot-small'},\n",
       "  'id': 'facebook/blenderbot-90M',\n",
       "  'downloads': 7576,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mt5-xxl': {'modelId': 'google/mt5-xxl',\n",
       "  'sha': 'd4ac5e6d5125f8d30cba8763cd0ad71e5d34c17b',\n",
       "  'lastModified': '2022-05-27T15:06:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-xxl',\n",
       "  'downloads': 7571,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kamalkraj/deberta-v2-xlarge': {'modelId': 'kamalkraj/deberta-v2-xlarge',\n",
       "  'sha': 'ee3a2dc756e780f6bfeb1e251d84bc7d7dc2da9c',\n",
       "  'lastModified': '2021-08-13T08:44:43.000Z',\n",
       "  'tags': ['tf',\n",
       "   'deberta-v2',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'kamalkraj',\n",
       "  'config': {'architectures': ['DebertaV2Model'], 'model_type': 'deberta-v2'},\n",
       "  'id': 'kamalkraj/deberta-v2-xlarge',\n",
       "  'downloads': 7540,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialogRPT-human-vs-rand': {'modelId': 'microsoft/DialogRPT-human-vs-rand',\n",
       "  'sha': '7206b425c2c016dd5533e2a99e665ba3546e5ce0',\n",
       "  'lastModified': '2021-05-23T09:18:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-classification',\n",
       "   'arxiv:2009.06978',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2ForSequenceClassification'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'microsoft/DialogRPT-human-vs-rand',\n",
       "  'downloads': 7522,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'UBC-NLP/MARBERT': {'modelId': 'UBC-NLP/MARBERT',\n",
       "  'sha': 'ef5bf8d54e104731fc045d5c76e72af8a23988cf',\n",
       "  'lastModified': '2022-01-19T20:37:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'transformers',\n",
       "   'Arabic BERT',\n",
       "   'MSA',\n",
       "   'Twitter',\n",
       "   'Masked Langauge Model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'UBC-NLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'UBC-NLP/MARBERT',\n",
       "  'downloads': 7506,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸáŸä ŸÑÿ∫ÿ© [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'tags': ['Arabic BERT', 'MSA', 'Twitter', 'Masked Langauge Model'],\n",
       "   'widget': [{'text': 'ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸáŸä ŸÑÿ∫ÿ© [MASK].'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-xl-lm-adapt': {'modelId': 'google/t5-xl-lm-adapt',\n",
       "  'sha': '6e644c517fc8a8a79a657e528be5c60777d87652',\n",
       "  'lastModified': '2021-11-01T13:59:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-xl-lm-adapt',\n",
       "  'downloads': 7412,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-xls-r-300m': {'modelId': 'facebook/wav2vec2-xls-r-300m',\n",
       "  'sha': 'e842f378fdbdb09aabc11d87c52f26b8f2dde333',\n",
       "  'lastModified': '2021-11-18T16:32:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'multilingual',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:multilingual_librispeech',\n",
       "   'arxiv:2111.09296',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'xls_r',\n",
       "   'xls_r_pretrained',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-xls-r-300m',\n",
       "  'downloads': 7341,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'datasets': ['common_voice', 'multilingual_librispeech'],\n",
       "   'tags': ['speech', 'xls_r', 'xls_r_pretrained'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'julien-c/dummy-diff-tokenizer': {'modelId': 'julien-c/dummy-diff-tokenizer',\n",
       "  'sha': '8b54c50bfd24739488683452f24d4471f5d75a21',\n",
       "  'lastModified': '2021-05-20T17:30:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'julien-c/dummy-diff-tokenizer',\n",
       "  'downloads': 7259,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-J-6B-Skein': {'modelId': 'KoboldAI/GPT-J-6B-Skein',\n",
       "  'sha': '95a7ea75328cc8e117fdbf967b9fa12f49d1d24c',\n",
       "  'lastModified': '2022-03-14T22:44:49.000Z',\n",
       "  'tags': ['pytorch', 'gptj', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'temperature': 1,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'KoboldAI/GPT-J-6B-Skein',\n",
       "  'downloads': 7252,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/vit-mae-base': {'modelId': 'facebook/vit-mae-base',\n",
       "  'sha': '87dd4faac12498cde93a176406329112584c0413',\n",
       "  'lastModified': '2022-03-29T16:18:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'vit_mae',\n",
       "   'pretraining',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2111.06377',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ViTMAEForPreTraining'],\n",
       "   'model_type': 'vit_mae'},\n",
       "  'id': 'facebook/vit-mae-base',\n",
       "  'downloads': 7214,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
       "  'sha': '5891ca7549f9dfe2ddae9a3c6cc915f17b214502',\n",
       "  'lastModified': '2022-06-22T17:00:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh-CN',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
       "  'downloads': 7209,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Chinese (zh-CN) by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-CN',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-CN'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 82.37},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 19.03}]}]}],\n",
       "  'cardData': {'language': 'zh-CN',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Chinese (zh-CN) by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-CN',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-CN'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 82.37},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 19.03}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity': {'modelId': 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity',\n",
       "  'sha': '8ed6c66504212201bd8f542a2467741baef8a133',\n",
       "  'lastModified': '2022-05-12T09:49:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'NLU',\n",
       "   'NLI',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'IDEA-CCNL',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity',\n",
       "  'downloads': 7200,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰ªäÂ§©ÂøÉÊÉÖ‰∏çÂ•Ω[SEP]‰ªäÂ§©ÂæàÂºÄÂøÉ'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'NLU', 'NLI'],\n",
       "   'inference': True,\n",
       "   'widget': [{'text': '‰ªäÂ§©ÂøÉÊÉÖ‰∏çÂ•Ω[SEP]‰ªäÂ§©ÂæàÂºÄÂøÉ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ur-en': {'modelId': 'Helsinki-NLP/opus-mt-ur-en',\n",
       "  'sha': 'c803d32b6f7a3a7a8cb1ba91d2947de0009f8cdc',\n",
       "  'lastModified': '2020-08-21T14:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ur',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ur-en',\n",
       "  'downloads': 7169,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ur', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv02-twitter': {'modelId': 'aubmindlab/bert-base-arabertv02-twitter',\n",
       "  'sha': '14bddd56ee5b02d1d92436ca14934687452a96ea',\n",
       "  'lastModified': '2021-10-16T22:10:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'dataset:Twitter',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv02-twitter',\n",
       "  'downloads': 7158,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' ÿπÿßÿµŸÖÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled',\n",
       "    'Twitter'],\n",
       "   'widget': [{'text': ' ÿπÿßÿµŸÖÿ© ŸÑÿ®ŸÜÿßŸÜ ŸáŸä [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Geotrend/bert-base-th-cased': {'modelId': 'Geotrend/bert-base-th-cased',\n",
       "  'sha': '607c7b08e70c5ec7b2e3e013f394f0743dd39ca3',\n",
       "  'lastModified': '2021-05-18T20:11:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'th',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Geotrend',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Geotrend/bert-base-th-cased',\n",
       "  'downloads': 7145,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'th',\n",
       "   'datasets': 'wikipedia',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-german': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
       "  'sha': '1e8033537f45651154ca94b1295817ffd11d5942',\n",
       "  'lastModified': '2022-06-22T17:05:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'de',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
       "  'downloads': 7139,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 German by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice de',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'de'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 12.06},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 2.92},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 8.74},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.28}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'de'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 32.75},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 13.64},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 26.6},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 12.58}]}]}],\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'de',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 German by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice de',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'de'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 12.06},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 2.92},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 8.74},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.28}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'de'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 32.75},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 13.64},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 26.6},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 12.58}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'unicamp-dl/translation-en-pt-t5': {'modelId': 'unicamp-dl/translation-en-pt-t5',\n",
       "  'sha': '8418d7e9b1837687137af06624cb3596b45c9343',\n",
       "  'lastModified': '2021-10-11T03:47:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'pt',\n",
       "   'dataset:EMEA',\n",
       "   'dataset:ParaCrawl 99k',\n",
       "   'dataset:CAPES',\n",
       "   'dataset:Scielo',\n",
       "   'dataset:JRC-Acquis',\n",
       "   'dataset:Biomedical Domain Corpora',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'unicamp-dl',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'unicamp-dl/translation-en-pt-t5',\n",
       "  'downloads': 7132,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'pt'],\n",
       "   'datasets': ['EMEA',\n",
       "    'ParaCrawl 99k',\n",
       "    'CAPES',\n",
       "    'Scielo',\n",
       "    'JRC-Acquis',\n",
       "    'Biomedical Domain Corpora'],\n",
       "   'tags': ['translation'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large': {'modelId': 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large',\n",
       "  'sha': 'd828558d1a570cbbb5e62a8dbf85c8f18bf7982a',\n",
       "  'lastModified': '2021-06-20T19:03:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large',\n",
       "  'downloads': 7119,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'charsiu/g2p_multilingual_byT5_small': {'modelId': 'charsiu/g2p_multilingual_byT5_small',\n",
       "  'sha': '834df67c125a811e1a60fbf9f0f39503115437ea',\n",
       "  'lastModified': '2022-05-19T05:02:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'charsiu',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'charsiu/g2p_multilingual_byT5_small',\n",
       "  'downloads': 7070,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-large-cased': {'modelId': 'allegro/herbert-large-cased',\n",
       "  'sha': '8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90',\n",
       "  'lastModified': '2022-06-26T14:18:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'pl',\n",
       "   'transformers',\n",
       "   'herbert',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allegro/herbert-large-cased',\n",
       "  'downloads': 7042,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl', 'tags': ['herbert'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-it': {'modelId': 'Helsinki-NLP/opus-mt-en-it',\n",
       "  'sha': '4c56f4ddc9fcfccec7799f5cef4d90f7c99dd658',\n",
       "  'lastModified': '2021-09-09T21:36:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-it',\n",
       "  'downloads': 7040,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/data2vec-text-base': {'modelId': 'facebook/data2vec-text-base',\n",
       "  'sha': 'bd0db19c3500ee7a0b626791db67fa6e9fda9a0b',\n",
       "  'lastModified': '2022-04-18T16:03:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'data2vec-text',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2202.03555',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Data2VecTextModel'],\n",
       "   'model_type': 'data2vec-text'},\n",
       "  'id': 'facebook/data2vec-text-base',\n",
       "  'downloads': 7037,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'lidiya/bart-large-xsum-samsum': {'modelId': 'lidiya/bart-large-xsum-samsum',\n",
       "  'sha': 'ea93258a6600f79d851b306864224595192edd51',\n",
       "  'lastModified': '2021-05-26T22:18:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'lidiya',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'lidiya/bart-large-xsum-samsum',\n",
       "  'downloads': 7029,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him üôÇ\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n\"}],\n",
       "  'likes': 8,\n",
       "  'model-index': [{'name': 'bart-large-xsum-samsum',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "       'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rogue-1',\n",
       "        'value': 54.3921},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 29.8078},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 45.1543},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 53.3059},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 28.355},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 44.0953}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bart', 'seq2seq', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him üôÇ\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n\"}],\n",
       "   'model-index': [{'name': 'bart-large-xsum-samsum',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "        'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rogue-1',\n",
       "         'value': 54.3921},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 29.8078},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 45.1543},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 53.3059},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 28.355},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 44.0953}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dkleczek/bert-base-polish-cased-v1': {'modelId': 'dkleczek/bert-base-polish-cased-v1',\n",
       "  'sha': 'fed744e81ebd16cf099b5c64c40688bc3e6ace67',\n",
       "  'lastModified': '2021-05-19T15:54:20.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'pretraining', 'pl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dkleczek',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'dkleczek/bert-base-polish-cased-v1',\n",
       "  'downloads': 7021,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/kldarek/polbert/master/img/polbert.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'klue/roberta-large': {'modelId': 'klue/roberta-large',\n",
       "  'sha': '5193b95701189160c45d02a1033a4ea55bdbe259',\n",
       "  'lastModified': '2021-10-20T16:13:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'klue/roberta-large',\n",
       "  'downloads': 6965,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': 'ÎåÄÌïúÎØºÍµ≠Ïùò ÏàòÎèÑÎäî [MASK] ÏûÖÎãàÎã§.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'rinna/japanese-gpt2-medium': {'modelId': 'rinna/japanese-gpt2-medium',\n",
       "  'sha': 'f464b76739c884d8b0479a0a7705b7fa71c3fd5a',\n",
       "  'lastModified': '2021-08-23T03:20:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'lm',\n",
       "   'nlp',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'rinna/japanese-gpt2-medium',\n",
       "  'downloads': 6895,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ÁîüÂëΩ„ÄÅÂÆáÂÆô„ÄÅ„Åù„Åó„Å¶‰∏áÁâ©„Å´„Å§„ÅÑ„Å¶„ÅÆÁ©∂Ê•µ„ÅÆÁñëÂïè„ÅÆÁ≠î„Åà„ÅØ'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-gpt2/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'gpt2', 'text-generation', 'lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia'],\n",
       "   'widget': [{'text': 'ÁîüÂëΩ„ÄÅÂÆáÂÆô„ÄÅ„Åù„Åó„Å¶‰∏áÁâ©„Å´„Å§„ÅÑ„Å¶„ÅÆÁ©∂Ê•µ„ÅÆÁñëÂïè„ÅÆÁ≠î„Åà„ÅØ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/biomed_roberta_base': {'modelId': 'allenai/biomed_roberta_base',\n",
       "  'sha': '6209646a5f79bbd383f8193d70e88ab00ae779f8',\n",
       "  'lastModified': '2021-05-20T13:00:31.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'allenai/biomed_roberta_base',\n",
       "  'downloads': 6883,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/allenai.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'TheGoldenToaster/DialoGPT-medium-Bot': {'modelId': 'TheGoldenToaster/DialoGPT-medium-Bot',\n",
       "  'sha': 'b9e2e669356dfda8108ccdf76d4db16cef38f227',\n",
       "  'lastModified': '2022-04-04T21:58:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'TheGoldenToaster',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'TheGoldenToaster/DialoGPT-medium-Bot',\n",
       "  'downloads': 6873,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/beit-base-patch16-224-pt22k-ft22k': {'modelId': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'sha': 'e10357ec82ca7a0de830c20ffc715b6c33cde963',\n",
       "  'lastModified': '2022-01-28T10:17:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'beit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2106.08254',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BeitForImageClassification'],\n",
       "   'model_type': 'beit'},\n",
       "  'id': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'downloads': 6857,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet', 'imagenet-21k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'sshleifer/distilbart-xsum-1-1': {'modelId': 'sshleifer/distilbart-xsum-1-1',\n",
       "  'sha': '891968fcbb0e421075cc2c3dfc8da8d4b24d54a4',\n",
       "  'lastModified': '2021-06-14T07:53:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'sshleifer/distilbart-xsum-1-1',\n",
       "  'downloads': 6853,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/data2vec-vision-base': {'modelId': 'facebook/data2vec-vision-base',\n",
       "  'sha': '72a7bdadab41d0e9a2c8d6887b9f8a50eebb8e0f',\n",
       "  'lastModified': '2022-05-03T15:52:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'data2vec-vision',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2202.03555',\n",
       "   'arxiv:2106.08254',\n",
       "   'transformers',\n",
       "   'image-classification',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Data2VecVisionModel'],\n",
       "   'model_type': 'data2vec-vision'},\n",
       "  'id': 'facebook/data2vec-vision-base',\n",
       "  'downloads': 6835,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet', 'imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-gmq-en': {'modelId': 'Helsinki-NLP/opus-mt-gmq-en',\n",
       "  'sha': '74efbe7477ba9acf0bcc143fcad9f5280db2fab4',\n",
       "  'lastModified': '2021-01-18T08:52:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'da',\n",
       "   'nb',\n",
       "   'sv',\n",
       "   'is',\n",
       "   'nn',\n",
       "   'fo',\n",
       "   'gmq',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-gmq-en',\n",
       "  'downloads': 6795,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['da', 'nb', 'sv', 'is', 'nn', 'fo', 'gmq', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kykim/bert-kor-base': {'modelId': 'kykim/bert-kor-base',\n",
       "  'sha': '1779cc0982ada0216dd6de0dd4e86fb78201926d',\n",
       "  'lastModified': '2021-05-19T21:17:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'kykim',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'kykim/bert-kor-base',\n",
       "  'downloads': 6787,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nvidia/segformer-b0-finetuned-ade-512-512': {'modelId': 'nvidia/segformer-b0-finetuned-ade-512-512',\n",
       "  'sha': 'f5533fed06b606a60402e8164ff745c02c8bc943',\n",
       "  'lastModified': '2022-02-21T20:07:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'segformer',\n",
       "   'dataset:scene_parse_150',\n",
       "   'arxiv:2105.15203',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'image-segmentation',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-segmentation',\n",
       "  'private': False,\n",
       "  'author': 'nvidia',\n",
       "  'config': {'architectures': ['SegformerForSemanticSegmentation'],\n",
       "   'model_type': 'segformer'},\n",
       "  'id': 'nvidia/segformer-b0-finetuned-ade-512-512',\n",
       "  'downloads': 6730,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "    'example_title': 'House'},\n",
       "   {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "    'example_title': 'Castle'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-segmentation'],\n",
       "   'datasets': ['scene_parse_150'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "     'example_title': 'House'},\n",
       "    {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "     'example_title': 'Castle'}]},\n",
       "  'transformersInfo': {'auto_model': 'SegformerForSemanticSegmentation',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Geotrend/bert-base-ru-cased': {'modelId': 'Geotrend/bert-base-ru-cased',\n",
       "  'sha': '2810f3d4fa13f6a045fcda6a6e91bfb085e60396',\n",
       "  'lastModified': '2021-05-18T20:09:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ru',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Geotrend',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Geotrend/bert-base-ru-cased',\n",
       "  'downloads': 6692,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω—è –∑–æ–≤—É—Ç [MASK] –∏ —è –∏–Ω–∂–µ–Ω–µ—Ä –∂–∏–≤—É—â–∏–π –≤ –ù—å—é-–ô–æ—Ä–∫–µ.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ru',\n",
       "   'datasets': 'wikipedia',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert-flax-only': {'modelId': 'hf-internal-testing/tiny-bert-flax-only',\n",
       "  'sha': 'c82d9a500998a5d90f393d8c72cadf1829eb0d51',\n",
       "  'lastModified': '2022-01-21T16:19:45.000Z',\n",
       "  'tags': ['jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-bert-flax-only',\n",
       "  'downloads': 6640,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/hubert-base-ls960': {'modelId': 'facebook/hubert-base-ls960',\n",
       "  'sha': 'dba3bb02fda4248b6e082697eee756de8fe8aa8a',\n",
       "  'lastModified': '2021-11-05T12:43:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'hubert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2106.07447',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['HubertModel'], 'model_type': 'hubert'},\n",
       "  'id': 'facebook/hubert-base-ls960',\n",
       "  'downloads': 6635,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eleldar/language-detection': {'modelId': 'eleldar/language-detection',\n",
       "  'sha': '735cc09f01b8b4d831228e0fa7640464e56a022e',\n",
       "  'lastModified': '2022-05-24T10:06:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'eleldar',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'eleldar/language-detection',\n",
       "  'downloads': 6635,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "    'results': []}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'metrics': ['accuracy', 'f1'],\n",
       "   'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "     'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_small_cased': {'modelId': 'flaubert/flaubert_small_cased',\n",
       "  'sha': '21a2d6f46294ad07a0b692d96af443990c07f790',\n",
       "  'lastModified': '2021-05-19T16:56:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'flaubert-small',\n",
       "   'cased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_small_cased',\n",
       "  'downloads': 6574,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'flaubert-small',\n",
       "    'cased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis': {'modelId': 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis',\n",
       "  'sha': '0392a911472f7fa3db4ebacee570be79b16187f2',\n",
       "  'lastModified': '2021-09-16T18:43:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'dataset:financial_phrasebank',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'financial',\n",
       "   'stocks',\n",
       "   'sentiment',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis',\n",
       "  'downloads': 6572,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .'}],\n",
       "  'likes': 17,\n",
       "  'model-index': [{'name': 'distilRoberta-financial-sentiment',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'financial_phrasebank',\n",
       "       'type': 'financial_phrasebank',\n",
       "       'args': 'sentences_allagree'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9823008849557522}]}]}],\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['generated_from_trainer', 'financial', 'stocks', 'sentiment'],\n",
       "   'widget': [{'text': 'Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .'}],\n",
       "   'datasets': ['financial_phrasebank'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'model-index': [{'name': 'distilRoberta-financial-sentiment',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'financial_phrasebank',\n",
       "        'type': 'financial_phrasebank',\n",
       "        'args': 'sentences_allagree'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9823008849557522}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/cocolm-large': {'modelId': 'microsoft/cocolm-large',\n",
       "  'sha': '6947b62b7ca98d5f883c291b7a32e9b2d54130ef',\n",
       "  'lastModified': '2022-02-07T22:49:54.000Z',\n",
       "  'tags': ['pytorch', 'arxiv:2102.08473', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {},\n",
       "  'id': 'microsoft/cocolm-large',\n",
       "  'downloads': 6557,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'vblagoje/bert-english-uncased-finetuned-pos': {'modelId': 'vblagoje/bert-english-uncased-finetuned-pos',\n",
       "  'sha': '46ec120264b121e8d92bef19b45c107d06d2cb99',\n",
       "  'lastModified': '2021-05-20T08:51:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'vblagoje',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'vblagoje/bert-english-uncased-finetuned-pos',\n",
       "  'downloads': 6542,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'AmbricJohnson5888/death': {'modelId': 'AmbricJohnson5888/death',\n",
       "  'sha': '68ca65a4f20454f7ea435dba297a0d959cd67183',\n",
       "  'lastModified': '2022-04-09T02:19:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AmbricJohnson5888',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AmbricJohnson5888/death',\n",
       "  'downloads': 6523,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/rut5-base-paraphraser': {'modelId': 'cointegrated/rut5-base-paraphraser',\n",
       "  'sha': '89213d06450b722514e23ba55ae7c16a2203a3b8',\n",
       "  'lastModified': '2022-02-08T13:06:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'dataset:cointegrated/ru-paraphrase-NMT-Leipzig',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'paraphrasing',\n",
       "   'paraphraser',\n",
       "   'paraphrase',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'cointegrated/rut5-base-paraphraser',\n",
       "  'downloads': 6480,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ö–∞–∂–¥—ã–π –æ—Ö–æ—Ç–Ω–∏–∫ –∂–µ–ª–∞–µ—Ç –∑–Ω–∞—Ç—å, –≥–¥–µ —Å–∏–¥–∏—Ç —Ñ–∞–∑–∞–Ω.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['russian', 'paraphrasing', 'paraphraser', 'paraphrase'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '–ö–∞–∂–¥—ã–π –æ—Ö–æ—Ç–Ω–∏–∫ –∂–µ–ª–∞–µ—Ç –∑–Ω–∞—Ç—å, –≥–¥–µ —Å–∏–¥–∏—Ç —Ñ–∞–∑–∞–Ω.'}],\n",
       "   'datasets': ['cointegrated/ru-paraphrase-NMT-Leipzig']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-large-cased-v1.1': {'modelId': 'dmis-lab/biobert-large-cased-v1.1',\n",
       "  'sha': 'c6775648fdc33f369c4342679bcf0f2691e08b3c',\n",
       "  'lastModified': '2020-10-14T06:19:39.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {},\n",
       "  'id': 'dmis-lab/biobert-large-cased-v1.1',\n",
       "  'downloads': 6434,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/s2t-small-mustc-en-fr-st': {'modelId': 'facebook/s2t-small-mustc-en-fr-st',\n",
       "  'sha': '90bd87c9a36fc51f1acb419760563551671e3b4e',\n",
       "  'lastModified': '2022-02-07T14:44:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'speech_to_text',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'dataset:mustc',\n",
       "   'arxiv:2010.05171',\n",
       "   'arxiv:1904.08779',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech-translation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Speech2TextForConditionalGeneration'],\n",
       "   'model_type': 'speech_to_text'},\n",
       "  'id': 'facebook/s2t-small-mustc-en-fr-st',\n",
       "  'downloads': 6413,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr'],\n",
       "   'datasets': ['mustc'],\n",
       "   'tags': ['audio', 'speech-translation', 'automatic-speech-recognition'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/pegasus-pubmed': {'modelId': 'google/pegasus-pubmed',\n",
       "  'sha': '27396b90fefc6b2f8365728fb1e23963d7feca2d',\n",
       "  'lastModified': '2020-10-22T16:33:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-pubmed',\n",
       "  'downloads': 6400,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-roa-en': {'modelId': 'Helsinki-NLP/opus-mt-roa-en',\n",
       "  'sha': '7d547ac627a5bee2cbeb5e614f9752927eb52aab',\n",
       "  'lastModified': '2020-08-21T14:42:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'it',\n",
       "   'ca',\n",
       "   'rm',\n",
       "   'es',\n",
       "   'ro',\n",
       "   'gl',\n",
       "   'co',\n",
       "   'wa',\n",
       "   'pt',\n",
       "   'oc',\n",
       "   'an',\n",
       "   'id',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'roa',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-roa-en',\n",
       "  'downloads': 6395,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Mi chiamo Sarah e vivo a Londra'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['it',\n",
       "    'ca',\n",
       "    'rm',\n",
       "    'es',\n",
       "    'ro',\n",
       "    'gl',\n",
       "    'co',\n",
       "    'wa',\n",
       "    'pt',\n",
       "    'oc',\n",
       "    'an',\n",
       "    'id',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'roa',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ro': {'modelId': 'Helsinki-NLP/opus-mt-en-ro',\n",
       "  'sha': '6d32771161c298fb3164c208e48fea050a85ab65',\n",
       "  'lastModified': '2021-09-09T21:38:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ro',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ro',\n",
       "  'downloads': 6371,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/distilbert-fa-zwnj-base-ner': {'modelId': 'HooshvareLab/distilbert-fa-zwnj-base-ner',\n",
       "  'sha': '36ccd9aa3dd64c3a83c76de0b8cc5b3f6fa3dc30',\n",
       "  'lastModified': '2021-03-21T14:32:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'fa',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'HooshvareLab/distilbert-fa-zwnj-base-ner',\n",
       "  'downloads': 6371,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿß€åŸÜ ÿ≥ÿ±€åÿßŸÑ ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿ±ÿ≥ŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ÿØŸáŸÖ ŸÖ€å €≤€∞€±€± ÿ™Ÿàÿ≥ÿ∑ ÿ¥ÿ®⁄©Ÿá ŸÅÿß⁄©ÿ≥ ÿ®ÿ±ÿß€å ŸæÿÆÿ¥ ÿ±ÿ≤ÿ±Ÿà ÿ¥ÿØ.'},\n",
       "   {'text': 'ÿØŸÅÿ™ÿ± ŸÖÿ±⁄©ÿ≤€å ÿ¥ÿ±⁄©ÿ™ Ÿæÿßÿ±ÿ≥\\u200cŸÖ€åŸÜŸà ÿØÿ± ÿ¥Ÿáÿ± ÿßÿ±ÿß⁄© ÿØÿ± ÿßÿ≥ÿ™ÿßŸÜ ŸÖÿ±⁄©ÿ≤€å ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ.'},\n",
       "   {'text': 'Ÿà€å ÿØÿ± ÿ≥ÿßŸÑ €≤€∞€±€≥ ÿØÿ±⁄Øÿ∞ÿ¥ÿ™ Ÿà ŸÖÿ≥ÿ¶ŸàŸÑ ÿÆÿß⁄©ÿ≥Ÿæÿßÿ±€å Ÿà ÿßŸÇŸàÿßŸÖÿ¥ ÿ®ÿ±ÿß€å ÿßŸà ŸÖÿ±ÿßÿ≥ŸÖ €åÿßÿØÿ®ŸàÿØ ⁄Øÿ±ŸÅÿ™ŸÜÿØ.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shahrukhx01/question-vs-statement-classifier': {'modelId': 'shahrukhx01/question-vs-statement-classifier',\n",
       "  'sha': '38767ed252cb888c5c1507ea7933150537500893',\n",
       "  'lastModified': '2021-08-25T08:17:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'neural-search-query-classification',\n",
       "   'neural-search'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'shahrukhx01',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'shahrukhx01/question-vs-statement-classifier',\n",
       "  'downloads': 6362,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'what did you eat in lunch?'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['neural-search-query-classification', 'neural-search'],\n",
       "   'widget': [{'text': 'what did you eat in lunch?'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'avichr/heBERT': {'modelId': 'avichr/heBERT',\n",
       "  'sha': '01566c04aa226325662d5054331458e14ef3ede1',\n",
       "  'lastModified': '2022-04-15T09:36:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'avichr',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'avichr/heBERT',\n",
       "  'downloads': 6336,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/upos-multi-fast': {'modelId': 'flair/upos-multi-fast',\n",
       "  'sha': '4610445ba7d097da7e0ff23c3a782e9c474382c8',\n",
       "  'lastModified': '2021-03-02T22:22:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'nl',\n",
       "   'pl',\n",
       "   'es',\n",
       "   'sv',\n",
       "   'da',\n",
       "   'no',\n",
       "   'fi',\n",
       "   'cs',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/upos-multi-fast',\n",
       "  'downloads': 6334,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'Ich liebe Berlin, as they say.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': ['en',\n",
       "    'de',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'nl',\n",
       "    'pl',\n",
       "    'es',\n",
       "    'sv',\n",
       "    'da',\n",
       "    'no',\n",
       "    'fi',\n",
       "    'cs'],\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'Ich liebe Berlin, as they say.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-ctrl': {'modelId': 'sshleifer/tiny-ctrl',\n",
       "  'sha': 'd76c849d54c665ccfd33b8aa501b17531289cae9',\n",
       "  'lastModified': '2020-05-13T23:21:48.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'ctrl', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['CTRLLMHeadModel'], 'model_type': 'ctrl'},\n",
       "  'id': 'sshleifer/tiny-ctrl',\n",
       "  'downloads': 6323,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large': {'modelId': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large',\n",
       "  'sha': '71a61139397cbb5fd773d8b8b72282a3387ff130',\n",
       "  'lastModified': '2021-06-12T02:53:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'luhua',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large',\n",
       "  'downloads': 6317,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êàë‰ΩèÂú®Âì™ÈáåÔºü', 'context': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'},\n",
       "   {'text': 'Êàë‰ΩèÂú®Âì™ÈáåÔºü', 'context': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'},\n",
       "   {'text': 'ÊàëÁöÑÂêçÂ≠óÊòØ‰ªÄ‰πàÔºü', 'context': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàë‰ΩèÂú®‰ºØÂÖãÂà©„ÄÇ'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-speech-encoder-decoder': {'modelId': 'hf-internal-testing/tiny-random-speech-encoder-decoder',\n",
       "  'sha': '880a6041222f5297adfceb3debd1a955d1c48ba5',\n",
       "  'lastModified': '2021-12-24T15:13:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'speech-encoder-decoder',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['SpeechEncoderDecoderModel'],\n",
       "   'model_type': 'speech-encoder-decoder'},\n",
       "  'id': 'hf-internal-testing/tiny-random-speech-encoder-decoder',\n",
       "  'downloads': 6286,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-spanish-large': {'modelId': 'flair/ner-spanish-large',\n",
       "  'sha': '9d4671d2f345c1258f37a29ce2321067f2ed296e',\n",
       "  'lastModified': '2021-05-08T15:36:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'es',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-spanish-large',\n",
       "  'downloads': 6285,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington fue a Washington'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'es',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington fue a Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-distilbert-base-cased': {'modelId': 'sshleifer/tiny-distilbert-base-cased',\n",
       "  'sha': '657df2b83a6986d88e4f528740259c9b49f796b1',\n",
       "  'lastModified': '2021-05-20T07:12:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'sshleifer/tiny-distilbert-base-cased',\n",
       "  'downloads': 6270,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jonesy/DialoGPT-small_JT': {'modelId': 'Jonesy/DialoGPT-small_JT',\n",
       "  'sha': '15890d5e6e8b1966ef6b5ef1ff5c37c27acceda0',\n",
       "  'lastModified': '2021-10-15T18:56:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Jonesy',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Jonesy/DialoGPT-small_JT',\n",
       "  'downloads': 6269,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/test_dynamic_model': {'modelId': 'hf-internal-testing/test_dynamic_model',\n",
       "  'sha': '2efddce40dddaccd37bae208c3c7ca66dbedf68a',\n",
       "  'lastModified': '2022-01-25T22:03:13.000Z',\n",
       "  'tags': ['pytorch', 'new-model', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['NewModel'], 'model_type': 'new-model'},\n",
       "  'id': 'hf-internal-testing/test_dynamic_model',\n",
       "  'downloads': 6269,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'DeepPavlov/bert-base-cased-conversational': {'modelId': 'DeepPavlov/bert-base-cased-conversational',\n",
       "  'sha': '5415204d80daf12299c85dfddec5f5a7fc7b620a',\n",
       "  'lastModified': '2021-11-08T13:07:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/bert-base-cased-conversational',\n",
       "  'downloads': 6268,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dumitrescustefan/bert-base-romanian-uncased-v1': {'modelId': 'dumitrescustefan/bert-base-romanian-uncased-v1',\n",
       "  'sha': '9eb44d518953103bdc9f088217ec978c6ec9a9e4',\n",
       "  'lastModified': '2021-11-02T15:26:10.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'ro', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dumitrescustefan',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dumitrescustefan/bert-base-romanian-uncased-v1',\n",
       "  'downloads': 6253,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ro'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rinna/japanese-roberta-base': {'modelId': 'rinna/japanese-roberta-base',\n",
       "  'sha': '1559edfaaadefcb1661c016455990b0f6f68b20d',\n",
       "  'lastModified': '2021-09-13T00:46:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'masked-lm',\n",
       "   'nlp',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'rinna/japanese-roberta-base',\n",
       "  'downloads': 6238,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[CLS]4Âπ¥„Å´1Â∫¶[MASK]„ÅØÈñã„Åã„Çå„Çã„ÄÇ'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-gpt2/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'roberta', 'masked-lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '[CLS]4Âπ¥„Å´1Â∫¶[MASK]„ÅØÈñã„Åã„Çå„Çã„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'digitalepidemiologylab/covid-twitter-bert-v2': {'modelId': 'digitalepidemiologylab/covid-twitter-bert-v2',\n",
       "  'sha': 'b113bc3c2590d7b32ed62603fe1ebe32e1e5beee',\n",
       "  'lastModified': '2021-09-22T08:20:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'Twitter',\n",
       "   'COVID-19',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'digitalepidemiologylab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'digitalepidemiologylab/covid-twitter-bert-v2',\n",
       "  'downloads': 6234,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/digitalepidemiologylab/covid-twitter-bert/master/images/COVID-Twitter-BERT_small.png',\n",
       "   'tags': ['Twitter', 'COVID-19'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/fastspeech2-en-ljspeech': {'modelId': 'facebook/fastspeech2-en-ljspeech',\n",
       "  'sha': 'a3e3e5e2e62bb7ca7514b11aa469e9c5b01a20bf',\n",
       "  'lastModified': '2022-01-28T23:25:24.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:ljspeech',\n",
       "   'arxiv:2006.04558',\n",
       "   'arxiv:2109.06912',\n",
       "   'fairseq',\n",
       "   'audio',\n",
       "   'text-to-speech'],\n",
       "  'pipeline_tag': 'text-to-speech',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': None,\n",
       "  'id': 'facebook/fastspeech2-en-ljspeech',\n",
       "  'downloads': 6229,\n",
       "  'library_name': 'fairseq',\n",
       "  'widgetData': [{'text': 'Hello, this is a test run.',\n",
       "    'example_title': 'Hello, this is a test run.'}],\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'library_name': 'fairseq',\n",
       "   'task': 'text-to-speech',\n",
       "   'tags': ['fairseq', 'audio', 'text-to-speech'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ljspeech'],\n",
       "   'widget': [{'text': 'Hello, this is a test run.',\n",
       "     'example_title': 'Hello, this is a test run.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'pranavpsv/gpt2-genre-story-generator': {'modelId': 'pranavpsv/gpt2-genre-story-generator',\n",
       "  'sha': 'd617d856b2aae05be1c253a2e9daf8c99f1d9c6d',\n",
       "  'lastModified': '2021-05-23T11:02:06.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'pranavpsv',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'pranavpsv/gpt2-genre-story-generator',\n",
       "  'downloads': 6225,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'haisongzhang/roberta-tiny-cased': {'modelId': 'haisongzhang/roberta-tiny-cased',\n",
       "  'sha': '2e8caf4404987b8cb20fa4b22955f56940b2ebc6',\n",
       "  'lastModified': '2021-05-19T17:53:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'haisongzhang',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'haisongzhang/roberta-tiny-cased',\n",
       "  'downloads': 6217,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Peltarion/xlm-roberta-longformer-base-4096': {'modelId': 'Peltarion/xlm-roberta-longformer-base-4096',\n",
       "  'sha': 'c2e164abd333ebd242de4178ea18c1260e00d330',\n",
       "  'lastModified': '2022-03-30T09:23:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikitext',\n",
       "   'transformers',\n",
       "   'longformer',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Peltarion',\n",
       "  'config': {'architectures': ['LongModelForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Peltarion/xlm-roberta-longformer-base-4096',\n",
       "  'downloads': 6197,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['longformer'],\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikitext']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-robust-ft-swbd-300h': {'modelId': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
       "  'sha': '828a8386883f64170e04fae06dd866e0fe97de6b',\n",
       "  'lastModified': '2022-04-05T16:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:libri_light',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:switchboard',\n",
       "   'dataset:fisher',\n",
       "   'arxiv:2104.01027',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
       "  'downloads': 6190,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['libri_light', 'common_voice', 'switchboard', 'fisher'],\n",
       "   'tags': ['speech', 'audio', 'automatic-speech-recognition'],\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Zixtrauce/JohnBot': {'modelId': 'Zixtrauce/JohnBot',\n",
       "  'sha': '3831946b0c973685e937c4db612ed7cce2733129',\n",
       "  'lastModified': '2022-01-02T06:43:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/JohnBot',\n",
       "  'downloads': 6141,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base': {'modelId': 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "  'sha': '4207aaa00b26aea91d99cb1abc2d7f56814fbe05',\n",
       "  'lastModified': '2021-12-17T10:33:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'dataset:common_language',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ivanlau',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "  'downloads': 6116,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'common_language',\n",
       "       'type': 'common_language',\n",
       "       'args': 'full'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9738386718094919}]}]}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'datasets': ['common_language'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'model-index': [{'name': 'language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'common_language',\n",
       "        'type': 'common_language',\n",
       "        'args': 'full'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9738386718094919}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/vit-base-patch32-224-in21k': {'modelId': 'google/vit-base-patch32-224-in21k',\n",
       "  'sha': 'ebb34016d84eb82beee2f88d5ae21a1f08a8ca88',\n",
       "  'lastModified': '2022-01-12T08:06:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTModel'], 'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch32-224-in21k',\n",
       "  'downloads': 6085,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-21k'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Luyu/co-condenser-marco': {'modelId': 'Luyu/co-condenser-marco',\n",
       "  'sha': 'e0cef0ab2410aae0f0994366ddefb5649a266709',\n",
       "  'lastModified': '2021-08-13T13:54:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Luyu',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Luyu/co-condenser-marco',\n",
       "  'downloads': 6084,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gelectra-large-germanquad': {'modelId': 'deepset/gelectra-large-germanquad',\n",
       "  'sha': 'cfcb7ec9b76cc01eeac1c1f0cfa0afa5b3b9cbb3',\n",
       "  'lastModified': '2021-10-21T12:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'dataset:deepset/germanquad',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/gelectra-large-germanquad',\n",
       "  'downloads': 6066,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den gr√∂√üten Teil des Amazonas-Beckens S√ºdamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der gr√∂√üte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Franz√∂sisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repr√§sentiert mehr als die H√§lfte der verbleibenden Regenw√§lder des Planeten und umfasst den gr√∂√üten und artenreichsten tropischen Regenwald der Welt mit gesch√§tzten 390 Milliarden Einzelb√§umen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germanquad'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/unixcoder-base': {'modelId': 'microsoft/unixcoder-base',\n",
       "  'sha': '02583b53b9290e674a43b6b74e89f98a71b2d22a',\n",
       "  'lastModified': '2022-03-23T06:05:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/unixcoder-base',\n",
       "  'downloads': 6050,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bert_uncased_L-8_H-512_A-8': {'modelId': 'google/bert_uncased_L-8_H-512_A-8',\n",
       "  'sha': '53b17ea0d90728ac770dfd13fcb3abdc75e4f4bf',\n",
       "  'lastModified': '2021-05-19T17:35:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-8_H-512_A-8',\n",
       "  'downloads': 6046,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/long-t5-local-base': {'modelId': 'google/long-t5-local-base',\n",
       "  'sha': 'e040d65029c54fb38eaefa4019bc3e2e31ba3c62',\n",
       "  'lastModified': '2022-06-22T09:04:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'longt5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2112.07916',\n",
       "   'arxiv:1912.08777',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['LongT5ForConditionalGeneration'],\n",
       "   'model_type': 'longt5'},\n",
       "  'id': 'google/long-t5-local-base',\n",
       "  'downloads': 6034,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gogamza/kobart-base-v2': {'modelId': 'gogamza/kobart-base-v2',\n",
       "  'sha': 'd9a1f640896cef8dcfd693b1bc57510a2b09a18f',\n",
       "  'lastModified': '2021-11-11T07:43:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'gogamza',\n",
       "  'config': {'architectures': ['BartModel'], 'model_type': 'bart'},\n",
       "  'id': 'gogamza/kobart-base-v2',\n",
       "  'downloads': 6030,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'tags': ['bart'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pierreguillou/bert-base-cased-squad-v1.1-portuguese': {'modelId': 'pierreguillou/bert-base-cased-squad-v1.1-portuguese',\n",
       "  'sha': 'fda61a9dc93104d7944a4abf5d48d51eba229a13',\n",
       "  'lastModified': '2022-01-04T09:57:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'dataset:squad',\n",
       "   'dataset:squad_v1_pt',\n",
       "   'transformers',\n",
       "   'bert-base',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'pierreguillou',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'pierreguillou/bert-base-cased-squad-v1.1-portuguese',\n",
       "  'downloads': 6007,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Quando come√ßou a pandemia de Covid-19 no mundo?',\n",
       "    'context': 'A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, uma doen√ßa respirat√≥ria aguda causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). A doen√ßa foi identificada pela primeira vez em Wuhan, na prov√≠ncia de Hubei, Rep√∫blica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'},\n",
       "   {'text': 'Onde foi descoberta a Covid-19?',\n",
       "    'context': 'A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, uma doen√ßa respirat√≥ria aguda causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). A doen√ßa foi identificada pela primeira vez em Wuhan, na prov√≠ncia de Hubei, Rep√∫blica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['question-answering', 'bert', 'bert-base', 'pytorch'],\n",
       "   'datasets': ['brWaC', 'squad', 'squad_v1_pt'],\n",
       "   'metrics': ['squad'],\n",
       "   'widget': [{'text': 'Quando come√ßou a pandemia de Covid-19 no mundo?',\n",
       "     'context': 'A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, uma doen√ßa respirat√≥ria aguda causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). A doen√ßa foi identificada pela primeira vez em Wuhan, na prov√≠ncia de Hubei, Rep√∫blica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'},\n",
       "    {'text': 'Onde foi descoberta a Covid-19?',\n",
       "     'context': 'A pandemia de COVID-19, tamb√©m conhecida como pandemia de coronav√≠rus, √© uma pandemia em curso de COVID-19, uma doen√ßa respirat√≥ria aguda causada pelo coronav√≠rus da s√≠ndrome respirat√≥ria aguda grave 2 (SARS-CoV-2). A doen√ßa foi identificada pela primeira vez em Wuhan, na prov√≠ncia de Hubei, Rep√∫blica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-base-distilled-patch16-224': {'modelId': 'facebook/deit-base-distilled-patch16-224',\n",
       "  'sha': 'ea2cf46b3d9c8865fe1de652bf26f3de547298dd',\n",
       "  'lastModified': '2022-03-18T14:37:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DeiTForImageClassificationWithTeacher'],\n",
       "   'model_type': 'deit'},\n",
       "  'id': 'facebook/deit-base-distilled-patch16-224',\n",
       "  'downloads': 5990,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'benjamin/gerpt2': {'modelId': 'benjamin/gerpt2',\n",
       "  'sha': '76b77997c1a715c3cf61a8d086fb75baa3816ded',\n",
       "  'lastModified': '2022-05-11T09:17:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'benjamin',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 100}}},\n",
       "  'id': 'benjamin/gerpt2',\n",
       "  'downloads': 5986,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einh√∂rner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einh√∂rner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/small-base': {'modelId': 'funnel-transformer/small-base',\n",
       "  'sha': '373ecb760257d0059f1efdf58b3796d4d616ad0a',\n",
       "  'lastModified': '2020-12-11T21:40:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelBaseModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/small-base',\n",
       "  'downloads': 5985,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/electra-base-squad2': {'modelId': 'deepset/electra-base-squad2',\n",
       "  'sha': 'a620af2f3aab26e35a80422941a3ada5b8387519',\n",
       "  'lastModified': '2021-10-21T12:16:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/electra-base-squad2',\n",
       "  'downloads': 5968,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-bert-large-uncased': {'modelId': 'princeton-nlp/sup-simcse-bert-large-uncased',\n",
       "  'sha': '6711247726a5d5f78c17babf57d76fa99f7b1fdf',\n",
       "  'lastModified': '2021-05-20T02:56:23.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/sup-simcse-bert-large-uncased',\n",
       "  'downloads': 5956,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/s2t-small-librispeech-asr': {'modelId': 'facebook/s2t-small-librispeech-asr',\n",
       "  'sha': '89d22e9beca033913df096434cccc2c41199d8c1',\n",
       "  'lastModified': '2022-05-24T10:45:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'speech_to_text',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2010.05171',\n",
       "   'arxiv:1904.08779',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Speech2TextForConditionalGeneration'],\n",
       "   'model_type': 'speech_to_text'},\n",
       "  'id': 'facebook/s2t-small-librispeech-asr',\n",
       "  'downloads': 5922,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 's2t-small-librispeech-asr',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 4.3}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'model-index': [{'name': 's2t-small-librispeech-asr',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 4.3}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'mrm8488/t5-base-finetuned-emotion': {'modelId': 'mrm8488/t5-base-finetuned-emotion',\n",
       "  'sha': 'e44a316825f11230724b36412fbf1899c76e82de',\n",
       "  'lastModified': '2021-06-23T12:46:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:emotion',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-emotion',\n",
       "  'downloads': 5916,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I wish you were here but it is impossible'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['emotion'],\n",
       "   'widget': [{'text': 'I wish you were here but it is impossible'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sbcBI/sentiment_analysis_model': {'modelId': 'sbcBI/sentiment_analysis_model',\n",
       "  'sha': 'a994fec145b4e096961210b871c376a0ba8440ba',\n",
       "  'lastModified': '2022-05-16T18:37:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:Confidential',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sbcBI',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sbcBI/sentiment_analysis_model',\n",
       "  'downloads': 5903,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['Confidential']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/bart-large-xsum-samsum': {'modelId': 'Salesforce/bart-large-xsum-samsum',\n",
       "  'sha': 'bf8a8779c158901df223516a72b9efaa887ed1df',\n",
       "  'lastModified': '2021-06-09T19:36:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'Salesforce/bart-large-xsum-samsum',\n",
       "  'downloads': 5900,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/trocr-base-handwritten': {'modelId': 'microsoft/trocr-base-handwritten',\n",
       "  'sha': 'bad90c41e8b5a5cd03f658fbd568b44b2ee047c5',\n",
       "  'lastModified': '2022-07-01T07:35:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'arxiv:2109.10282',\n",
       "   'transformers',\n",
       "   'trocr',\n",
       "   'image-to-text'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'microsoft/trocr-base-handwritten',\n",
       "  'downloads': 5889,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['trocr', 'image-to-text']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/rbt3': {'modelId': 'hfl/rbt3',\n",
       "  'sha': '0aa0527ff4170f29e1dfd3eb6ef60dc67e1bf75c',\n",
       "  'lastModified': '2021-05-19T19:19:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/rbt3',\n",
       "  'downloads': 5868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'tags': ['bert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'pipeline_tag': 'fill-mask'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'indobenchmark/indobert-base-p2': {'modelId': 'indobenchmark/indobert-base-p2',\n",
       "  'sha': '94b4e0a82081fa57f227fcc2024d1ea89b57ac1f',\n",
       "  'lastModified': '2021-05-19T20:24:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'id',\n",
       "   'dataset:Indo4B',\n",
       "   'arxiv:2009.05387',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indobenchmark',\n",
       "   'indonlu',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'indobenchmark',\n",
       "  'config': {'architectures': ['BertModel'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'indobenchmark/indobert-base-p2',\n",
       "  'downloads': 5856,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indobenchmark', 'indonlu'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['Indo4B']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ktrapeznikov/albert-xlarge-v2-squad-v2': {'modelId': 'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
       "  'sha': 'fb1e05445e376bdb883e8d4f6696a0acaf62e0ae',\n",
       "  'lastModified': '2020-12-11T21:48:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ktrapeznikov',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
       "  'downloads': 5854,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased-whole-word-masking': {'modelId': 'bert-large-cased-whole-word-masking',\n",
       "  'sha': '8b5d59881077680d99e2cac339412f20a180bd45',\n",
       "  'lastModified': '2021-05-18T16:30:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased-whole-word-masking',\n",
       "  'downloads': 5842,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sbcBI/sentiment_analysis': {'modelId': 'sbcBI/sentiment_analysis',\n",
       "  'sha': '2e9e3afe68478a6168a11adb6c6f1b741e00ae83',\n",
       "  'lastModified': '2022-04-22T06:42:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:Confidential',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sbcBI',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sbcBI/sentiment_analysis',\n",
       "  'downloads': 5828,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['Confidential']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-uncased': {'modelId': 'dbmdz/bert-base-italian-uncased',\n",
       "  'sha': 'd91243bae3a97a72691e9a6bfdf5d9f8fa4be9e4',\n",
       "  'lastModified': '2021-05-19T15:00:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-uncased',\n",
       "  'downloads': 5826,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma √® la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita √® [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KETI-AIR/ke-t5-small': {'modelId': 'KETI-AIR/ke-t5-small',\n",
       "  'sha': '3a2efa3a340d88de8aa93be0cad7884c34a64128',\n",
       "  'lastModified': '2021-06-23T03:13:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KETI-AIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'KETI-AIR/ke-t5-small',\n",
       "  'downloads': 5815,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/unicoil-msmarco-passage': {'modelId': 'castorini/unicoil-msmarco-passage',\n",
       "  'sha': 'a9379ff729899cf1255960e604496c1a638346ce',\n",
       "  'lastModified': '2021-07-13T22:28:03.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'castorini/unicoil-msmarco-passage',\n",
       "  'downloads': 5796,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-base-generator': {'modelId': 'google/electra-base-generator',\n",
       "  'sha': '1c65e3f5f4597679b87620707df5774c08c6606d',\n",
       "  'lastModified': '2021-04-30T07:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-base-generator',\n",
       "  'downloads': 5791,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_t5_xl_half_uniref50-enc': {'modelId': 'Rostlab/prot_t5_xl_half_uniref50-enc',\n",
       "  'sha': '2646ade9d44b7620ceac59797b2d9efd3341da37',\n",
       "  'lastModified': '2022-06-29T08:22:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'protein',\n",
       "   'dataset:UniRef50',\n",
       "   'transformers',\n",
       "   'protein language model'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['T5EncoderModel'], 'model_type': 't5'},\n",
       "  'id': 'Rostlab/prot_t5_xl_half_uniref50-enc',\n",
       "  'downloads': 5786,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['UniRef50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-en-ru': {'modelId': 'facebook/wmt19-en-ru',\n",
       "  'sha': 'a96448f0cef6a0b22c8f73f6f0e74b610efe806f',\n",
       "  'lastModified': '2020-12-11T21:39:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ru',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-en-ru',\n",
       "  'downloads': 5785,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'ru'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-lv-en': {'modelId': 'Helsinki-NLP/opus-mt-lv-en',\n",
       "  'sha': '3b019339e88ac4f79044be45cfa75ff5fedbceea',\n",
       "  'lastModified': '2021-09-10T13:57:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'lv',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-lv-en',\n",
       "  'downloads': 5779,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'moussaKam/frugalscore_tiny_bert-base_bert-score': {'modelId': 'moussaKam/frugalscore_tiny_bert-base_bert-score',\n",
       "  'sha': 'a487e5a875e63ef1f9cf6015a3a11be2d80aa550',\n",
       "  'lastModified': '2022-02-01T10:50:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:2110.08559',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'moussaKam',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'moussaKam/frugalscore_tiny_bert-base_bert-score',\n",
       "  'downloads': 5773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-e2m-intent': {'modelId': 'mrm8488/t5-base-finetuned-e2m-intent',\n",
       "  'sha': '84f655dbb0f40e64e12ad1a61c125a1225fc2917',\n",
       "  'lastModified': '2020-12-11T21:55:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:event2Mind',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-e2m-intent',\n",
       "  'downloads': 5762,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['event2Mind']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B': {'modelId': 'KoboldAI/fairseq-dense-13B',\n",
       "  'sha': 'e936211b7bb8f406cb78efca22a5f7c43ba090b3',\n",
       "  'lastModified': '2022-02-01T22:51:59.000Z',\n",
       "  'tags': ['pytorch', 'xglm', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B',\n",
       "  'downloads': 5735,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/wmt19-de-en-6-6-base': {'modelId': 'allenai/wmt19-de-en-6-6-base',\n",
       "  'sha': 'a9ec1968c8c3962f0f85f9f38ee4b8093ce84f24',\n",
       "  'lastModified': '2020-12-11T21:33:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:2006.10369',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'allenai',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'allenai/wmt19-de-en-6-6-base',\n",
       "  'downloads': 5735,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['translation', 'wmt19', 'allenai'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-multi_news': {'modelId': 'google/pegasus-multi_news',\n",
       "  'sha': 'e68ac31f04c1daf2956f36d5ad1701f2d6f91932',\n",
       "  'lastModified': '2020-10-22T16:33:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-multi_news',\n",
       "  'downloads': 5725,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'typeform/mobilebert-uncased-mnli': {'modelId': 'typeform/mobilebert-uncased-mnli',\n",
       "  'sha': 'b60d566014db63a45a440ee32b3e9e9a01d2a1fc',\n",
       "  'lastModified': '2021-02-14T09:11:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mobilebert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'typeform',\n",
       "  'config': {'architectures': ['MobileBertForSequenceClassification'],\n",
       "   'model_type': 'mobilebert'},\n",
       "  'id': 'typeform/mobilebert-uncased-mnli',\n",
       "  'downloads': 5707,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['mobilebert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-CoLA': {'modelId': 'textattack/bert-base-uncased-CoLA',\n",
       "  'sha': '5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391',\n",
       "  'lastModified': '2021-05-20T07:31:05.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-CoLA',\n",
       "  'downloads': 5694,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v2-xxlarge': {'modelId': 'microsoft/deberta-v2-xxlarge',\n",
       "  'sha': 'da58c8f337be794a1bb98c218d0d8cc72c324884',\n",
       "  'lastModified': '2022-01-13T20:00:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v2-xxlarge',\n",
       "  'downloads': 5686,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sachin/vit2distilgpt2': {'modelId': 'sachin/vit2distilgpt2',\n",
       "  'sha': '51be2b2bcadf5f37a4b2da6466f64e764c85add7',\n",
       "  'lastModified': '2022-01-27T12:15:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'en',\n",
       "   'dataset:coco2017',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'sachin',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'sachin/vit2distilgpt2',\n",
       "  'downloads': 5681,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['image-to-text'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['coco2017']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EEE/DialoGPT-medium-brooke': {'modelId': 'EEE/DialoGPT-medium-brooke',\n",
       "  'sha': '6c5ccd6420a957b3116bbd02f21ed4e5ae1ac59d',\n",
       "  'lastModified': '2021-09-27T06:25:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'EEE',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'EEE/DialoGPT-medium-brooke',\n",
       "  'downloads': 5657,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SAPOSS/password-model': {'modelId': 'SAPOSS/password-model',\n",
       "  'sha': '0ffd8651e74d7548268af1d0b2a611709149dcdf',\n",
       "  'lastModified': '2021-10-11T13:45:53.000Z',\n",
       "  'tags': ['tf', 'roberta', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'SAPOSS',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'SAPOSS/password-model',\n",
       "  'downloads': 5634,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'razent/SciFive-base-Pubmed': {'modelId': 'razent/SciFive-base-Pubmed',\n",
       "  'sha': '7ecd3e2966a97aa898461113a2dbb8da1acac625',\n",
       "  'lastModified': '2022-03-20T17:47:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:pubmed',\n",
       "   'arxiv:2106.03598',\n",
       "   'transformers',\n",
       "   'token-classification',\n",
       "   'text-classification',\n",
       "   'question-answering',\n",
       "   'text-generation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'razent',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'razent/SciFive-base-Pubmed',\n",
       "  'downloads': 5626,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['token-classification',\n",
       "    'text-classification',\n",
       "    'question-answering',\n",
       "    'text2text-generation',\n",
       "    'text-generation'],\n",
       "   'datasets': ['pubmed']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment': {'modelId': 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment',\n",
       "  'sha': '21ff55d0bd2f7904d2a5380165ac2fd6d0d74b81',\n",
       "  'lastModified': '2022-05-27T07:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'NLU',\n",
       "   'Sentiment',\n",
       "   'Chinese',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'IDEA-CCNL',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment',\n",
       "  'downloads': 5606,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰ªäÂ§©ÂøÉÊÉÖ‰∏çÂ•Ω'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'NLU', 'Sentiment', 'Chinese'],\n",
       "   'inference': True,\n",
       "   'widget': [{'text': '‰ªäÂ§©ÂøÉÊÉÖ‰∏çÂ•Ω'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k': {'modelId': 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k',\n",
       "  'sha': 'bb8a876bc157b5cf3c405994accb798c49146016',\n",
       "  'lastModified': '2021-09-23T15:48:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri1Mix',\n",
       "   'dataset:enh_single',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k',\n",
       "  'downloads': 5600,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri1Mix', 'enh_single'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'allenai/led-large-16384-arxiv': {'modelId': 'allenai/led-large-16384-arxiv',\n",
       "  'sha': '6d566f57e58195c1810dd8497ccf7f015409a1a9',\n",
       "  'lastModified': '2021-01-12T23:14:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:scientific_papers',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-large-16384-arxiv',\n",
       "  'downloads': 5598,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['scientific_papers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flax-community/vqgan_f16_16384': {'modelId': 'flax-community/vqgan_f16_16384',\n",
       "  'sha': '38503412bfd13702a4a96b986366a86a62eb322d',\n",
       "  'lastModified': '2021-07-25T15:15:36.000Z',\n",
       "  'tags': ['jax', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'flax-community',\n",
       "  'config': {'architectures': ['del']},\n",
       "  'id': 'flax-community/vqgan_f16_16384',\n",
       "  'downloads': 5595,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'del'}},\n",
       " 'google/bert_uncased_L-12_H-768_A-12': {'modelId': 'google/bert_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'cc478efa3482fefe275eb2733363db9713d499ef',\n",
       "  'lastModified': '2021-05-19T17:27:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 5578,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Nerys-v2': {'modelId': 'KoboldAI/fairseq-dense-13B-Nerys-v2',\n",
       "  'sha': '45d7cf7f2a4f285bbe36a3421f2497c925f86ef4',\n",
       "  'lastModified': '2022-06-25T11:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Nerys-v2',\n",
       "  'downloads': 5575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kha-white/manga-ocr-base': {'modelId': 'kha-white/manga-ocr-base',\n",
       "  'sha': 'aa6573bd10b0d446cbf622e29c3e084914df9741',\n",
       "  'lastModified': '2022-06-22T15:34:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'ja',\n",
       "   'dataset:manga109s',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'kha-white',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'kha-white/manga-ocr-base',\n",
       "  'downloads': 5545,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'tags': ['image-to-text'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['manga109s']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L12-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L12-v2',\n",
       "  'sha': '8f010f24d5c0e1ee9735f056d024fcda6557f70f',\n",
       "  'lastModified': '2022-06-15T20:18:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L12-v2',\n",
       "  'downloads': 5534,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/distilbart-cnn-12-6-samsum': {'modelId': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "  'sha': '196850410ee8bb4bab96e4d79b8ffa2cf3d1150c',\n",
       "  'lastModified': '2022-06-24T11:24:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'sagemaker',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "  'downloads': 5489,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Jeff: Can I train a ü§ó Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face '}],\n",
       "  'likes': 4,\n",
       "  'model-index': [{'name': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'samsum',\n",
       "       'type': 'samsum',\n",
       "       'config': 'samsum',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 41.0895,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 20.7459,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 31.5952,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 38.3389,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 1.4566329717636108,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 59.6032,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sagemaker', 'bart', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': 'Jeff: Can I train a ü§ó Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face '}],\n",
       "   'model-index': [{'name': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'samsum',\n",
       "        'type': 'samsum',\n",
       "        'config': 'samsum',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 41.0895,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 20.7459,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 31.5952,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 38.3389,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 1.4566329717636108,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 59.6032,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eugenesiow/drln-bam': {'modelId': 'eugenesiow/drln-bam',\n",
       "  'sha': 'c5d6efffdb6d7ddda5909f2d9105b33b69f4039f',\n",
       "  'lastModified': '2021-09-13T08:34:40.000Z',\n",
       "  'tags': ['DRLN',\n",
       "   'dataset:eugenesiow/Div2k',\n",
       "   'dataset:eugenesiow/Set5',\n",
       "   'dataset:eugenesiow/Set14',\n",
       "   'dataset:eugenesiow/BSD100',\n",
       "   'dataset:eugenesiow/Urban100',\n",
       "   'arxiv:1906.12021',\n",
       "   'arxiv:2104.07566',\n",
       "   'transformers',\n",
       "   'super-image',\n",
       "   'image-super-resolution',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'eugenesiow',\n",
       "  'config': {'model_type': 'DRLN'},\n",
       "  'id': 'eugenesiow/drln-bam',\n",
       "  'downloads': 5455,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['super-image', 'image-super-resolution'],\n",
       "   'datasets': ['eugenesiow/Div2k',\n",
       "    'eugenesiow/Set5',\n",
       "    'eugenesiow/Set14',\n",
       "    'eugenesiow/BSD100',\n",
       "    'eugenesiow/Urban100'],\n",
       "   'metrics': ['pnsr', 'ssim']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'AmbricJohnson5888/claura': {'modelId': 'AmbricJohnson5888/claura',\n",
       "  'sha': 'bb46422b9136a2fc1217cd6debdf362e49f26743',\n",
       "  'lastModified': '2022-04-09T04:18:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AmbricJohnson5888',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AmbricJohnson5888/claura',\n",
       "  'downloads': 5449,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert/camembert-base': {'modelId': 'camembert/camembert-base',\n",
       "  'sha': 'e12767c19b74b1efc75b0af07bbde51ddd26b529',\n",
       "  'lastModified': '2022-06-17T23:06:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'arxiv:1911.03894',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'camembert',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'camembert/camembert-base',\n",
       "  'downloads': 5428,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-bert-base-fin': {'modelId': 'Langboat/mengzi-bert-base-fin',\n",
       "  'sha': 'b7b290d3b4dd5ec87f47d3cf5d55c9d00bd69e59',\n",
       "  'lastModified': '2021-10-18T05:53:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Langboat/mengzi-bert-base-fin',\n",
       "  'downloads': 5397,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'robot-test/dummy-tokenizer-fast-with-model-config': {'modelId': 'robot-test/dummy-tokenizer-fast-with-model-config',\n",
       "  'sha': '8a27784052ebf721c837afe249050f6460c6a587',\n",
       "  'lastModified': '2021-05-31T15:40:58.000Z',\n",
       "  'tags': ['albert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'robot-test',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'robot-test/dummy-tokenizer-fast-with-model-config',\n",
       "  'downloads': 5374,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-distilbert-cos-v5': {'modelId': 'sentence-transformers/msmarco-distilbert-cos-v5',\n",
       "  'sha': '97bf29337ec20da8a1fb1ff2bd5555de5b566baf',\n",
       "  'lastModified': '2022-06-15T21:48:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-cos-v5',\n",
       "  'downloads': 5360,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'richielleisart/Childe': {'modelId': 'richielleisart/Childe',\n",
       "  'sha': '5da9de8d7f9e9cfde2c126b6ac6531b3ddff606a',\n",
       "  'lastModified': '2022-01-19T18:52:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'richielleisart',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'richielleisart/Childe',\n",
       "  'downloads': 5335,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlm-base-cased': {'modelId': 'microsoft/layoutlm-base-cased',\n",
       "  'sha': '91acf0f93186fbcebb9f80c2e2259754f0ef922b',\n",
       "  'lastModified': '2021-09-27T05:55:31.000Z',\n",
       "  'tags': ['pytorch', 'layoutlm', 'arxiv:1912.13318', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlm'},\n",
       "  'id': 'microsoft/layoutlm-base-cased',\n",
       "  'downloads': 5328,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/rag-token-base': {'modelId': 'facebook/rag-token-base',\n",
       "  'sha': 'bdeb4ef1d547bcfe5445aba1704ece55af71dd58',\n",
       "  'lastModified': '2020-12-11T21:39:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagTokenForGeneration'], 'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-token-base',\n",
       "  'downloads': 5324,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagTokenForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-t5-base': {'modelId': 'Langboat/mengzi-t5-base',\n",
       "  'sha': 'fbd9c58ba8e1a5393668fbdfe477ec70267c01e7',\n",
       "  'lastModified': '2021-10-21T12:33:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'Langboat/mengzi-t5-base',\n",
       "  'downloads': 5299,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'esiebomajeremiah/autonlp-email-classification-657119381': {'modelId': 'esiebomajeremiah/autonlp-email-classification-657119381',\n",
       "  'sha': '484ba1babc3906d77331d95c1587aea7f3683637',\n",
       "  'lastModified': '2022-03-22T13:57:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:esiebomajeremiah/autonlp-data-email-classification',\n",
       "   'transformers',\n",
       "   'autonlp',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'esiebomajeremiah',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'esiebomajeremiah/autonlp-email-classification-657119381',\n",
       "  'downloads': 5281,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I love AutoNLP ü§ó'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': 'autonlp',\n",
       "   'language': 'en',\n",
       "   'widget': [{'text': 'I love AutoNLP ü§ó'}],\n",
       "   'datasets': ['esiebomajeremiah/autonlp-data-email-classification'],\n",
       "   'co2_eq_emissions': 3.516233232503715},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'BeIR/query-gen-msmarco-t5-base-v1': {'modelId': 'BeIR/query-gen-msmarco-t5-base-v1',\n",
       "  'sha': 'f86569026d333fbf584ce9f8d86b49dc521a7db0',\n",
       "  'lastModified': '2021-06-23T02:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'BeIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'BeIR/query-gen-msmarco-t5-base-v1',\n",
       "  'downloads': 5274,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best': {'modelId': 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best',\n",
       "  'sha': '71320176d2a8de165fe0d2a8ef3687460e8af8d7',\n",
       "  'lastModified': '2021-07-02T12:57:18.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:mini-an4',\n",
       "   'arxiv:1804.00015',\n",
       "   'espnet',\n",
       "   'audio',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'espnet',\n",
       "  'config': None,\n",
       "  'id': 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best',\n",
       "  'downloads': 5259,\n",
       "  'library_name': 'espnet',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['espnet', 'audio', 'automatic-speech-recognition'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['mini-an4'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/distilbart-mnli-12-9': {'modelId': 'valhalla/distilbart-mnli-12-9',\n",
       "  'sha': '66a037d826920a2f84a9d83edcbeb23a0951ed2e',\n",
       "  'lastModified': '2021-06-14T10:34:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-9',\n",
       "  'downloads': 5254,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sahri/indonesiasentiment': {'modelId': 'sahri/indonesiasentiment',\n",
       "  'sha': '99f38e6c1b34109bbf4a6d7c6556c56f5d2eef6a',\n",
       "  'lastModified': '2022-01-17T04:50:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'id',\n",
       "   'dataset:indonlu',\n",
       "   'arxiv:1907.11692',\n",
       "   'transformers',\n",
       "   'indonesian-roberta-base-sentiment-classifier',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sahri',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'sahri/indonesiasentiment',\n",
       "  'downloads': 5236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'tidak jelek tapi keren'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indonesian-roberta-base-sentiment-classifier'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['indonlu'],\n",
       "   'widget': [{'text': 'tidak jelek tapi keren'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-char': {'modelId': 'cl-tohoku/bert-base-japanese-char',\n",
       "  'sha': '6aa4c7bc39337858fee3e70f258edeada2e308ea',\n",
       "  'lastModified': '2021-09-23T13:45:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char',\n",
       "  'downloads': 5211,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰ªôÂè∞„ÅØ„Äå[MASK]„ÅÆÈÉΩ„Äç„Å®Âëº„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '‰ªôÂè∞„ÅØ„Äå[MASK]„ÅÆÈÉΩ„Äç„Å®Âëº„Å∞„Çå„Å¶„ÅÑ„Çã„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ThomasSimonini/t5-end2end-question-generation': {'modelId': 'ThomasSimonini/t5-end2end-question-generation',\n",
       "  'sha': '1dda3f93db6cfa1e7fc84e1208d0a49febb5fb5c',\n",
       "  'lastModified': '2021-10-10T08:30:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ThomasSimonini',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ThomasSimonini/t5-end2end-question-generation',\n",
       "  'downloads': 5197,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': {'error': 'Schema validation error. \"[0].results[0].metrics\" is required'},\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'datasets': ['squad'],\n",
       "   'model-index': [{'name': 't5-end2end-question-generation',\n",
       "     'results': [{'task': {'name': 'Sequence-to-sequence Language Modeling',\n",
       "        'type': 'text2text-generation'},\n",
       "       'dataset': {'name': 'squad',\n",
       "        'type': 'squad',\n",
       "        'args': 'plain_text'}}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-mpnet-base-v2': {'modelId': 'sentence-transformers/stsb-mpnet-base-v2',\n",
       "  'sha': '9f9d3d9da582d245066b519ab1e99c3f54a0594e',\n",
       "  'lastModified': '2021-08-05T08:31:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/stsb-mpnet-base-v2',\n",
       "  'downloads': 5179,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-hu-en': {'modelId': 'Helsinki-NLP/opus-mt-hu-en',\n",
       "  'sha': '62599d9d6eca96022d26974486779623f09ebc9c',\n",
       "  'lastModified': '2021-09-09T22:10:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'hu',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-hu-en',\n",
       "  'downloads': 5178,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/muppet-roberta-large': {'modelId': 'facebook/muppet-roberta-large',\n",
       "  'sha': '87df24857474bf92dc6789bf1e5a8d73bc7510cb',\n",
       "  'lastModified': '2021-06-28T21:44:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2101.11038',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'facebook/muppet-roberta-large',\n",
       "  'downloads': 5172,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'clue/albert_chinese_tiny': {'modelId': 'clue/albert_chinese_tiny',\n",
       "  'sha': '654acaf73c361ad56e4f4b1e2bb0023cbb1872b2',\n",
       "  'lastModified': '2020-12-11T21:35:55.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'zh', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'clue',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'clue/albert_chinese_tiny',\n",
       "  'downloads': 5169,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/all-roberta-large-v1': {'modelId': 'sentence-transformers/all-roberta-large-v1',\n",
       "  'sha': '42d37b9d8c9929c64dce4a2b25f6eaa0f59eaf99',\n",
       "  'lastModified': '2021-08-31T09:33:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/all-roberta-large-v1',\n",
       "  'downloads': 5152,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'laxya007/gpt2_bd2': {'modelId': 'laxya007/gpt2_bd2',\n",
       "  'sha': '92cf1b3a812d597077a02d3b734294bff30d0840',\n",
       "  'lastModified': '2022-03-19T13:11:52.000Z',\n",
       "  'tags': ['pytorch', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'laxya007',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'laxya007/gpt2_bd2',\n",
       "  'downloads': 5147,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DB13067/Peterbot': {'modelId': 'DB13067/Peterbot',\n",
       "  'sha': '8562a504120603feadd5d9c676ea3e3f8c5ff72b',\n",
       "  'lastModified': '2022-03-14T13:51:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'DB13067',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'DB13067/Peterbot',\n",
       "  'downloads': 5130,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/bart-large-chinese': {'modelId': 'fnlp/bart-large-chinese',\n",
       "  'sha': 'b47be247db39e74f5383784524c68bfddf0aa496',\n",
       "  'lastModified': '2021-10-29T05:19:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'Chinese',\n",
       "   'seq2seq'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/bart-large-chinese',\n",
       "  'downloads': 5129,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['text2text-generation', 'Chinese', 'seq2seq'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/test-two-configs': {'modelId': 'hf-internal-testing/test-two-configs',\n",
       "  'sha': 'f7afc8d394d28773381f2b9797624982c25c045d',\n",
       "  'lastModified': '2022-01-24T23:27:07.000Z',\n",
       "  'tags': ['bert', 'fill-mask', 'transformers', 'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/test-two-configs',\n",
       "  'downloads': 5126,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanfordnlp/stanza-en': {'modelId': 'stanfordnlp/stanza-en',\n",
       "  'sha': 'ca17c5a027df47832c797cadf6907b5d219811a1',\n",
       "  'lastModified': '2022-05-28T07:40:43.000Z',\n",
       "  'tags': ['en', 'stanza', 'token-classification', 'license:apache-2.0'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'stanfordnlp',\n",
       "  'config': None,\n",
       "  'id': 'stanfordnlp/stanza-en',\n",
       "  'downloads': 5102,\n",
       "  'library_name': 'stanza',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['stanza', 'token-classification'],\n",
       "   'library_name': 'stanza',\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uw-hai/polyjuice': {'modelId': 'uw-hai/polyjuice',\n",
       "  'sha': 'f5bef2f7053c2ce6c3fd19875c3cff77754479ef',\n",
       "  'lastModified': '2021-05-24T01:21:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'counterfactual generation'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uw-hai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'uw-hai/polyjuice',\n",
       "  'downloads': 5078,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'It is great for kids. <|perturb|> [negation] It [BLANK] great for kids. [SEP]'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['counterfactual generation'],\n",
       "   'widget': [{'text': 'It is great for kids. <|perturb|> [negation] It [BLANK] great for kids. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/bigscience-small-testing': {'modelId': 'bigscience/bigscience-small-testing',\n",
       "  'sha': '9db81994d104ea82084a364542406cc52d7863d7',\n",
       "  'lastModified': '2022-06-27T18:30:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bloom',\n",
       "   'feature-extraction',\n",
       "   'eng',\n",
       "   'transformers',\n",
       "   'integration',\n",
       "   'text-generation'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['BloomModel'], 'model_type': 'bloom'},\n",
       "  'id': 'bigscience/bigscience-small-testing',\n",
       "  'downloads': 5077,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['eng'],\n",
       "   'tags': ['integration'],\n",
       "   'pipeline_tag': 'text-generation'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-v3': {'modelId': 'sentence-transformers/msmarco-distilbert-base-v3',\n",
       "  'sha': 'cabff1a69c4aecef223c78fdb32c9f3fc4bda7dc',\n",
       "  'lastModified': '2022-06-15T21:45:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-v3',\n",
       "  'downloads': 5075,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es': {'modelId': 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es',\n",
       "  'sha': '617bf244e3106b6d50abfc600d62b858d798867d',\n",
       "  'lastModified': '2022-04-08T14:10:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:2109.03570',\n",
       "   'arxiv:2109.07765',\n",
       "   'transformers',\n",
       "   'biomedical',\n",
       "   'clinical',\n",
       "   'spanish',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'PlanTL-GOB-ES',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es',\n",
       "  'downloads': 5072,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'El √∫nico antecedente personal a rese√±ar era la <mask> arterial.'},\n",
       "   {'text': 'Las radiolog√≠as √≥seas de cuerpo entero no detectan alteraciones <mask>, ni alteraciones vertebrales.'},\n",
       "   {'text': 'En el <mask> toraco-abd√≥mino-p√©lvico no se encontraron hallazgos patol√≥gicos de inter√©s.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'],\n",
       "   'tags': ['biomedical', 'clinical', 'spanish'],\n",
       "   'license': 'apache-2.0',\n",
       "   'metrics': ['ppl'],\n",
       "   'widget': [{'text': 'El √∫nico antecedente personal a rese√±ar era la <mask> arterial.'},\n",
       "    {'text': 'Las radiolog√≠as √≥seas de cuerpo entero no detectan alteraciones <mask>, ni alteraciones vertebrales.'},\n",
       "    {'text': 'En el <mask> toraco-abd√≥mino-p√©lvico no se encontraron hallazgos patol√≥gicos de inter√©s.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'openai/clip-vit-base-patch16': {'modelId': 'openai/clip-vit-base-patch16',\n",
       "  'sha': '6cef4adda11be098f7c823c95de721298611f514',\n",
       "  'lastModified': '2022-03-14T18:00:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-base-patch16',\n",
       "  'downloads': 5064,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'human-centered-summarization/financial-summarization-pegasus': {'modelId': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "  'sha': 'a720f829427cb196a5618a0416473b8597cd106e',\n",
       "  'lastModified': '2022-06-29T06:25:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:xsum',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'human-centered-summarization',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "  'downloads': 5058,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'National Commercial Bank (NCB), Saudi Arabia‚Äôs largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba‚Äôs Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region‚Äôs third-largest lender. The entity‚Äôs $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East‚Äôs biggest lender with about $268 billion of assets.'}],\n",
       "  'likes': 20,\n",
       "  'model-index': [{'name': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'xsum',\n",
       "       'type': 'xsum',\n",
       "       'config': 'default',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 35.2055,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 16.5689,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 30.1285,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 30.1706,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 2.7092134952545166,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 15.1414,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': 'summarization',\n",
       "   'datasets': ['xsum'],\n",
       "   'metrics': ['rouge'],\n",
       "   'widget': [{'text': 'National Commercial Bank (NCB), Saudi Arabia‚Äôs largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba‚Äôs Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region‚Äôs third-largest lender. The entity‚Äôs $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East‚Äôs biggest lender with about $268 billion of assets.'}],\n",
       "   'model-index': [{'name': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'xsum',\n",
       "        'type': 'xsum',\n",
       "        'config': 'default',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 35.2055,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 16.5689,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 30.1285,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 30.1706,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 2.7092134952545166,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 15.1414,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco': {'modelId': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n",
       "  'sha': 'a03882cb371476b74ca3557366452cd868ac4f42',\n",
       "  'lastModified': '2021-04-15T08:54:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:ms_marco',\n",
       "   'arxiv:2104.06967',\n",
       "   'transformers',\n",
       "   'dpr',\n",
       "   'dense-passage-retrieval',\n",
       "   'knowledge-distillation'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sebastian-hofstaetter',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n",
       "  'downloads': 5056,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['dpr', 'dense-passage-retrieval', 'knowledge-distillation'],\n",
       "   'datasets': ['ms_marco']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/BERT-Tiny_L-2_H-128_A-2': {'modelId': 'nreimers/BERT-Tiny_L-2_H-128_A-2',\n",
       "  'sha': '9cb03776b08d300ae73aa6ba4860a760c606f62d',\n",
       "  'lastModified': '2021-05-28T11:05:21.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/BERT-Tiny_L-2_H-128_A-2',\n",
       "  'downloads': 5003,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/chinese_roberta_L-2_H-128': {'modelId': 'uer/chinese_roberta_L-2_H-128',\n",
       "  'sha': '3c66eff1fba8d47108e92380f9b7be15356b1774',\n",
       "  'lastModified': '2022-02-19T08:59:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'uer/chinese_roberta_L-2_H-128',\n",
       "  'downloads': 4996,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Âåó‰∫¨ÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': 'Âåó‰∫¨ÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'voidful/albert_chinese_tiny': {'modelId': 'voidful/albert_chinese_tiny',\n",
       "  'sha': 'd40f566a40f057e5d8a6f7b2cd5171a4f104126f',\n",
       "  'lastModified': '2021-08-03T05:07:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'voidful',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'voidful/albert_chinese_tiny',\n",
       "  'downloads': 4996,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '‰ªäÂ§©[MASK]ÊÉÖÂæàÂ•Ω'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': '‰ªäÂ§©[MASK]ÊÉÖÂæàÂ•Ω'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-uk-en': {'modelId': 'Helsinki-NLP/opus-mt-uk-en',\n",
       "  'sha': 'd6c1e62ab5c03e34a3d118382be7a27b704241f0',\n",
       "  'lastModified': '2021-09-11T10:51:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'uk',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-uk-en',\n",
       "  'downloads': 4980,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '–ú–µ–Ω–µ –∑–≤–∞—Ç–∏ –í–æ–ª—å—Ñ“ë–∞–Ω“ë —ñ —è –∂–∏–≤—É –≤ –ë–µ—Ä–ª—ñ–Ω—ñ.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-xlnet-base-cased': {'modelId': 'sshleifer/tiny-xlnet-base-cased',\n",
       "  'sha': '275d2c323ddd18dad60cd585934383c29027878b',\n",
       "  'lastModified': '2020-05-08T15:35:32.000Z',\n",
       "  'tags': ['pytorch', 'xlnet', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text_generation': {'max_length': 210}}},\n",
       "  'id': 'sshleifer/tiny-xlnet-base-cased',\n",
       "  'downloads': 4924,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/detr-resnet-50-panoptic': {'modelId': 'facebook/detr-resnet-50-panoptic',\n",
       "  'sha': 'fc15262cfd4c13cbdad6d1d55ff0cd31a2251a27',\n",
       "  'lastModified': '2022-06-27T08:30:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'image-segmentation',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-segmentation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForSegmentation'], 'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-50-panoptic',\n",
       "  'downloads': 4903,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg',\n",
       "    'example_title': 'Dog & Cat'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/construction-site.jpg',\n",
       "    'example_title': 'Construction Site'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/apple-orange.jpg',\n",
       "    'example_title': 'Apple & Orange'}],\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-segmentation', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg',\n",
       "     'example_title': 'Dog & Cat'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/construction-site.jpg',\n",
       "     'example_title': 'Construction Site'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/apple-orange.jpg',\n",
       "     'example_title': 'Apple & Orange'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageSegmentation',\n",
       "   'pipeline_tag': 'image-segmentation',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-en-fi': {'modelId': 'Helsinki-NLP/opus-mt-en-fi',\n",
       "  'sha': '627fe90df5c335be61521cd89c68f62e2bdce050',\n",
       "  'lastModified': '2021-09-09T21:35:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fi',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-fi',\n",
       "  'downloads': 4877,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanleychu2/t5-transition': {'modelId': 'stanleychu2/t5-transition',\n",
       "  'sha': '3188c7352c52d6ee172c050a3346f6a10bfd6635',\n",
       "  'lastModified': '2022-03-07T09:06:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stanleychu2',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'stanleychu2/t5-transition',\n",
       "  'downloads': 4876,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner': {'modelId': 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner',\n",
       "  'sha': '40b8059a6f3bfbb49d64038e131f49b93cc37417',\n",
       "  'lastModified': '2021-10-17T11:13:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'arxiv:2103.06678',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'CAMeL-Lab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner',\n",
       "  'downloads': 4798,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ•ŸÖÿßÿ±ÿ© ÿ£ÿ®Ÿàÿ∏ÿ®Ÿä ŸáŸä ÿ•ÿ≠ÿØŸâ ÿ•ŸÖÿßÿ±ÿßÿ™ ÿØŸàŸÑÿ© ÿßŸÑÿ•ŸÖÿßÿ±ÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ÿßŸÑÿ≥ÿ®ÿπ'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': 'ÿ•ŸÖÿßÿ±ÿ© ÿ£ÿ®Ÿàÿ∏ÿ®Ÿä ŸáŸä ÿ•ÿ≠ÿØŸâ ÿ•ŸÖÿßÿ±ÿßÿ™ ÿØŸàŸÑÿ© ÿßŸÑÿ•ŸÖÿßÿ±ÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ÿßŸÑÿ≥ÿ®ÿπ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-sequence-base': {'modelId': 'facebook/rag-sequence-base',\n",
       "  'sha': '7c7ae51878178639f47b6d416bef67a35a5a41f9',\n",
       "  'lastModified': '2020-12-11T21:39:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rag',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagSequenceForGeneration'],\n",
       "   'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-sequence-base',\n",
       "  'downloads': 4780,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagSequenceForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'patrickvonplaten/longformer-random-tiny': {'modelId': 'patrickvonplaten/longformer-random-tiny',\n",
       "  'sha': '8f15d46e686753d8c1ffb6e876fa90740a1c32c3',\n",
       "  'lastModified': '2020-08-05T09:22:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'longformer',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'patrickvonplaten',\n",
       "  'config': {'architectures': ['LongformerModel'], 'model_type': 'longformer'},\n",
       "  'id': 'patrickvonplaten/longformer-random-tiny',\n",
       "  'downloads': 4773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-MRPC': {'modelId': 'textattack/bert-base-uncased-MRPC',\n",
       "  'sha': 'd421614df8fbeb22d6826a24d6397809fdc1e3ff',\n",
       "  'lastModified': '2021-05-20T07:32:52.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-MRPC',\n",
       "  'downloads': 4772,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR': {'modelId': 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR',\n",
       "  'sha': '9e7c17b7f5ef120e895120c49721f3a000e5a240',\n",
       "  'lastModified': '2022-01-05T13:46:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'tprincessazula',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR',\n",
       "  'downloads': 4767,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli': {'modelId': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n",
       "  'sha': 'dada2f51640768682a07331e7c244c0d25e9dc85',\n",
       "  'lastModified': '2022-06-18T09:27:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta-v2',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'bg',\n",
       "   'de',\n",
       "   'el',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'hi',\n",
       "   'ru',\n",
       "   'sw',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'ur',\n",
       "   'vu',\n",
       "   'zh',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:xnli',\n",
       "   'arxiv:2111.09543',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'MoritzLaurer',\n",
       "  'config': {'architectures': ['DebertaV2ForSequenceClassification'],\n",
       "   'model_type': 'deberta-v2'},\n",
       "  'id': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n",
       "  'downloads': 4735,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU',\n",
       "    'candidate_labels': 'politics, economy, entertainment, environment'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'en',\n",
       "    'ar',\n",
       "    'bg',\n",
       "    'de',\n",
       "    'el',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'hi',\n",
       "    'ru',\n",
       "    'sw',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'ur',\n",
       "    'vu',\n",
       "    'zh'],\n",
       "   'tags': ['zero-shot-classification',\n",
       "    'text-classification',\n",
       "    'nli',\n",
       "    'pytorch'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'datasets': ['multi_nli', 'xnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU',\n",
       "     'candidate_labels': 'politics, economy, entertainment, environment'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/grappa_large_jnt': {'modelId': 'Salesforce/grappa_large_jnt',\n",
       "  'sha': '0d2500a00f3a4b71addaf09a0c1abe30788362dd',\n",
       "  'lastModified': '2021-05-20T12:23:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'Salesforce/grappa_large_jnt',\n",
       "  'downloads': 4727,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ImAPizza/DialoGPT-medium-alberttwo': {'modelId': 'ImAPizza/DialoGPT-medium-alberttwo',\n",
       "  'sha': 'bedcf2148b3c45ebc5c0c8632d41fe4f4cde1d9f',\n",
       "  'lastModified': '2021-08-29T13:39:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'ImAPizza',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'ImAPizza/DialoGPT-medium-alberttwo',\n",
       "  'downloads': 4723,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-MiniLM-L-6-v3': {'modelId': 'sentence-transformers/msmarco-MiniLM-L-6-v3',\n",
       "  'sha': '195276c0c8647b99dfe128bd8bc4ecd1a66d41f8',\n",
       "  'lastModified': '2022-06-15T21:52:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/msmarco-MiniLM-L-6-v3',\n",
       "  'downloads': 4710,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Parth/result': {'modelId': 'Parth/result',\n",
       "  'sha': 'b06ca90f3eda5e5ee4e1dc3a55714e7cb5ffcc00',\n",
       "  'lastModified': '2021-06-23T03:47:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Parth',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Parth/result',\n",
       "  'downloads': 4704,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-gram-zh': {'modelId': 'nghuyong/ernie-gram-zh',\n",
       "  'sha': '257fee0915f1cba8dbea92c976493dcdd0491174',\n",
       "  'lastModified': '2022-04-04T06:00:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2010.12148',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-gram-zh',\n",
       "  'downloads': 4698,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-ctx_encoder-multiset-base': {'modelId': 'facebook/dpr-ctx_encoder-multiset-base',\n",
       "  'sha': '6c01adf9e9e7c812c0fa998fed97eec3262c2cf4',\n",
       "  'lastModified': '2020-11-25T16:58:57.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-ctx_encoder-multiset-base',\n",
       "  'downloads': 4688,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialogRPT-updown': {'modelId': 'microsoft/DialogRPT-updown',\n",
       "  'sha': 'afe1247fd7e1b3abea28a52ea72db4ce1c8d2186',\n",
       "  'lastModified': '2021-05-23T09:19:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-classification',\n",
       "   'arxiv:2009.06978',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2ForSequenceClassification'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'microsoft/DialogRPT-updown',\n",
       "  'downloads': 4686,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hakurei/lit-6B': {'modelId': 'hakurei/lit-6B',\n",
       "  'sha': 'cc2e78adb62590cb2889d338d46f8cf1ef396453',\n",
       "  'lastModified': '2021-11-08T23:02:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'causal-lm',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hakurei',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'temperature': 1,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'hakurei/lit-6B',\n",
       "  'downloads': 4663,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['pytorch', 'causal-lm'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-sla-en': {'modelId': 'Helsinki-NLP/opus-mt-sla-en',\n",
       "  'sha': 'ad888a10d15c0cbe1e45b94c18e260fdc19035f7',\n",
       "  'lastModified': '2020-08-21T14:42:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'be',\n",
       "   'hr',\n",
       "   'mk',\n",
       "   'cs',\n",
       "   'ru',\n",
       "   'pl',\n",
       "   'bg',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'sla',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-sla-en',\n",
       "  'downloads': 4658,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['be',\n",
       "    'hr',\n",
       "    'mk',\n",
       "    'cs',\n",
       "    'ru',\n",
       "    'pl',\n",
       "    'bg',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'sla',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa': {'modelId': 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa',\n",
       "  'sha': '829047a5ce1a8ef73bde97666eff10b8e128b42e',\n",
       "  'lastModified': '2021-05-20T00:24:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa',\n",
       "  'downloads': 4610,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Dove vivo?',\n",
       "    'context': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Dove vivo?', 'context': 'Mi chiamo Sarah e vivo a Londra'},\n",
       "   {'text': 'Come mio chiamo?',\n",
       "    'context': 'Mi chiamo Clara e vivo a Berkeley.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/sentence-bert-base-ja-mean-tokens-v2': {'modelId': 'sonoisa/sentence-bert-base-ja-mean-tokens-v2',\n",
       "  'sha': 'a230680fdb31ed495808f08e2d700361dc982542',\n",
       "  'lastModified': '2021-12-26T08:33:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'sentence-transformers',\n",
       "   'sentence-bert',\n",
       "   'sentence-similarity',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sonoisa/sentence-bert-base-ja-mean-tokens-v2',\n",
       "  'downloads': 4589,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'sentence-bert',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'blanchefort/rubert-base-cased-sentiment': {'modelId': 'blanchefort/rubert-base-cased-sentiment',\n",
       "  'sha': '1dfb5bcf1904a12eb157a0dfaf06029e606ce7c7',\n",
       "  'lastModified': '2021-05-19T13:05:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'sentiment'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'blanchefort',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'blanchefort/rubert-base-cased-sentiment',\n",
       "  'downloads': 4583,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '–¢—ã –º–Ω–µ –Ω—Ä–∞–≤–∏—à—å—Å—è. –Ø —Ç–µ–±—è –ª—é–±–ª—é'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['sentiment', 'text-classification']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deep-learning-analytics/GrammarCorrector': {'modelId': 'deep-learning-analytics/GrammarCorrector',\n",
       "  'sha': '6ca90bd771c373a0542d4257a5c34d26cd0d3c59',\n",
       "  'lastModified': '2021-12-23T02:51:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'deep-learning-analytics',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'deep-learning-analytics/GrammarCorrector',\n",
       "  'downloads': 4571,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-pert-base': {'modelId': 'hfl/chinese-pert-base',\n",
       "  'sha': '54f84f9b553c9184d92e1d476010299aac42cf86',\n",
       "  'lastModified': '2022-02-24T02:57:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-pert-base',\n",
       "  'downloads': 4565,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-th-en': {'modelId': 'Helsinki-NLP/opus-mt-th-en',\n",
       "  'sha': '90080f69e69c567e2b145fc8723c1e53f4f760e6',\n",
       "  'lastModified': '2020-08-21T14:42:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'th',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-th-en',\n",
       "  'downloads': 4548,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['th', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/muppet-roberta-base': {'modelId': 'facebook/muppet-roberta-base',\n",
       "  'sha': 'caf238c63db946bdfbd00575713462838e823997',\n",
       "  'lastModified': '2021-06-28T21:44:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2101.11038',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'facebook/muppet-roberta-base',\n",
       "  'downloads': 4543,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-klej-cased-tokenizer-v1': {'modelId': 'allegro/herbert-klej-cased-tokenizer-v1',\n",
       "  'sha': '22f03e76fc129ea8f15a92e7213d5beeba83f4c8',\n",
       "  'lastModified': '2021-05-28T16:19:05.000Z',\n",
       "  'tags': ['xlm', 'pl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'model_type': 'xlm'},\n",
       "  'id': 'allegro/herbert-klej-cased-tokenizer-v1',\n",
       "  'downloads': 4505,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'asi/gpt-fr-cased-small': {'modelId': 'asi/gpt-fr-cased-small',\n",
       "  'sha': 'd4ddd1d506690415df78683829f5aba3878888a3',\n",
       "  'lastModified': '2021-06-30T13:47:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'text-generation',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'asi',\n",
       "  'config': {'model_type': 'gpt2'},\n",
       "  'id': 'asi/gpt-fr-cased-small',\n",
       "  'downloads': 4500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"Mon nom est Julien et j'aime\"},\n",
       "   {'text': 'Mon nom est Thomas et mon principal'},\n",
       "   {'text': 'Il √©tait une fois'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'tags': ['tf', 'pytorch', 'gpt2', 'text-generation'],\n",
       "   'license': 'apache-2.0',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'zenham/wail_m_e4_16h_2k': {'modelId': 'zenham/wail_m_e4_16h_2k',\n",
       "  'sha': '5ac007e21763708150470814fc800ab1e72c9582',\n",
       "  'lastModified': '2022-03-10T03:06:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'zenham',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'zenham/wail_m_e4_16h_2k',\n",
       "  'downloads': 4493,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'akreal/tiny-random-bert': {'modelId': 'akreal/tiny-random-bert',\n",
       "  'sha': '843b6aea20ebae1c96598b6187b1bc26c105652a',\n",
       "  'lastModified': '2021-08-18T14:42:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'akreal',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'akreal/tiny-random-bert',\n",
       "  'downloads': 4477,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'abjbpi/Dwight_Schrute': {'modelId': 'abjbpi/Dwight_Schrute',\n",
       "  'sha': '451aab582fe08f5210a58859f9ec1c79278e341b',\n",
       "  'lastModified': '2021-06-04T11:43:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'abjbpi',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'abjbpi/Dwight_Schrute',\n",
       "  'downloads': 4473,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt': {'modelId': 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt',\n",
       "  'sha': 'cf511f7e00de089e67e80fbedd5fbfb8e76ea067',\n",
       "  'lastModified': '2021-04-01T14:09:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'ydshieh',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt',\n",
       "  'downloads': 4473,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Large 53 - Chinese (zh-CN), by Yih-Dar SHIEH',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-CN',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-CN'},\n",
       "      'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 20.9}]}]}],\n",
       "  'cardData': {'language': 'zh',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Large 53 - Chinese (zh-CN), by Yih-Dar SHIEH',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-CN',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-CN'},\n",
       "       'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 20.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'benjamin/gerpt2-large': {'modelId': 'benjamin/gerpt2-large',\n",
       "  'sha': 'd0ec9b299d7a96e24d03303de120ffb81769f366',\n",
       "  'lastModified': '2022-05-11T09:16:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'benjamin',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'benjamin/gerpt2-large',\n",
       "  'downloads': 4459,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einh√∂rner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einh√∂rner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-char-v2': {'modelId': 'cl-tohoku/bert-base-japanese-char-v2',\n",
       "  'sha': 'e17e40a15857ad47d63f6eb4cc9fb62c136d2301',\n",
       "  'lastModified': '2021-09-23T13:45:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char-v2',\n",
       "  'downloads': 4453,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': 'Êù±ÂåóÂ§ßÂ≠¶„Åß[MASK]„ÅÆÁ†îÁ©∂„Çí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/flava-full': {'modelId': 'facebook/flava-full',\n",
       "  'sha': '57949b6a84a80fd01c3dd62a09450d8670f1c418',\n",
       "  'lastModified': '2022-05-25T07:53:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flava',\n",
       "   'pretraining',\n",
       "   'arxiv:2112.04482',\n",
       "   'arxiv:2108.10904',\n",
       "   'transformers',\n",
       "   'license:bsd-3-clause'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FlavaForPreTraining'], 'model_type': 'flava'},\n",
       "  'id': 'facebook/flava-full',\n",
       "  'downloads': 4451,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'bsd-3-clause'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'hfl/chinese-xlnet-base': {'modelId': 'hfl/chinese-xlnet-base',\n",
       "  'sha': '34b827684078f956411389834966eb55588f5254',\n",
       "  'lastModified': '2021-03-03T01:44:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'], 'model_type': 'xlnet'},\n",
       "  'id': 'hfl/chinese-xlnet-base',\n",
       "  'downloads': 4450,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Êú±Âà©ÂÆâÔºåÊàëÂñúÊ¨¢'},\n",
       "   {'text': 'ÊàëÂè´ÊâòÈ©¨ÊñØÔºåÊàëÁöÑ‰∏ªË¶Å'},\n",
       "   {'text': 'ÊàëÂè´Áéõ‰∏Ω‰∫öÔºåÊàëÊúÄÂñúÊ¨¢ÁöÑ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàëÊòØ'},\n",
       "   {'text': '‰ªéÂâçÔºå'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Zixtrauce/BaekBot': {'modelId': 'Zixtrauce/BaekBot',\n",
       "  'sha': '8dd7cbc7e2f1f6d711330aabb4a165d87a8dc7f5',\n",
       "  'lastModified': '2021-12-31T08:30:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/BaekBot',\n",
       "  'downloads': 4439,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/xtremedistil-l6-h384-uncased': {'modelId': 'microsoft/xtremedistil-l6-h384-uncased',\n",
       "  'sha': '359df7d52613d4edc15647e6d65e0d87200eb747',\n",
       "  'lastModified': '2021-08-05T17:48:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2106.04563',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/xtremedistil-l6-h384-uncased',\n",
       "  'downloads': 4434,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aware-ai/roberta-large-squadv2': {'modelId': 'aware-ai/roberta-large-squadv2',\n",
       "  'sha': '59a93e1104aa42295190ecec42bf829fbc83b0bb',\n",
       "  'lastModified': '2021-05-20T12:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'aware-ai',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'aware-ai/roberta-large-squadv2',\n",
       "  'downloads': 4421,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stas/t5-very-small-random': {'modelId': 'stas/t5-very-small-random',\n",
       "  'sha': '988f491d1f2b837d47895885d96b1d4992a25d0e',\n",
       "  'lastModified': '2021-04-21T02:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stas',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'stas/t5-very-small-random',\n",
       "  'downloads': 4417,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-large-lm-adapt': {'modelId': 'google/t5-large-lm-adapt',\n",
       "  'sha': '96ce18564557a62d6ff1cb3771af167433827961',\n",
       "  'lastModified': '2021-11-01T14:00:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-large-lm-adapt',\n",
       "  'downloads': 4415,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli': {'modelId': 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
       "  'sha': 'e6331eecd7f50dd13c8ebcdc57a9f0a22f2ff56e',\n",
       "  'lastModified': '2021-09-30T11:27:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'ar',\n",
       "   'bg',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'ru',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'ur',\n",
       "   'vn',\n",
       "   'zh',\n",
       "   'dataset:SNLI',\n",
       "   'dataset:MNLI',\n",
       "   'dataset:ANLI',\n",
       "   'dataset:XNLI',\n",
       "   'sentence-transformers',\n",
       "   'zero-shot-classification',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'symanto',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
       "  'downloads': 4415,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'Ÿáÿ∞ÿß ÿ¥ÿÆÿµ ÿ≥ÿπŸäÿØ',\n",
       "    'sentences': ['Ÿáÿ∞ÿß ŸÉŸÑÿ® ÿ≥ÿπŸäÿØ', 'Ÿáÿ∞ÿß ÿ¥ÿÆÿµ ÿ≥ÿπŸäÿØ ÿ¨ÿØÿß', 'ÿßŸÑŸäŸàŸÖ ŸáŸà ŸäŸàŸÖ ŸÖÿ¥ŸÖÿ≥']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'bg',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'ru',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'ur',\n",
       "    'vn',\n",
       "    'zh'],\n",
       "   'datasets': ['SNLI', 'MNLI', 'ANLI', 'XNLI'],\n",
       "   'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['zero-shot-classification',\n",
       "    'sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Huffon/sentence-klue-roberta-base': {'modelId': 'Huffon/sentence-klue-roberta-base',\n",
       "  'sha': 'a5aca746f7931205aa44992e81fdeb7faf7c443c',\n",
       "  'lastModified': '2021-06-20T17:32:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'dataset:klue',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'Huffon',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'Huffon/sentence-klue-roberta-base',\n",
       "  'downloads': 4410,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['roberta', 'sentence-transformers'],\n",
       "   'datasets': ['klue']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ca-en': {'modelId': 'Helsinki-NLP/opus-mt-ca-en',\n",
       "  'sha': '22113f5e0e8e89677d6e0142e55c85402eecb455',\n",
       "  'lastModified': '2021-09-09T21:28:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ca-en',\n",
       "  'downloads': 4380,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'OFA-Sys/OFA-base': {'modelId': 'OFA-Sys/OFA-base',\n",
       "  'sha': '965ce12f22b00caeb1a45b9f69559c4818ab3fd1',\n",
       "  'lastModified': '2022-05-03T06:03:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'ofa',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'OFA-Sys',\n",
       "  'config': {'architectures': ['OFAForConditionalGeneration'],\n",
       "   'model_type': 'ofa'},\n",
       "  'id': 'OFA-Sys/OFA-base',\n",
       "  'downloads': 4351,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation'}},\n",
       " 'nlpconnect/roberta-base-squad2-nq': {'modelId': 'nlpconnect/roberta-base-squad2-nq',\n",
       "  'sha': 'b925b1c7e676c8b46fb103307fc9f065bd46f392',\n",
       "  'lastModified': '2021-08-31T03:56:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'dataset:natural_questions',\n",
       "   'transformers',\n",
       "   'qa',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'nlpconnect',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'nlpconnect/roberta-base-squad2-nq',\n",
       "  'downloads': 4348,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['qa'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['squad_v2', 'natural_questions']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csebuetnlp/banglabert': {'modelId': 'csebuetnlp/banglabert',\n",
       "  'sha': '7bed1e381af5564564faadc9718f25c6116491e0',\n",
       "  'lastModified': '2022-05-10T05:17:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'bn',\n",
       "   'arxiv:2101.00204',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'csebuetnlp',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'csebuetnlp/banglabert',\n",
       "  'downloads': 4319,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['bn'], 'licenses': ['cc-by-nc-sa-4.0']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'savasy/bert-base-turkish-sentiment-cased': {'modelId': 'savasy/bert-base-turkish-sentiment-cased',\n",
       "  'sha': '330ec37b18140dcd5c5dd6357d59463ae9deb2e0',\n",
       "  'lastModified': '2021-05-20T04:55:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'tr',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'savasy',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'savasy/bert-base-turkish-sentiment-cased',\n",
       "  'downloads': 4285,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/cino-large': {'modelId': 'hfl/cino-large',\n",
       "  'sha': '03c0611c2dd4b1e82eece4a6ff964510615f2eab',\n",
       "  'lastModified': '2022-01-24T09:28:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'bo',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'mn',\n",
       "   'ug',\n",
       "   'yue',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'hfl/cino-large',\n",
       "  'downloads': 4279,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Â∑¥ÈªéÊòØ<mask>ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}, {'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ<mask>„ÄÇ'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh', 'bo', 'kk', 'ko', 'mn', 'ug', 'yue'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/chinese_roberta_L-4_H-512': {'modelId': 'uer/chinese_roberta_L-4_H-512',\n",
       "  'sha': '45bff68199891d98fdeee7b222f2d4d35912a46a',\n",
       "  'lastModified': '2022-02-19T09:11:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'uer/chinese_roberta_L-4_H-512',\n",
       "  'downloads': 4258,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Âåó‰∫¨ÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': 'Âåó‰∫¨ÊòØ[MASK]ÂõΩÁöÑÈ¶ñÈÉΩ„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-klej-cased-v1': {'modelId': 'allegro/herbert-klej-cased-v1',\n",
       "  'sha': '6953ff83476f8e7a4afb4131cb629c0cffde6c9e',\n",
       "  'lastModified': '2021-05-28T16:18:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'pl',\n",
       "   'arxiv:2005.00630',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'allegro/herbert-klej-cased-v1',\n",
       "  'downloads': 4237,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deutsche-telekom/bert-multi-english-german-squad2': {'modelId': 'deutsche-telekom/bert-multi-english-german-squad2',\n",
       "  'sha': '7d4c38391cca9950a1bcee19ecdbe287d8ceffb4',\n",
       "  'lastModified': '2021-07-14T13:17:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'english',\n",
       "   'german',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deutsche-telekom',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deutsche-telekom/bert-multi-english-german-squad2',\n",
       "  'downloads': 4225,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den gr√∂√üten Teil des Amazonas-Beckens S√ºdamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der gr√∂√üte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Franz√∂sisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repr√§sentiert mehr als die H√§lfte der verbleibenden Regenw√§lder des Planeten und umfasst den gr√∂√üten und artenreichsten tropischen Regenwald der Welt mit gesch√§tzten 390 Milliarden Einzelb√§umen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['english', 'german']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hivemind/gpt-j-6B-8bit': {'modelId': 'hivemind/gpt-j-6B-8bit',\n",
       "  'sha': '636b67ff80cb47e083bdfd8074c45857f72cac65',\n",
       "  'lastModified': '2022-02-10T23:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'arxiv:2106.09685',\n",
       "   'arxiv:2110.02861',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hivemind',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 1}}},\n",
       "  'id': 'hivemind/gpt-j-6B-8bit',\n",
       "  'downloads': 4215,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 58,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_base_uncased': {'modelId': 'flaubert/flaubert_base_uncased',\n",
       "  'sha': '56ea0bf6e54b59c192f99f2397e932a9915cae4c',\n",
       "  'lastModified': '2021-10-18T08:14:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'flaubert-base',\n",
       "   'uncased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_base_uncased',\n",
       "  'downloads': 4204,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'flaubert-base',\n",
       "    'uncased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-base-trivia-itc': {'modelId': 'google/bigbird-base-trivia-itc',\n",
       "  'sha': '29c5c29e0297ad7eb9b90ef69fecba71508f5ca4',\n",
       "  'lastModified': '2021-06-02T14:53:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'big_bird',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:trivia_qa',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdForQuestionAnswering'],\n",
       "   'model_type': 'big_bird'},\n",
       "  'id': 'google/bigbird-base-trivia-itc',\n",
       "  'downloads': 4203,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['trivia_qa']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/tk-instruct-base-def-pos': {'modelId': 'allenai/tk-instruct-base-def-pos',\n",
       "  'sha': '196e8998944bded8e53c6fe3a757a905a3d5382f',\n",
       "  'lastModified': '2022-05-27T06:30:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:natural instructions v2.0',\n",
       "   'arxiv:1910.10683',\n",
       "   'arxiv:2204.07705',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'allenai/tk-instruct-base-def-pos',\n",
       "  'downloads': 4202,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['natural instructions v2.0']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12': {'modelId': 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'a0ce2a0e0feb8f4caf0346b139266f5320b90322',\n",
       "  'lastModified': '2021-09-24T07:45:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:pubmed',\n",
       "   'transformers',\n",
       "   'bluebert',\n",
       "   'license:cc0-1.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'bionlp',\n",
       "  'config': {},\n",
       "  'id': 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 4179,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['bluebert'],\n",
       "   'license': 'cc0-1.0',\n",
       "   'datasets': ['pubmed']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hf-internal-testing/tiny-albert': {'modelId': 'hf-internal-testing/tiny-albert',\n",
       "  'sha': 'de839e2e6aa81798e2ac85a8ac414da41862e23a',\n",
       "  'lastModified': '2021-07-16T01:27:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'hf-internal-testing/tiny-albert',\n",
       "  'downloads': 4173,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EMBEDDIA/crosloengual-bert': {'modelId': 'EMBEDDIA/crosloengual-bert',\n",
       "  'sha': '750255b6915cf42623143690d8ea79ceab8ee2e8',\n",
       "  'lastModified': '2021-05-18T18:21:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'hr',\n",
       "   'sl',\n",
       "   'en',\n",
       "   'multilingual',\n",
       "   'arxiv:2006.07890',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'EMBEDDIA',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'EMBEDDIA/crosloengual-bert',\n",
       "  'downloads': 4169,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['hr', 'sl', 'en', 'multilingual'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepChem/ChemBERTa-77M-MLM': {'modelId': 'DeepChem/ChemBERTa-77M-MLM',\n",
       "  'sha': 'ed8a5374f2024ec8da53760af91a33fb8f6a15ff',\n",
       "  'lastModified': '2022-01-20T18:02:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'DeepChem',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'DeepChem/ChemBERTa-77M-MLM',\n",
       "  'downloads': 4166,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/convnext-tiny-224': {'modelId': 'facebook/convnext-tiny-224',\n",
       "  'sha': '0d1c8dedaa107d4ae537c5b10e5cd0a8c865e84e',\n",
       "  'lastModified': '2022-02-26T12:15:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'convnext',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2201.03545',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ConvNextForImageClassification'],\n",
       "   'model_type': 'convnext'},\n",
       "  'id': 'facebook/convnext-tiny-224',\n",
       "  'downloads': 4163,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'flair/ner-english-ontonotes': {'modelId': 'flair/ner-english-ontonotes',\n",
       "  'sha': '4e50d09d85d60fd36e2c78175d4e405b1e3caa8c',\n",
       "  'lastModified': '2021-03-02T22:07:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes',\n",
       "  'downloads': 4149,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George Washington won 1 dollar.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George Washington won 1 dollar.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uer/roberta-base-finetuned-dianping-chinese': {'modelId': 'uer/roberta-base-finetuned-dianping-chinese',\n",
       "  'sha': '9498566e5da5b6cdc52f8eea002be9c24aae959a',\n",
       "  'lastModified': '2022-02-20T07:57:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1708.02657',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'uer/roberta-base-finetuned-dianping-chinese',\n",
       "  'downloads': 4132,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ËøôÊú¨‰π¶ÁúüÁöÑÂæà‰∏çÈîô'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh', 'widget': [{'text': 'ËøôÊú¨‰π¶ÁúüÁöÑÂæà‰∏çÈîô'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-large-finetuned-tacred': {'modelId': 'studio-ousia/luke-large-finetuned-tacred',\n",
       "  'sha': 'ba3d02d7791d738d6bd480592ed814525124fbbc',\n",
       "  'lastModified': '2022-03-23T12:31:16.000Z',\n",
       "  'tags': ['pytorch', 'luke', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForEntityPairClassification'],\n",
       "   'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-large-finetuned-tacred',\n",
       "  'downloads': 4112,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'LukeForEntityPairClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jeniya/BERTOverflow': {'modelId': 'jeniya/BERTOverflow',\n",
       "  'sha': '0361ca9842fd3f88b2e4eea1626d56c2e1265bce',\n",
       "  'lastModified': '2021-05-19T20:47:17.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'jeniya',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'jeniya/BERTOverflow',\n",
       "  'downloads': 4107,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-e2e-qg': {'modelId': 'valhalla/t5-small-e2e-qg',\n",
       "  'sha': 'feec82746b18ab037724c14f11277f320bd73920',\n",
       "  'lastModified': '2021-07-30T13:10:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '}}},\n",
       "  'id': 'valhalla/t5-small-e2e-qg',\n",
       "  'downloads': 4104,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Python is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'Python is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/gpt2-base-chinese': {'modelId': 'ckiplab/gpt2-base-chinese',\n",
       "  'sha': '5b35975016d00703e7d812b9197ea81f295b65c3',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'ckiplab/gpt2-base-chinese',\n",
       "  'downloads': 4091,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Êú±Âà©ÂÆâÔºåÊàëÂñúÊ¨¢'},\n",
       "   {'text': 'ÊàëÂè´ÊâòÈ©¨ÊñØÔºåÊàëÁöÑ‰∏ªË¶Å'},\n",
       "   {'text': 'ÊàëÂè´Áéõ‰∏Ω‰∫öÔºåÊàëÊúÄÂñúÊ¨¢ÁöÑ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàëÊòØ'},\n",
       "   {'text': '‰ªéÂâçÔºå'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'gpt2', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shibing624/text2vec-base-chinese': {'modelId': 'shibing624/text2vec-base-chinese',\n",
       "  'sha': 'b455bb011898ad5d8b16cea238d070cd34db4b05',\n",
       "  'lastModified': '2022-03-14T06:43:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'text2vec',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'shibing624',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'shibing624/text2vec-base-chinese',\n",
       "  'downloads': 4076,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['text2vec',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Norod78/hebrew-bad_wiki-gpt_neo-tiny': {'modelId': 'Norod78/hebrew-bad_wiki-gpt_neo-tiny',\n",
       "  'sha': '3429083d9673b269efa98b256d3c84e040e97a8f',\n",
       "  'lastModified': '2022-07-04T07:25:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'he',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Norod78',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'], 'model_type': 'gpt_neo'},\n",
       "  'id': 'Norod78/hebrew-bad_wiki-gpt_neo-tiny',\n",
       "  'downloads': 4044,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '◊û◊™◊û◊ò◊ô◊ß◊î:'},\n",
       "   {'text': '◊¢◊ú◊ô◊ô◊™ ◊î◊û◊õ◊ï◊†◊ï◊™'},\n",
       "   {'text': '◊ï◊ô◊ß◊ô◊§◊ì◊ô◊î ◊î◊¢◊ë◊®◊ô◊™'},\n",
       "   {'text': '◊î◊ê◊ô◊®◊ï◊ï◊ô◊ñ◊ô◊ï◊ü ◊î◊ï◊ê'},\n",
       "   {'text': '◊ì◊ï◊ì ◊ë◊ü-◊í◊ï◊®◊ô◊ï◊ü ◊î◊ô◊î'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'he',\n",
       "   'thumbnail': 'https://avatars1.githubusercontent.com/u/3617152?norod.jpg',\n",
       "   'widget': [{'text': '◊û◊™◊û◊ò◊ô◊ß◊î:'},\n",
       "    {'text': '◊¢◊ú◊ô◊ô◊™ ◊î◊û◊õ◊ï◊†◊ï◊™'},\n",
       "    {'text': '◊ï◊ô◊ß◊ô◊§◊ì◊ô◊î ◊î◊¢◊ë◊®◊ô◊™'},\n",
       "    {'text': '◊î◊ê◊ô◊®◊ï◊ï◊ô◊ñ◊ô◊ï◊ü ◊î◊ï◊ê'},\n",
       "    {'text': '◊ì◊ï◊ì ◊ë◊ü-◊í◊ï◊®◊ô◊ï◊ü ◊î◊ô◊î'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Zixtrauce/BDBot4Epoch': {'modelId': 'Zixtrauce/BDBot4Epoch',\n",
       "  'sha': '77357067c689ccb8c19220a32137eb8646bf87e5',\n",
       "  'lastModified': '2022-01-01T23:46:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/BDBot4Epoch',\n",
       "  'downloads': 4040,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilroberta-base-paraphrase-v1': {'modelId': 'sentence-transformers/distilroberta-base-paraphrase-v1',\n",
       "  'sha': '0191e446424b49506ba016264788b49bb7b11eb9',\n",
       "  'lastModified': '2022-06-15T21:53:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/distilroberta-base-paraphrase-v1',\n",
       "  'downloads': 4029,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-13b': {'modelId': 'facebook/opt-13b',\n",
       "  'sha': '45d913414643f29e9273a362ef881109c36b72a5',\n",
       "  'lastModified': '2022-06-24T05:21:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-13b',\n",
       "  'downloads': 4013,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['opt', 'text-generation'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/distilbert-base-cased-CoLA': {'modelId': 'textattack/distilbert-base-cased-CoLA',\n",
       "  'sha': '73fd8dc841293aab1caea98581bb57481c87ff55',\n",
       "  'lastModified': '2020-06-09T16:45:43.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'textattack/distilbert-base-cased-CoLA',\n",
       "  'downloads': 4009,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'setu4993/LaBSE': {'modelId': 'setu4993/LaBSE',\n",
       "  'sha': '082cccca1eea3e0fab80749de8e8aded21bec253',\n",
       "  'lastModified': '2021-12-05T06:10:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'bo',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'rw',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tk',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'tt',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:CommonCrawl',\n",
       "   'dataset:Wikipedia',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'multilingual',\n",
       "   'google',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'setu4993',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'setu4993/LaBSE',\n",
       "  'downloads': 4000,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'bo',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'rw',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tk',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'tt',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'tags': ['bert',\n",
       "    'sentence_embedding',\n",
       "    'multilingual',\n",
       "    'google',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['CommonCrawl', 'Wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/BERT-Banking77': {'modelId': 'philschmid/BERT-Banking77',\n",
       "  'sha': 'e08d5e191921b9e0713327dc7e29293ecb286043',\n",
       "  'lastModified': '2022-06-24T14:31:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:banking77',\n",
       "   'transformers',\n",
       "   'autotrain',\n",
       "   'model-index',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'philschmid/BERT-Banking77',\n",
       "  'downloads': 3994,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I am still waiting on my card?'}],\n",
       "  'model-index': [{'name': 'BERT-Banking77',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'BANKING77', 'type': 'banking77'},\n",
       "      'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 92.64},\n",
       "       {'name': 'Macro F1', 'type': 'macro-f1', 'value': 92.64},\n",
       "       {'name': 'Weighted F1', 'type': 'weighted-f1', 'value': 92.6}]},\n",
       "     {'task': {'type': 'text-classification', 'name': 'Text Classification'},\n",
       "      'dataset': {'name': 'banking77',\n",
       "       'type': 'banking77',\n",
       "       'config': 'default',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Macro',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9305185253845069,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Micro',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Weighted',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9305185253845071,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Macro',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974028,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Micro',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Weighted',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Macro',\n",
       "        'type': 'f1',\n",
       "        'value': 0.927623314966026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Micro',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Weighted',\n",
       "        'type': 'f1',\n",
       "        'value': 0.927623314966026,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.3199225962162018,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'tags': 'autotrain',\n",
       "   'language': 'en',\n",
       "   'widget': [{'text': 'I am still waiting on my card?'}],\n",
       "   'datasets': ['banking77'],\n",
       "   'model-index': [{'name': 'BERT-Banking77',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'BANKING77', 'type': 'banking77'},\n",
       "       'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 92.64},\n",
       "        {'name': 'Macro F1', 'type': 'macro-f1', 'value': 92.64},\n",
       "        {'name': 'Weighted F1', 'type': 'weighted-f1', 'value': 92.6}]},\n",
       "      {'task': {'type': 'text-classification', 'name': 'Text Classification'},\n",
       "       'dataset': {'name': 'banking77',\n",
       "        'type': 'banking77',\n",
       "        'config': 'default',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Macro',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9305185253845069,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Micro',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Weighted',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9305185253845071,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Macro',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974028,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Micro',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Weighted',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Macro',\n",
       "         'type': 'f1',\n",
       "         'value': 0.927623314966026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Micro',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Weighted',\n",
       "         'type': 'f1',\n",
       "         'value': 0.927623314966026,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.3199225962162018,\n",
       "         'verified': True}]}]}],\n",
       "   'co2_eq_emissions': 0.03330651014155927},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepESP/gpt2-spanish': {'modelId': 'DeepESP/gpt2-spanish',\n",
       "  'sha': '1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e',\n",
       "  'lastModified': '2021-10-19T08:52:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'es',\n",
       "   'dataset:ebooks',\n",
       "   'transformers',\n",
       "   'GPT-2',\n",
       "   'Spanish',\n",
       "   'ebooks',\n",
       "   'nlg',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'DeepESP',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'DeepESP/gpt2-spanish',\n",
       "  'downloads': 3987,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Quisiera saber que va a suceder'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'tags': ['GPT-2', 'Spanish', 'ebooks', 'nlg'],\n",
       "   'datasets': ['ebooks'],\n",
       "   'widget': [{'text': 'Quisiera saber que va a suceder'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/TinyBERT_L-4_H-312_v2': {'modelId': 'nreimers/TinyBERT_L-4_H-312_v2',\n",
       "  'sha': 'd782507ee95c6565fe5924fcd6090999055e8db6',\n",
       "  'lastModified': '2021-05-28T11:02:32.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/TinyBERT_L-4_H-312_v2',\n",
       "  'downloads': 3984,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-unispeech-sat': {'modelId': 'hf-internal-testing/tiny-random-unispeech-sat',\n",
       "  'sha': '4a779774d9473a62b9436b87cf9ac885b97e6f16',\n",
       "  'lastModified': '2022-01-26T13:47:21.000Z',\n",
       "  'tags': ['pytorch', 'unispeech-sat', 'audio-classification', 'transformers'],\n",
       "  'pipeline_tag': 'audio-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['UniSpeechSatForSequenceClassification'],\n",
       "   'model_type': 'unispeech-sat'},\n",
       "  'id': 'hf-internal-testing/tiny-random-unispeech-sat',\n",
       "  'downloads': 3952,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForAudioClassification',\n",
       "   'pipeline_tag': 'audio-classification',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'enelpol/poleval2021-task3': {'modelId': 'enelpol/poleval2021-task3',\n",
       "  'sha': 'b6e13ae11eca4e958f21dc6ce4b4f8f161c2da30',\n",
       "  'lastModified': '2022-04-25T12:29:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'enelpol',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'enelpol/poleval2021-task3',\n",
       "  'downloads': 3950,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-offensive': {'modelId': 'cardiffnlp/twitter-roberta-base-offensive',\n",
       "  'sha': 'afebc0c0c4c58177a8e6ab683c25beffeb351135',\n",
       "  'lastModified': '2021-05-20T15:05:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-offensive',\n",
       "  'downloads': 3937,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KM4STfulltext/SSCI-SciBERT-e2': {'modelId': 'KM4STfulltext/SSCI-SciBERT-e2',\n",
       "  'sha': '94c8bf53e16f6a586c5fa7d105b628898bb2aeab',\n",
       "  'lastModified': '2022-06-01T09:25:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KM4STfulltext',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KM4STfulltext/SSCI-SciBERT-e2',\n",
       "  'downloads': 3933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-bert-base': {'modelId': 'Langboat/mengzi-bert-base',\n",
       "  'sha': 'a685cb1101fb1ea116e8432b2e14042194e4738b',\n",
       "  'lastModified': '2021-10-14T09:01:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Langboat/mengzi-bert-base',\n",
       "  'downloads': 3917,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': 'ÁîüÊ¥ªÁöÑÁúüË∞õÊòØ[MASK]„ÄÇ'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/gpt2-chinese-lyric': {'modelId': 'uer/gpt2-chinese-lyric',\n",
       "  'sha': '5cd62aaad5739037a840c90f4dd3244bd8cdce10',\n",
       "  'lastModified': '2022-02-20T05:02:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'Chinese',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 128}}},\n",
       "  'id': 'uer/gpt2-chinese-lyric',\n",
       "  'downloads': 3915,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'ÊúÄÁæéÁöÑ‰∏çÊòØ‰∏ãÈõ®Â§©ÔºåÊòØÊõæ‰∏é‰Ω†Ë∫≤ËøáÈõ®ÁöÑÂ±ãÊ™ê'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'widget': [{'text': 'ÊúÄÁæéÁöÑ‰∏çÊòØ‰∏ãÈõ®Â§©ÔºåÊòØÊõæ‰∏é‰Ω†Ë∫≤ËøáÈõ®ÁöÑÂ±ãÊ™ê'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/large': {'modelId': 'funnel-transformer/large',\n",
       "  'sha': '3f56dd1209e9470b922bd24715c78db03af0fe51',\n",
       "  'lastModified': '2022-06-07T15:26:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/large',\n",
       "  'downloads': 3910,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-4-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-4-v2',\n",
       "  'sha': '1f1ab0943a42a52afd702e7e8337bec985c189ea',\n",
       "  'lastModified': '2021-08-05T08:39:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-4-v2',\n",
       "  'downloads': 3882,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KETI-AIR/ke-t5-base': {'modelId': 'KETI-AIR/ke-t5-base',\n",
       "  'sha': '3fdf631a2986e928ad8b22863f9da16da74d9906',\n",
       "  'lastModified': '2021-06-23T02:50:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KETI-AIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'KETI-AIR/ke-t5-base',\n",
       "  'downloads': 3880,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/MiniLM-L6-H384-uncased': {'modelId': 'nreimers/MiniLM-L6-H384-uncased',\n",
       "  'sha': '3276f0fac9d818781d7a1327b3ff818fc4e643c0',\n",
       "  'lastModified': '2021-08-30T20:05:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/MiniLM-L6-H384-uncased',\n",
       "  'downloads': 3879,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sampathkethineedi/industry-classification': {'modelId': 'sampathkethineedi/industry-classification',\n",
       "  'sha': '9914232048445adee24e4d4683c4d1873a9bafb4',\n",
       "  'lastModified': '2020-07-16T15:27:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'industry',\n",
       "   'buisiness',\n",
       "   'description',\n",
       "   'multi-class',\n",
       "   'classification'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sampathkethineedi',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sampathkethineedi/industry-classification',\n",
       "  'downloads': 3879,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/sampathkethineedi',\n",
       "   'tags': ['distilbert',\n",
       "    'pytorch',\n",
       "    'tensorflow',\n",
       "    'text-classification',\n",
       "    'industry',\n",
       "    'buisiness',\n",
       "    'description',\n",
       "    'multi-class',\n",
       "    'classification'],\n",
       "   'liscence': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jplu/tf-camembert-base': {'modelId': 'jplu/tf-camembert-base',\n",
       "  'sha': '2a40769e6a43da48783193e96e28cb2ddd5fbdb0',\n",
       "  'lastModified': '2020-12-11T21:47:52.000Z',\n",
       "  'tags': ['tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'jplu',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'jplu/tf-camembert-base',\n",
       "  'downloads': 3868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'indigo-ai/BERTino': {'modelId': 'indigo-ai/BERTino',\n",
       "  'sha': '4539d128176da9bbadf98ab813c9acaf68e8d8f1',\n",
       "  'lastModified': '2021-09-22T08:51:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'DISTILbert',\n",
       "   'Italian',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'indigo-ai',\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'indigo-ai/BERTino',\n",
       "  'downloads': 3854,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Vado al [MASK] a fare la spesa'},\n",
       "   {'text': 'Vado al parco a guardare le [MASK]'},\n",
       "   {'text': 'Il cielo √® [MASK] di stelle.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'tags': ['DISTILbert', 'Italian'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Vado al [MASK] a fare la spesa'},\n",
       "    {'text': 'Vado al parco a guardare le [MASK]'},\n",
       "    {'text': 'Il cielo √® [MASK] di stelle.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/nli-distilroberta-base-v2': {'modelId': 'sentence-transformers/nli-distilroberta-base-v2',\n",
       "  'sha': 'ee9754ad61d9164d693c8e4c458238433037023f',\n",
       "  'lastModified': '2022-06-15T21:56:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/nli-distilroberta-base-v2',\n",
       "  'downloads': 3854,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'optimum/distilbert-base-uncased-mnli': {'modelId': 'optimum/distilbert-base-uncased-mnli',\n",
       "  'sha': 'f2a41159d38dd6a54b91839de259af674c938e69',\n",
       "  'lastModified': '2022-03-24T16:19:12.000Z',\n",
       "  'tags': ['onnx',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'transformers',\n",
       "   'distilbert',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'optimum',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification']},\n",
       "  'id': 'optimum/distilbert-base-uncased-mnli',\n",
       "  'downloads': 3847,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilbert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification'}},\n",
       " 'facebook/mgenre-wiki': {'modelId': 'facebook/mgenre-wiki',\n",
       "  'sha': 'dbb6f7bc18c4f477073231b125254182f1290155',\n",
       "  'lastModified': '2022-06-14T14:23:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bm',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gn',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kg',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'qu',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'ti',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'arxiv:2103.12528',\n",
       "   'arxiv:2001.08210',\n",
       "   'transformers',\n",
       "   'retrieval',\n",
       "   'entity-retrieval',\n",
       "   'named-entity-disambiguation',\n",
       "   'entity-disambiguation',\n",
       "   'named-entity-linking',\n",
       "   'entity-linking',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mgenre-wiki',\n",
       "  'downloads': 3830,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bm',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gn',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kg',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'qu',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'ti',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yo',\n",
       "    'zh'],\n",
       "   'tags': ['retrieval',\n",
       "    'entity-retrieval',\n",
       "    'named-entity-disambiguation',\n",
       "    'entity-disambiguation',\n",
       "    'named-entity-linking',\n",
       "    'entity-linking',\n",
       "    'text2text-generation']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Prime2911/DialoGPT-medium-handsomejack': {'modelId': 'Prime2911/DialoGPT-medium-handsomejack',\n",
       "  'sha': 'c9d3196d32519f073bec0ce80aeb01abe78d8075',\n",
       "  'lastModified': '2022-03-11T07:54:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Prime2911',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Prime2911/DialoGPT-medium-handsomejack',\n",
       "  'downloads': 3826,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bakrianoo/sinai-voice-ar-stt': {'modelId': 'bakrianoo/sinai-voice-ar-stt',\n",
       "  'sha': '2d226249edf809b01a0e11159d1201ae1704b63c',\n",
       "  'lastModified': '2022-03-23T18:25:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ar',\n",
       "   'dataset:mozilla-foundation/common_voice_8_0',\n",
       "   'transformers',\n",
       "   'hf-asr-leaderboard',\n",
       "   'robust-speech-event',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'bakrianoo',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'bakrianoo/sinai-voice-ar-stt',\n",
       "  'downloads': 3822,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Example 1',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19077324.mp3'},\n",
       "   {'example_title': 'Example 2',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19205138.mp3'},\n",
       "   {'example_title': 'Example 3',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19331711.mp3'}],\n",
       "  'likes': 6,\n",
       "  'model-index': [{'name': 'Sinai Voice Arabic Speech Recognition Model',\n",
       "    'results': [{'task': {'type': 'automatic-speech-recognition',\n",
       "       'name': 'Speech Recognition'},\n",
       "      'dataset': {'type': 'mozilla-foundation/common_voice_8_0',\n",
       "       'name': 'Common Voice ar',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'type': 'wer', 'value': 0.181, 'name': 'Test WER'},\n",
       "       {'type': 'cer', 'value': 0.049, 'name': 'Test CER'}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 93.03}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Test Data',\n",
       "       'type': 'speech-recognition-community-v2/eval_data',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 90.79}]}]}],\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard',\n",
       "    'robust-speech-event'],\n",
       "   'datasets': ['mozilla-foundation/common_voice_8_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'model-index': [{'name': 'Sinai Voice Arabic Speech Recognition Model',\n",
       "     'results': [{'task': {'type': 'automatic-speech-recognition',\n",
       "        'name': 'Speech Recognition'},\n",
       "       'dataset': {'type': 'mozilla-foundation/common_voice_8_0',\n",
       "        'name': 'Common Voice ar',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'type': 'wer', 'value': 0.181, 'name': 'Test WER'},\n",
       "        {'type': 'cer', 'value': 0.049, 'name': 'Test CER'}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 93.03}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Test Data',\n",
       "        'type': 'speech-recognition-community-v2/eval_data',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 90.79}]}]}],\n",
       "   'widget': [{'example_title': 'Example 1',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19077324.mp3'},\n",
       "    {'example_title': 'Example 2',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19205138.mp3'},\n",
       "    {'example_title': 'Example 3',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19331711.mp3'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/canine-c': {'modelId': 'google/canine-c',\n",
       "  'sha': '1e8c8b3a4e860cb2a23a14c3fbba61ef3aed51f6',\n",
       "  'lastModified': '2021-08-13T08:24:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'canine',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2103.06874',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['CanineModel'], 'model_type': 'canine'},\n",
       "  'id': 'google/canine-c',\n",
       "  'downloads': 3806,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'AK270802/DialoGPT-small-harrypotter': {'modelId': 'AK270802/DialoGPT-small-harrypotter',\n",
       "  'sha': '5e5434fd66c852ebf69cc07279d85f55a645768e',\n",
       "  'lastModified': '2022-01-16T11:19:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AK270802',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AK270802/DialoGPT-small-harrypotter',\n",
       "  'downloads': 3761,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gogamza/kobart-summarization': {'modelId': 'gogamza/kobart-summarization',\n",
       "  'sha': '8a63d6913edc0e16a902e3fa8b688a134f0dd776',\n",
       "  'lastModified': '2021-11-22T10:59:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'gogamza',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'gogamza/kobart-summarization',\n",
       "  'downloads': 3760,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'tags': ['bart'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'banden/DialoGPT-medium-RickBot': {'modelId': 'banden/DialoGPT-medium-RickBot',\n",
       "  'sha': '010c54cb71eeb60b5a15b270842d132bde6254aa',\n",
       "  'lastModified': '2021-09-21T14:58:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'banden',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'banden/DialoGPT-medium-RickBot',\n",
       "  'downloads': 3751,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'inokufu/flaubert-base-uncased-xnli-sts': {'modelId': 'inokufu/flaubert-base-uncased-xnli-sts',\n",
       "  'sha': 'e4fcf0bc37d8d55fbbe1cc8096eaeaa4057d2560',\n",
       "  'lastModified': '2022-02-18T16:50:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'feature-extraction',\n",
       "   'fr',\n",
       "   'dataset:xnli',\n",
       "   'dataset:stsb_multi_mt',\n",
       "   'arxiv:1809.05053',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'xnli',\n",
       "   'stsb_multi_mt'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'inokufu',\n",
       "  'config': {'architectures': ['FlaubertModel'], 'model_type': 'flaubert'},\n",
       "  'id': 'inokufu/flaubert-base-uncased-xnli-sts',\n",
       "  'downloads': 3746,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'source_sentence': \"C'est une personne heureuse\",\n",
       "    'sentences': [\"C'est un chien heureux\",\n",
       "     \"C'est une personne tr√®s heureuse\",\n",
       "     \"Aujourd'hui est une journ√©e ensoleill√©e\"]}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'fr',\n",
       "   'tags': ['sentence-similarity',\n",
       "    'transformers',\n",
       "    'fr',\n",
       "    'flaubert',\n",
       "    'sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'xnli',\n",
       "    'stsb_multi_mt'],\n",
       "   'datasets': ['xnli', 'stsb_multi_mt']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nvidia/mit-b0': {'modelId': 'nvidia/mit-b0',\n",
       "  'sha': 'f13c39228a46f6f160f23f27cbef82160b8c8773',\n",
       "  'lastModified': '2022-03-02T14:59:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'segformer',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet_1k',\n",
       "   'arxiv:2105.15203',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'nvidia',\n",
       "  'config': {'architectures': ['SegformerForImageClassification'],\n",
       "   'model_type': 'segformer'},\n",
       "  'id': 'nvidia/mit-b0',\n",
       "  'downloads': 3745,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "    'example_title': 'House'},\n",
       "   {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "    'example_title': 'Castle'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet_1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "     'example_title': 'House'},\n",
       "    {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "     'example_title': 'Castle'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'sentence-transformers/roberta-base-nli-mean-tokens': {'modelId': 'sentence-transformers/roberta-base-nli-mean-tokens',\n",
       "  'sha': '993765530351e8b2a4da74bed694d80de826cbb3',\n",
       "  'lastModified': '2022-06-15T21:54:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-base-nli-mean-tokens',\n",
       "  'downloads': 3745,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ahotrod/albert_xxlargev1_squad2_512': {'modelId': 'ahotrod/albert_xxlargev1_squad2_512',\n",
       "  'sha': '291f0fa26d2c80d8a473b6116164a083d252b4fe',\n",
       "  'lastModified': '2020-12-11T21:31:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ahotrod',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ahotrod/albert_xxlargev1_squad2_512',\n",
       "  'downloads': 3740,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amaz√¥nica or Amaz√¥nia; Spanish: Selva Amaz√≥nica, Amazon√≠a or usually Amazonia; French: For√™t amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nboost/pt-tinybert-msmarco': {'modelId': 'nboost/pt-tinybert-msmarco',\n",
       "  'sha': 'b0de5c59e4779c149295b9bd0e5988a8f2cd2be7',\n",
       "  'lastModified': '2021-05-20T01:28:00.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nboost',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nboost/pt-tinybert-msmarco',\n",
       "  'downloads': 3724,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment': {'modelId': 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment',\n",
       "  'sha': 'b0df630d23e5aa5f8a326017605337f6d0863ecd',\n",
       "  'lastModified': '2021-10-17T11:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ar',\n",
       "   'arxiv:2103.06678',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'CAMeL-Lab',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment',\n",
       "  'downloads': 3718,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ£ŸÜÿß ÿ®ÿÆŸäÿ±'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': 'ÿ£ŸÜÿß ÿ®ÿÆŸäÿ±'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-base-mnli': {'modelId': 'microsoft/deberta-base-mnli',\n",
       "  'sha': 'a80a6eb013898011540b19bf1f64e21eb61e53d6',\n",
       "  'lastModified': '2021-12-09T13:36:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-base-mnli',\n",
       "  'downloads': 3712,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TransQuest/monotransquest-da-multilingual': {'modelId': 'TransQuest/monotransquest-da-multilingual',\n",
       "  'sha': 'cd947f301588992a749d22fc867e535bc9cb1703',\n",
       "  'lastModified': '2021-06-03T19:06:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual-multilingual',\n",
       "   'transformers',\n",
       "   'Quality Estimation',\n",
       "   'monotransquest',\n",
       "   'DA',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'TransQuest',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'TransQuest/monotransquest-da-multilingual',\n",
       "  'downloads': 3696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual-multilingual',\n",
       "   'tags': ['Quality Estimation', 'monotransquest', 'DA'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'alvaroalon2/biobert_diseases_ner': {'modelId': 'alvaroalon2/biobert_diseases_ner',\n",
       "  'sha': 'ce0fd86ac9e145d1a6ca3455219843e0a855471f',\n",
       "  'lastModified': '2021-07-07T12:35:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'English',\n",
       "   'dataset:BC5CDR-diseases',\n",
       "   'dataset:ncbi_disease',\n",
       "   'transformers',\n",
       "   'NER',\n",
       "   'Biomedical',\n",
       "   'Diseases',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'alvaroalon2',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'alvaroalon2/biobert_diseases_ner',\n",
       "  'downloads': 3696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'English',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['token-classification', 'NER', 'Biomedical', 'Diseases'],\n",
       "   'datasets': ['BC5CDR-diseases', 'ncbi_disease']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/nli-roberta-base': {'modelId': 'cross-encoder/nli-roberta-base',\n",
       "  'sha': '1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42',\n",
       "  'lastModified': '2021-08-05T08:41:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:snli',\n",
       "   'transformers',\n",
       "   'roberta-base',\n",
       "   'license:apache-2.0',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/nli-roberta-base',\n",
       "  'downloads': 3686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system‚Äôs largest world. Konstantin Batygin did not set out to solve one of the solar system‚Äôs most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system‚Äôs missing ‚ÄúPlanet Nine,‚Äù spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn‚Äôt rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: ‚ÄúOh! This is how Europa formed.‚Äù Europa is one of Jupiter‚Äôs four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the C√¥te d‚ÄôAzur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system‚Äôs formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['roberta-base'],\n",
       "   'datasets': ['multi_nli', 'snli'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-large': {'modelId': 'studio-ousia/luke-large',\n",
       "  'sha': '0729d044dfe301d9ecabc222d60633f92ac450eb',\n",
       "  'lastModified': '2022-04-13T09:06:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'luke',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1906.08237',\n",
       "   'arxiv:1903.07785',\n",
       "   'arxiv:2002.01808',\n",
       "   'transformers',\n",
       "   'named entity recognition',\n",
       "   'entity typing',\n",
       "   'relation classification',\n",
       "   'question answering',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForMaskedLM'], 'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-large',\n",
       "  'downloads': 3686,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://github.com/studio-ousia/luke/raw/master/resources/luke_logo.png',\n",
       "   'tags': ['luke',\n",
       "    'named entity recognition',\n",
       "    'entity typing',\n",
       "    'relation classification',\n",
       "    'question answering'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'osanseviero/dalle-mini-fork': {'modelId': 'osanseviero/dalle-mini-fork',\n",
       "  'sha': '18bdfd0e6c1c68a01666e9517d67875be0180dd0',\n",
       "  'lastModified': '2021-08-17T10:30:17.000Z',\n",
       "  'tags': ['jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'generic',\n",
       "   'text-to-image'],\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'private': False,\n",
       "  'author': 'osanseviero',\n",
       "  'config': {'architectures': ['omFlaxBartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'osanseviero/dalle-mini-fork',\n",
       "  'downloads': 3681,\n",
       "  'library_name': 'generic',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'library_name': 'generic',\n",
       "   'language': ['en'],\n",
       "   'pipeline_tag': 'text-to-image'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-fa-zwnj-base': {'modelId': 'HooshvareLab/bert-fa-zwnj-base',\n",
       "  'sha': '3880fac085e1a338e9564907cba0adeb9e14bc72',\n",
       "  'lastModified': '2021-05-18T21:05:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-fa-zwnj-base',\n",
       "  'downloads': 3672,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÿ≤ŸÜÿØ⁄Ø€å €å⁄© ÿ≥ŸàÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿß€åŸÜ ⁄©Ÿá ⁄Ü⁄ØŸàŸÜŸá [MASK] ⁄©ŸÜ€åŸÖ Ÿæÿßÿ≥ÿÆ ÿß€åŸÜ ÿ≥ŸàÿßŸÑ!'},\n",
       "   {'text': 'ÿ≤ŸÜÿØ⁄Ø€å ÿßÿ≤ ŸÖÿ±⁄Ø Ÿæÿ±ÿ≥€åÿØ: ⁄Üÿ±ÿß ŸáŸÖŸá ŸÖŸÜ ÿ±ÿß [MASK] ÿØÿßÿ±ŸÜÿØ ÿßŸÖÿß ÿßÿ≤ ÿ™Ÿà ŸÖÿ™ŸÜŸÅÿ±ŸÜÿØÿü'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-stance-climate': {'modelId': 'cardiffnlp/twitter-roberta-base-stance-climate',\n",
       "  'sha': 'a2802f328ed851a97696c724f46e75773c34e098',\n",
       "  'lastModified': '2021-05-20T15:10:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-stance-climate',\n",
       "  'downloads': 3671,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-0': {'modelId': 'Jeevesh8/bert_ft_qqp-0',\n",
       "  'sha': '17645bf0b6f171d517ac3e9a13f50eb1908b5b4d',\n",
       "  'lastModified': '2022-05-07T12:10:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-0',\n",
       "  'downloads': 3665,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'finiteautomata/bertweet-base-emotion-analysis': {'modelId': 'finiteautomata/bertweet-base-emotion-analysis',\n",
       "  'sha': '64046df9cc41eab40e1ecde7d2b7fb42b971be5b',\n",
       "  'lastModified': '2021-12-10T13:28:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'emotion-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'finiteautomata/bertweet-base-emotion-analysis',\n",
       "  'downloads': 3660,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['emotion-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-base-chinese-ner': {'modelId': 'ckiplab/albert-base-chinese-ner',\n",
       "  'sha': 'ef1b19225fc53f10fafd8bfdefb41ac2f2f4e177',\n",
       "  'lastModified': '2022-05-10T03:28:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForTokenClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-base-chinese-ner',\n",
       "  'downloads': 3638,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'ÊàëÂè´Ê≤ÉÂ∞îÂ§´ÂÜàÔºåÊàë‰ΩèÂú®ÊüèÊûó„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´Ëê®ÊãâÔºåÊàë‰ΩèÂú®‰º¶Êï¶„ÄÇ'},\n",
       "   {'text': 'ÊàëÂè´ÂÖãÊãâÊãâÔºåÊàë‰ΩèÂú®Âä†Â∑û‰ºØÂÖãÂà©„ÄÇ'}],\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'elastic/distilbert-base-cased-finetuned-conll03-english': {'modelId': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "  'sha': '3043a315aae69b6e2f88056b23100e144791ac99',\n",
       "  'lastModified': '2022-06-24T09:30:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'elastic',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "  'downloads': 3620,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': [{'name': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "    'results': [{'task': {'type': 'token-classification',\n",
       "       'name': 'Token Classification'},\n",
       "      'dataset': {'name': 'conll2003',\n",
       "       'type': 'conll2003',\n",
       "       'config': 'conll2003',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9834432212868665,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9857564461012737,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9882123948925569,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9869828926905132,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.07748260349035263,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['conll2003'],\n",
       "   'model-index': [{'name': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "     'results': [{'task': {'type': 'token-classification',\n",
       "        'name': 'Token Classification'},\n",
       "       'dataset': {'name': 'conll2003',\n",
       "        'type': 'conll2003',\n",
       "        'config': 'conll2003',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9834432212868665,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9857564461012737,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9882123948925569,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9869828926905132,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.07748260349035263,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stas/tiny-wmt19-en-de': {'modelId': 'stas/tiny-wmt19-en-de',\n",
       "  'sha': '18ca8fc156edb91968dd4d70e33fbe5989d04368',\n",
       "  'lastModified': '2021-05-03T01:48:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'dataset:wmt19',\n",
       "   'transformers',\n",
       "   'wmt19',\n",
       "   'testing',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stas',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'stas/tiny-wmt19-en-de',\n",
       "  'downloads': 3620,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['wmt19', 'testing'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/bert-small-mm_retrieval-passage_encoder': {'modelId': 'deepset/bert-small-mm_retrieval-passage_encoder',\n",
       "  'sha': 'c764744512975bd3823f689601ab0e388a29c366',\n",
       "  'lastModified': '2021-10-19T16:14:29.000Z',\n",
       "  'tags': ['pytorch', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/bert-small-mm_retrieval-passage_encoder',\n",
       "  'downloads': 3618,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/bert-small-mm_retrieval-question_encoder': {'modelId': 'deepset/bert-small-mm_retrieval-question_encoder',\n",
       "  'sha': 'a34edf571667cc1ba38cec55c56f2905f13336a2',\n",
       "  'lastModified': '2021-10-19T15:51:37.000Z',\n",
       "  'tags': ['pytorch', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/bert-small-mm_retrieval-question_encoder',\n",
       "  'downloads': 3618,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/incoder-1B': {'modelId': 'facebook/incoder-1B',\n",
       "  'sha': '01f46041ac45a5a1ac9a60875189c15209d2fee9',\n",
       "  'lastModified': '2022-05-31T16:56:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'arxiv:2204.05999',\n",
       "   'transformers',\n",
       "   'code',\n",
       "   'python',\n",
       "   'javascript',\n",
       "   'license:cc-by-nc-4.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'facebook/incoder-1B',\n",
       "  'downloads': 3587,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-nc-4.0',\n",
       "   'tags': ['code', 'python', 'javascript']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/speaker-segmentation': {'modelId': 'pyannote/speaker-segmentation',\n",
       "  'sha': '3e45863bb592c3fa20db5fd25c1d297acd1ce9d0',\n",
       "  'lastModified': '2022-03-23T09:23:09.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-segmentation',\n",
       "   'speaker-diarization',\n",
       "   'speaker-change-detection',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/speaker-segmentation',\n",
       "  'downloads': 3575,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-segmentation',\n",
       "    'speaker-diarization',\n",
       "    'speaker-change-detection',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dandelin/vilt-b32-finetuned-coco': {'modelId': 'dandelin/vilt-b32-finetuned-coco',\n",
       "  'sha': '2f3f7f3f62a3f4f429c26309256485bbd9b0e40a',\n",
       "  'lastModified': '2022-01-23T09:45:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vilt',\n",
       "   'arxiv:2102.03334',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dandelin',\n",
       "  'config': {'architectures': ['ViltForImageAndTextRetrieval'],\n",
       "   'model_type': 'vilt'},\n",
       "  'id': 'dandelin/vilt-b32-finetuned-coco',\n",
       "  'downloads': 3565,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'ViltForImageAndTextRetrieval',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-44': {'modelId': 'Jeevesh8/init_bert_ft_qqp-44',\n",
       "  'sha': 'e91e1d668d5362aab64271c3c2ff620674b9cecf',\n",
       "  'lastModified': '2022-06-02T12:39:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-44',\n",
       "  'downloads': 3563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-48': {'modelId': 'Jeevesh8/init_bert_ft_qqp-48',\n",
       "  'sha': 'e8b3d062302ce6d91c785b14b848974ecb336fab',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-48',\n",
       "  'downloads': 3563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-52': {'modelId': 'Jeevesh8/init_bert_ft_qqp-52',\n",
       "  'sha': '038c33de491e995aa97d15abd229aa8bc1177e63',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-52',\n",
       "  'downloads': 3562,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-40': {'modelId': 'Jeevesh8/init_bert_ft_qqp-40',\n",
       "  'sha': '9d032ef7b58b123d2763e8789ec1f424a804d181',\n",
       "  'lastModified': '2022-06-02T12:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-40',\n",
       "  'downloads': 3561,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-1': {'modelId': 'Jeevesh8/init_bert_ft_qqp-1',\n",
       "  'sha': 'f7a2fb5ce719ac903ffd3b5b500262046abb1319',\n",
       "  'lastModified': '2022-06-02T12:37:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-1',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-0': {'modelId': 'Jeevesh8/init_bert_ft_qqp-0',\n",
       "  'sha': '347f6a3eeb6992f6d9cc1647834c293e9934248c',\n",
       "  'lastModified': '2022-06-02T12:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-0',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-58': {'modelId': 'Jeevesh8/init_bert_ft_qqp-58',\n",
       "  'sha': 'adb129bcb03fe426a47c15db5171aac20fa634f8',\n",
       "  'lastModified': '2022-06-02T12:39:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-58',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-51': {'modelId': 'Jeevesh8/init_bert_ft_qqp-51',\n",
       "  'sha': '5336d616552fb1360f6e1ef4421fad176c787a95',\n",
       "  'lastModified': '2022-06-02T12:39:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-51',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-22': {'modelId': 'Jeevesh8/init_bert_ft_qqp-22',\n",
       "  'sha': 'cad4666abbe0793ea90ad26a0a72c38ea5678ad9',\n",
       "  'lastModified': '2022-06-02T12:40:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-22',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-25': {'modelId': 'Jeevesh8/init_bert_ft_qqp-25',\n",
       "  'sha': '411d83e71e6ab008f1a37692975ac1fd9477b2b6',\n",
       "  'lastModified': '2022-06-02T12:39:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-25',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-53': {'modelId': 'Jeevesh8/init_bert_ft_qqp-53',\n",
       "  'sha': '7f3a8e70c3a817a9793e402af7a6fa4cdc6d1cf4',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-53',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-56': {'modelId': 'Jeevesh8/init_bert_ft_qqp-56',\n",
       "  'sha': '5d6a7eb9d10e20374bfc95d431e63ef6d99965c1',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-56',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-99': {'modelId': 'Jeevesh8/bert_ft_qqp-99',\n",
       "  'sha': 'e429b7cd0c1dd70376f8dc03dfa8b773fda50395',\n",
       "  'lastModified': '2022-05-09T13:43:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-99',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-6': {'modelId': 'Jeevesh8/init_bert_ft_qqp-6',\n",
       "  'sha': '0c42c612266c732b13e08c8829450408fef0324a',\n",
       "  'lastModified': '2022-06-02T12:37:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-6',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-16': {'modelId': 'Jeevesh8/init_bert_ft_qqp-16',\n",
       "  'sha': 'dd2887d6ba950e43507ef5733e649a19c8ef8e36',\n",
       "  'lastModified': '2022-06-02T12:41:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-16',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-15': {'modelId': 'Jeevesh8/init_bert_ft_qqp-15',\n",
       "  'sha': 'eef52e925b9ff308fc3724a434031a7173eb5ccf',\n",
       "  'lastModified': '2022-06-02T12:41:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-15',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-14': {'modelId': 'Jeevesh8/init_bert_ft_qqp-14',\n",
       "  'sha': '8a10dcb1ef0483531a42defd28531e94f2480e34',\n",
       "  'lastModified': '2022-06-02T12:39:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-14',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-21': {'modelId': 'Jeevesh8/init_bert_ft_qqp-21',\n",
       "  'sha': '5de6a21340b75619113f3242c83d775732609067',\n",
       "  'lastModified': '2022-06-02T12:39:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-21',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-17': {'modelId': 'Jeevesh8/init_bert_ft_qqp-17',\n",
       "  'sha': 'a4ac389a3423dba0036456b1051d98542e97af54',\n",
       "  'lastModified': '2022-06-02T12:40:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-17',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-27': {'modelId': 'Jeevesh8/init_bert_ft_qqp-27',\n",
       "  'sha': '62a06d82e4a48fd86ee4a565a1511b7c7e2126c8',\n",
       "  'lastModified': '2022-06-02T12:39:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-27',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-59': {'modelId': 'Jeevesh8/init_bert_ft_qqp-59',\n",
       "  'sha': '96d09ede67b2103851c3990670febdba6ff1ba80',\n",
       "  'lastModified': '2022-06-02T12:39:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-59',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-60': {'modelId': 'Jeevesh8/init_bert_ft_qqp-60',\n",
       "  'sha': '60010aa746dbfd973a9c10ca03c27d0639783804',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-60',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-57': {'modelId': 'Jeevesh8/init_bert_ft_qqp-57',\n",
       "  'sha': '0efe5b17b3a8f1795dca2ead725c1e9f9d1cee36',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-57',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-47': {'modelId': 'Jeevesh8/init_bert_ft_qqp-47',\n",
       "  'sha': '8513a6f7da0321943894907cf82c223ebbfa8041',\n",
       "  'lastModified': '2022-06-02T12:39:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-47',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-55': {'modelId': 'Jeevesh8/init_bert_ft_qqp-55',\n",
       "  'sha': '01eed3cc7e735fa2f9accc6d28c3092ed3c50222',\n",
       "  'lastModified': '2022-06-02T12:41:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-55',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-37': {'modelId': 'Jeevesh8/init_bert_ft_qqp-37',\n",
       "  'sha': 'cd796932624b2969733f3319b77c85bfb50fc808',\n",
       "  'lastModified': '2022-06-02T12:39:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-37',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-36': {'modelId': 'Jeevesh8/init_bert_ft_qqp-36',\n",
       "  'sha': '490a3538c45e526812d876156703ea6680d71a66',\n",
       "  'lastModified': '2022-06-02T12:40:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-36',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-69': {'modelId': 'Jeevesh8/init_bert_ft_qqp-69',\n",
       "  'sha': '11b27b2e536206739dc19c848cd2d1daf4869c22',\n",
       "  'lastModified': '2022-06-02T12:40:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-69',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-92': {'modelId': 'Jeevesh8/bert_ft_qqp-92',\n",
       "  'sha': '4660394cec78461b551822615881290dc63f45a0',\n",
       "  'lastModified': '2022-05-09T13:25:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-92',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-3': {'modelId': 'Jeevesh8/init_bert_ft_qqp-3',\n",
       "  'sha': '9d948e08358914a74874f4eb5cf3c57a71e94045',\n",
       "  'lastModified': '2022-06-02T12:37:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-3',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-4': {'modelId': 'Jeevesh8/init_bert_ft_qqp-4',\n",
       "  'sha': 'c94e68de0e093b7aee98c4bc134f6eb76a0c6504',\n",
       "  'lastModified': '2022-06-02T12:37:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-4',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-5': {'modelId': 'Jeevesh8/init_bert_ft_qqp-5',\n",
       "  'sha': '578cc4e2c37914baa3ac728fc4234ff1c129ec47',\n",
       "  'lastModified': '2022-06-02T12:37:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-5',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-7': {'modelId': 'Jeevesh8/init_bert_ft_qqp-7',\n",
       "  'sha': '933e4d15c36040fd937caeea8723820bbaeca1ab',\n",
       "  'lastModified': '2022-06-02T12:38:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-7',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-10': {'modelId': 'Jeevesh8/init_bert_ft_qqp-10',\n",
       "  'sha': '85dcd3d2c38a5e71b605474e285b9bbb1b121c9d',\n",
       "  'lastModified': '2022-06-02T12:37:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-10',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-9': {'modelId': 'Jeevesh8/init_bert_ft_qqp-9',\n",
       "  'sha': '3738e5bf4ff0956b53bb322dda7014c693ecdd8a',\n",
       "  'lastModified': '2022-06-02T12:37:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-9',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"modelinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m {m[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]: m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m data}\n",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m {m[\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m]: m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m data}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "data = {m[\"id\"]: m for m in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICSE-AE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f978456172793b1e5202b1e32483855cb2f0db51d84bae17525fe5e1390177d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
